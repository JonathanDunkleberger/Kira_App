# âœ¨ Kira AI â€” Voice-First AI Media Companion

![Kira AI Banner](./packages/docs/KIRA2%20Demo.png)

**An AI you talk to while watching, gaming, or reading â€” hands free, in real time.**

Kira is a voice AI companion built for media consumption. You speak, she listens. She can see your screen, remember what you talked about last week, and hold a natural conversation without you ever touching your keyboard.

ğŸ”— **Try it live:** [xoxokira.com](https://www.xoxokira.com)

---

## ğŸ¯ What It Does

You're watching *Garden of Words* and want to talk about it without pausing. You say "Have you seen this? What do you think about the animation?" and Kira responds conversationally â€” she can see your screen, knows what you're watching, and remembers that you love Makoto Shinkai films from a conversation two weeks ago.

The experience is built around a simple idea: **the best companion for media doesn't compete with it for your hands or your eyes.** Voice-in, voice-out, always listening, always watching alongside you.

---

## ğŸ§  How It Works

Not a wrapper around a single API â€” it's a full real-time system where every component operates under tight latency constraints:

**Streaming voice pipeline** â€” Microphone audio flows through a client-side AudioWorklet, streams to Deepgram for real-time transcription, routes through GPT-4o for a response, then streams sentence-by-sentence through Azure TTS back to your speakers. Every stage operates incrementally so nothing waits for anything else to finish. The first sentence of a response plays while the third is still being written.

**Adaptive conversation timing** â€” The hardest UX problem in voice AI is knowing when someone has stopped talking. Say "yes" and you want an instant response. Ask a long philosophical question with a thinking pause in the middle and you don't want to get cut off. The adaptive silence detector scales its patience based on how long you've been speaking â€” fast for short replies, patient for complex thoughts.

**Vision system** â€” Screen share frames run through a scene-change detection algorithm that maintains a rolling buffer of visual context. When you speak, Kira receives not just a single screenshot but a timeline of the last 3 scene changes plus the current frame â€” so she can answer "what just happened?" not just "what's on screen right now?"

**Self-healing connections** â€” Third-party WebSocket connections die silently during long sessions. No error, no close event â€” they just stop producing data. Health monitoring detects when the speech-to-text stream has gone quiet and automatically tears down and re-establishes the connection transparently.

**Persistent memory** â€” Conversations are summarized and stored. When you come back tomorrow, Kira remembers your name, your taste in anime, and that you were on episode 3 of something last time.

---

## ğŸ—ï¸ Architecture

```mermaid
flowchart LR
    subgraph Browser["ğŸ–¥ï¸ Browser (Next.js on Vercel)"]
        MIC[ğŸ™ï¸ Microphone] --> AW[AudioWorklet\nVAD + Adaptive EOU]
        SCR[ğŸ–µ Screen Share] --> SD[Scene Detection\nTemporal Buffer]
        AW --> WS_C[WebSocket]
        SD --> WS_C
        Q[ğŸ”Š Playback Queue] --> SPK[Speaker]
    end

    subgraph Server["âš¡ Node.js Server (Render)"]
        WS_S[WebSocket] --> STT[Deepgram STT\nStreaming]
        STT --> TB[Transcript Buffer\nStale Filter + Dedup]
        TB --> LLM[GPT-4o\nStreaming + Vision]
        LLM --> SB[Sentence Splitter]
        SB --> TTS[Azure TTS\nSSML Streaming]
        TTS --> WS_S
        LLM --> MEM[Memory]
        MEM --> DB[(PostgreSQL)]
    end

    WS_C <-->|"audio + images + control"| WS_S
    WS_S -->|"audio + transcripts + state"| Q
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|:------|:-----------|
| Frontend | Next.js 14, React 18, Tailwind CSS, Vercel |
| Realtime Server | Node.js, `ws`, custom streaming orchestration |
| Speech-to-Text | Deepgram (live WebSocket) |
| Language Model | OpenAI GPT-4o (streaming + tool use + vision) |
| Text-to-Speech | Azure Cognitive Services (per-sentence SSML) |
| Vision | `getDisplayMedia` + canvas scene diffing â†’ GPT-4o |
| Auth & Billing | Clerk + Stripe |
| Database | Supabase (PostgreSQL) + Prisma ORM |

---

## ğŸ“‚ Project Structure

```
ai-media-companion/
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ web/                        # Next.js frontend
â”‚   â”‚   â”œâ”€â”€ src/hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ useKiraSocket.ts        # WebSocket + VAD + adaptive EOU
â”‚   â”‚   â”‚   â””â”€â”€ useSceneDetection.ts    # Screen share scene diffing
â”‚   â”‚   â””â”€â”€ public/worklets/
â”‚   â”‚       â””â”€â”€ AudioWorkletProcessor.js
â”‚   â”‚
â”‚   â””â”€â”€ server/                     # Realtime voice server
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ server.ts               # Pipeline orchestration
â”‚           â”œâ”€â”€ DeepgramSTTStreamer.ts   # STT with self-healing
â”‚           â””â”€â”€ AzureTTSStreamer.ts      # SSML synthesis
â”‚
â””â”€â”€ prisma/                         # Shared schema + migrations
```

---

## ğŸš€ Run It Yourself

```bash
git clone https://github.com/JonathanDunkleberger/Kira_AI_2.git
cd Kira_AI_2/ai-media-companion
pnpm install
```

Create `.env.local` files at root and in `packages/server/` â€” see the `.env.example` files for what's needed.

```bash
pnpm dev:web      # â†’ http://localhost:3000
pnpm dev:server   # â†’ ws://localhost:10000
```

Deployment details (Vercel + Render setup) are in [`DEPLOY.md`](./DEPLOY.md).

---

## ğŸ“„ License

MIT
