===== packages/web/src/components/Live2DAvatar.tsx =====
"use client";

import { useEffect, useRef, useState } from "react";
import { LipSyncEngine } from "../lib/LipSyncEngine";

// Silent in production unless ?debug is in the URL
const isDebug = typeof window !== 'undefined' && (process.env.NODE_ENV !== 'production' || window.location.search.includes('debug'));
function debugLog(...args: any[]) { if (isDebug) console.log(...args); }

// pixi-live2d-display requires PIXI on window before import.
// Dynamic import is handled below to avoid SSR issues.

/** Load the Cubism 4 Core SDK if not already present */
function loadCubismCore(): Promise<void> {
  return new Promise((resolve, reject) => {
    if ((window as any).Live2DCubismCore) {
      resolve();
      return;
    }
    const script = document.createElement("script");
    script.src = "https://cubism.live2d.com/sdk-web/cubismcore/live2dcubismcore.min.js";
    script.onload = () => {
      debugLog("[Live2D] Cubism Core loaded");
      resolve();
    };
    script.onerror = () => reject(new Error("Failed to load Cubism Core SDK"));
    document.head.appendChild(script);
  });
}

/** Static emotion→expression map (used in init flush + expression effect) */
const EMOTION_MAP_STATIC: Record<string, string | null> = {
  neutral: null,
  happy: null,
  excited: "star_eyes",
  love: "heart_eyes",
  blush: "blush",
  sad: "tears",
  angry: "angry",
  playful: "tongue_out",
  thinking: "dazed",
  speechless: "speechless",
  eyeroll: "eye_roll",
  sleepy: "sleeping",
  frustrated: "dark_face",
  confused: "spiral_eyes",
  surprised: "donut_mouth",
};

/** Per-emotion parameter overrides (brows, head tilt, mouth shape) */
const EMOTION_PARAMS: Record<string, Record<string, number>> = {
  sad: {
    "ParamBrowLY": -0.7,      // Brows droop down
    "ParamBrowRY": -0.7,
    "ParamBrowLForm": -0.5,   // Inner brows raise (worried shape)
    "ParamBrowRForm": -0.5,
    "ParamAngle8": -3,         // Slight head down
  },
  angry: {
    "ParamBrowLY": -0.3,      // Brows down
    "ParamBrowRY": -0.3,
    "ParamBrowLForm": 0.8,    // Brows furrowed inward
    "ParamBrowRForm": 0.8,
    "ParamAngle9": -2,         // Slight aggressive head tilt
  },
  thinking: {
    "ParamBrowLY": 0.5,       // One brow up
    "ParamBrowRY": -0.2,      // Other slightly down (asymmetric = thinking)
    "ParamAngle9": 4,          // Head tilt to side
    "ParamAngle8": 2,          // Slight head up
  },
  excited: {
    "ParamBrowLY": 0.8,       // Brows up!
    "ParamBrowRY": 0.8,
    "ParamAngle8": 2,          // Head up (enthusiastic)
  },
  love: {
    "ParamBrowLY": 0.4,       // Soft raised brows
    "ParamBrowRY": 0.4,
    "ParamAngle9": 3,          // Gentle head tilt
  },
  blush: {
    "ParamBrowLY": 0.3,
    "ParamBrowRY": 0.3,
    "ParamAngle8": -2,         // Shy head down
    "ParamAngle9": 3,          // Head tilt
  },
  happy: {
    "ParamBrowLY": 0.4,       // Slightly raised brows
    "ParamBrowRY": 0.4,
    "ParamEyeRSmile": 0.6,    // Eye smile
    "ParamEyeLSmile": 0.6,
  },
  sleepy: {
    "ParamBrowLY": -0.5,      // Drooping brows
    "ParamBrowRY": -0.5,
    "ParamAngle8": -4,         // Head drooping down
  },
  speechless: {
    "ParamBrowLY": 0.9,       // Brows way up (shock)
    "ParamBrowRY": 0.9,
  },
  eyeroll: {
    "ParamBrowLY": 0.3,
    "ParamBrowRY": 0.3,
    "ParamAngle9": 5,          // Exasperated head tilt
  },
  playful: {
    "ParamBrowLY": 0.5,       // One brow raised (mischievous)
    "ParamBrowRY": -0.1,
    "ParamAngle9": -3,         // Cheeky head tilt
  },
  frustrated: {
    "ParamBrowLY": -0.5,
    "ParamBrowRY": -0.5,
    "ParamBrowLForm": 0.7,
    "ParamBrowRForm": 0.7,
    "ParamAngle8": -3,         // Head down (menacing)
  },
  confused: {
    "ParamBrowLY": 0.6,       // One brow way up
    "ParamBrowRY": -0.3,      // Other down (asymmetric = confused)
    "ParamAngle9": 5,          // Head tilt
  },
  surprised: {
    "ParamBrowLY": 0.9,
    "ParamBrowRY": 0.9,
    "ParamAngle8": 2,          // Head back slightly
  },
};

interface Live2DAvatarProps {
  isSpeaking: boolean;
  analyserNode: AnalyserNode | null;
  emotion?: string | null;
  accessories?: string[];
  action?: string | null;
  onModelReady?: () => void;
  onLoadError?: () => void;
}

/** Accessory conflict groups — applying one removes conflicting ones */
const ACCESSORY_CONFLICTS: Record<string, string[]> = {
  headphones_on: ["neck_headphones", "earbuds"],
  neck_headphones: ["headphones_on", "earbuds"],
  earbuds: ["headphones_on", "neck_headphones"],
};

/** Session hairstyle cycle — rotates every 5 minutes */
const HAIRSTYLE_CYCLE = [
  null,              // Default look (no hairstyle expression)
  "low_twintails",   // Twin tails
  "short_hair",      // Short cut
  "clip_bangs",      // Bangs clipped back
];
const HAIRSTYLE_CYCLE_INTERVAL = 5 * 60 * 1000; // 5 minutes per style

/** Hair accessory IDs — managed exclusively by the cycle timer, blocked from other systems */
const HAIR_ACCESSORIES = new Set(["clip_bangs", "low_twintails", "short_hair"]);

/** Log JS heap usage (Chrome only — no-op on Safari/Firefox) */
function logMemory(label: string) {
  try {
    const mem = (performance as any).memory;
    if (mem) {
      debugLog(`[Memory] ${label} — Used: ${(mem.usedJSHeapSize / 1024 / 1024).toFixed(1)}MB, Total: ${(mem.totalJSHeapSize / 1024 / 1024).toFixed(1)}MB`);
    }
  } catch {}
}

export default function Live2DAvatar({ isSpeaking, analyserNode, emotion, accessories, action, onModelReady, onLoadError }: Live2DAvatarProps) {
  const containerRef = useRef<HTMLDivElement>(null);
  const appRef = useRef<any>(null);
  const modelRef = useRef<any>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null); // explicit canvas ref for cleanup
  const glRef = useRef<WebGLRenderingContext | WebGL2RenderingContext | null>(null); // stored GL context — NEVER call getContext during cleanup
  const contextLostHandlerRef = useRef<((e: Event) => void) | null>(null); // stored for removal
  const animFrameRef = useRef<number>(0);
  const expressionTimeoutRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const activeAccessoriesRef = useRef<Set<string>>(new Set());
  const initializedRef = useRef(false);
  const modelStableRef = useRef(false);
  const modelStableTimer = useRef<ReturnType<typeof setTimeout> | null>(null);
  const pendingEmotion = useRef<string | null>(null);
  const pendingAccessories = useRef<string[]>([]);
  const actionTimeoutRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const activeActionRef = useRef<string | null>(null);
  const accessoryTimeoutRefs = useRef<Map<string, ReturnType<typeof setTimeout>>>(new Map());
  const webglCrashCount = useRef(0);
  const pixiCreatedAt = useRef(0); // timestamp for crash diagnostics
  const pixiResolutionRef = useRef(1); // store actual PIXI resolution for positioning math
  const baseScaleRef = useRef(0);
  const baseYRef = useRef(0);
  const lastPinchDistance = useRef<number | null>(null);
  const [zoomLevel, setZoomLevel] = useState(1.0);
  const zoomLevelRef = useRef(1.0);
  const onModelReadyRef = useRef(onModelReady);
  onModelReadyRef.current = onModelReady;
  const onLoadErrorRef = useRef(onLoadError);
  onLoadErrorRef.current = onLoadError;
  const [modelReady, setModelReady] = useState(false);

  // Lip sync engine — smooth mouth movement with attack/release dynamics
  const lipSyncEngine = useRef(new LipSyncEngine({
    attackSpeed: 0.45,    // Mouth opens quickly
    releaseSpeed: 0.10,   // Mouth closes gently (prevents snapping shut)
    noiseGate: 0.04,      // Ignore very quiet audio
    ampFloor: 0.06,       // Start opening mouth here
    ampCeiling: 0.45,     // Fully open at this amplitude
    maxMouthOpen: 0.8,    // Don't go above 80% open (looks more natural)
  }));

  // Hairstyle cycle — rotates through styles every 5 minutes
  const hairstyleIndexRef = useRef(Math.floor(Math.random() * HAIRSTYLE_CYCLE.length));
  const hairstyleCycleTimerRef = useRef<ReturnType<typeof setInterval> | null>(null);

  /**
   * Full GPU + memory cleanup — must release ALL resources to prevent the
   * "second conversation crash" on mobile (iOS limits WebGL to ~2 contexts).
   *
   * Order matters:
   *   1. Stop render loop (no more GPU draw calls)
   *   2. Destroy Live2D model (releases model buffers)
   *   3. Remove webglcontextlost listener (prevent closure leak)
   *   4. Destroy PIXI app *while context is still valid* (so it can delete
   *      textures / framebuffers via real WebGL calls)
   *   5. Lose WebGL context (forces the browser to free GPU memory)
   *   6. Remove canvas from DOM
   *   7. Clear PIXI global texture caches (module-level singletons that survive unmount)
   *   8. Reset refs
   */
  const cleanupLive2D = useRef(() => {
    debugLog("[Live2D] Starting full cleanup…");
    logMemory("before cleanup");

    // 1. Stop render loop
    if (animFrameRef.current) {
      cancelAnimationFrame(animFrameRef.current);
      animFrameRef.current = 0;
    }

    // Clear timers
    if (modelStableTimer.current) {
      clearTimeout(modelStableTimer.current);
      modelStableTimer.current = null;
    }
    if (expressionTimeoutRef.current) {
      clearTimeout(expressionTimeoutRef.current);
      expressionTimeoutRef.current = null;
    }
    if (hairstyleCycleTimerRef.current) {
      clearInterval(hairstyleCycleTimerRef.current);
      hairstyleCycleTimerRef.current = null;
    }

    // 2. Destroy the Live2D model (frees model buffers + child display objects)
    if (modelRef.current) {
      try {
        modelRef.current.destroy({ children: true });
        debugLog("[Live2D] Model destroyed");
      } catch (e) {
        console.warn("[Live2D] Model destroy error (may already be destroyed):", e);
      }
      modelRef.current = null;
    }

    // 3. Remove the webglcontextlost listener (its closure captures the PIXI app,
    //    preventing GC if left attached)
    if (canvasRef.current && contextLostHandlerRef.current) {
      canvasRef.current.removeEventListener("webglcontextlost", contextLostHandlerRef.current);
      contextLostHandlerRef.current = null;
    }

    // 4. Destroy the PIXI Application *before* losing the context —
    //    PIXI needs a live context to call deleteTexture / deleteBuffer etc.
    if (appRef.current) {
      try {
        appRef.current.ticker.stop();
      } catch {}
      try {
        appRef.current.destroy(true, {
          children: true,
          texture: true,
          baseTexture: true,
        });
        debugLog("[Live2D] PIXI app destroyed");
      } catch (e) {
        console.warn("[Live2D] PIXI app destroy error:", e);
      }
      appRef.current = null;
    }

    // 5. Explicitly lose the WebGL context using the STORED reference.
    //    NEVER call getContext() here — on iOS it can CREATE a new context
    //    (counting against the ~2-3 context limit) instead of returning the old one.
    if (glRef.current) {
      try {
        if (!glRef.current.isContextLost()) {
          const ext = glRef.current.getExtension("WEBGL_lose_context");
          if (ext) {
            ext.loseContext();
            debugLog("[Live2D] WebGL context explicitly released");
          }
        }
      } catch {}
      glRef.current = null;
    }

    // 6. Remove canvas from DOM (PIXI's destroy(true) should do this,
    //    but belt-and-suspenders for the context-loss path)
    if (canvasRef.current && canvasRef.current.parentNode) {
      canvasRef.current.parentNode.removeChild(canvasRef.current);
    }
    canvasRef.current = null;

    // 7. Flush PIXI's global texture caches — these are module-level Maps
    //    that survive component unmount and hold GPU texture references.
    try {
      const PIXI = (window as any).PIXI;
      if (PIXI) {
        if (PIXI.utils?.TextureCache) {
          for (const key in PIXI.utils.TextureCache) {
            try { PIXI.utils.TextureCache[key].destroy(true); } catch {}
          }
        }
        if (PIXI.utils?.BaseTextureCache) {
          for (const key in PIXI.utils.BaseTextureCache) {
            try { PIXI.utils.BaseTextureCache[key].destroy(); } catch {}
          }
        }
        debugLog("[Live2D] PIXI texture caches cleared");
      }
    } catch (e) {
      console.warn("[Live2D] Cache cleanup error:", e);
    }

    // 8. Reset all state refs
    initializedRef.current = false;
    modelStableRef.current = false;
    pendingEmotion.current = null;
    pendingAccessories.current = [];
    activeAccessoriesRef.current = new Set();
    setModelReady(false);

    // 9. Clear PIXI from window — the Live2D SDK reads from window.PIXI,
    //    and stale references from a previous instance can cause init failures.
    try {
      delete (window as any).PIXI;
    } catch {}

    logMemory("after cleanup");
    debugLog("[Live2D] Cleanup complete");
  });

  // Initialize PixiJS app + load model (runs once on mount)
  useEffect(() => {
    if (!containerRef.current || initializedRef.current) return;

    // Guard: destroy any orphaned PIXI app from a previous mount (React strict mode)
    if (appRef.current) {
      console.warn("[Live2D] PIXI app already exists — running full cleanup first");
      cleanupLive2D.current();
    }

    // Remove orphaned canvases from previous mounts (React strict mode).
    // Do NOT call getContext on them — on iOS that can CREATE a new context
    // and waste one of the ~2-3 available context slots.
    const container = containerRef.current;
    if (container) {
      const oldCanvases = container.querySelectorAll("canvas");
      oldCanvases.forEach(c => {
        c.remove();
        debugLog("[Live2D] Removed orphaned canvas");
      });
    }

    initializedRef.current = true;
    logMemory("before init");

    let destroyed = false;
    let loadTimeoutId: ReturnType<typeof setTimeout> | null = null;

    (async () => {
      const loadStart = performance.now();
      try {
        const MODEL_LOAD_TIMEOUT = 30000;

        // Race the entire init against a timeout
        const timeoutPromise = new Promise<never>((_, reject) => {
          loadTimeoutId = setTimeout(
            () => reject(new Error(`Model load timeout (${MODEL_LOAD_TIMEOUT}ms)`)),
            MODEL_LOAD_TIMEOUT
          );
        });

        await Promise.race([_initLive2D(), timeoutPromise]);
      } catch (err) {
        if (!destroyed) {
          console.error(`[Live2D] Initialization failed after ${(performance.now() - loadStart).toFixed(0)}ms:`, err);
          onLoadErrorRef.current?.();
        }
      } finally {
        if (loadTimeoutId) clearTimeout(loadTimeoutId);
      }

      async function _initLive2D() {
        // 1. Load Cubism Core SDK (required by pixi-live2d-display)
        await loadCubismCore();

        // 2. Dynamic imports to avoid SSR — pixi-live2d-display touches window/document
        const PIXI = await import("pixi.js");
        // Set PIXI on window BEFORE importing pixi-live2d-display
        (window as any).PIXI = PIXI;

        const { Live2DModel } = await import("pixi-live2d-display/cubism4");

        if (destroyed || !containerRef.current) return;

        // Detect mobile for GPU budget decisions
        const isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);
        // Full native resolution — no cap. iPhone 12 gets 3x, desktop whatever the display supports.
        const resolution = window.devicePixelRatio || 1;

        let app: InstanceType<typeof PIXI.Application>;
        try {
          app = new PIXI.Application({
            backgroundAlpha: 0,
            resizeTo: containerRef.current,
            resolution,
            autoDensity: true,
            antialias: !isMobile,
            powerPreference: isMobile ? "low-power" : "default",
          });
          containerRef.current.appendChild(app.view as unknown as HTMLCanvasElement);
        } catch (pixiErr) {
          console.error("[Live2D] Failed to create PIXI app:", pixiErr);
          onLoadErrorRef.current?.();
          return;
        }
        appRef.current = app;
        // Store canvas ref for cleanup (PIXI.view is the <canvas>)
        canvasRef.current = app.view as unknown as HTMLCanvasElement;
        pixiCreatedAt.current = Date.now();
        pixiResolutionRef.current = resolution;
        debugLog(`[Live2D] PIXI app created (resolution: ${resolution}, antialias: ${!isMobile})`);

        // Listen for WebGL context loss (iOS kills GPU context under memory pressure)
        const canvas = canvasRef.current!;
        // Store the GL context NOW — never call getContext again (iOS context limit)
        const gl = canvas.getContext("webgl2") || canvas.getContext("webgl");
        glRef.current = gl;
        if (gl) {
          // Log GPU memory budget if available (WEBGL_debug_renderer_info)
          try {
            const ext = gl.getExtension("WEBGL_debug_renderer_info");
            if (ext) {
              debugLog(`[Live2D] GPU: ${gl.getParameter(ext.UNMASKED_RENDERER_WEBGL)}`);
            }
          } catch {}
        }
        const handleContextLost = (e: Event) => {
          e.preventDefault();
          webglCrashCount.current++;
          const aliveSeconds = ((Date.now() - pixiCreatedAt.current) / 1000).toFixed(1);
          console.error(`[Live2D] WebGL context lost (crash #${webglCrashCount.current}) after ${aliveSeconds}s`);
          logMemory("at context loss");
          if (webglCrashCount.current >= 2) {
            console.error("[Live2D] Multiple WebGL crashes — staying on orb permanently");
          }
          // Stop the PIXI ticker to prevent further render attempts on a dead context
          try { appRef.current?.ticker.stop(); } catch {}
          cancelAnimationFrame(animFrameRef.current);
          onLoadErrorRef.current?.();
        };
        // Store handler ref so cleanup can remove it (prevents closure leak)
        contextLostHandlerRef.current = handleContextLost;
        canvas.addEventListener("webglcontextlost", handleContextLost);

        let model;
        try {
          const modelPath = isMobile
            ? "/worklets/models/Kira/kira.mobile.model3.json"
            : "/worklets/models/Kira/kira.model3.json";
          model = await Live2DModel.from(
            modelPath,
            { autoInteract: false }
          );
        } catch (modelErr) {
          console.error("[Live2D] Failed to load model:", modelErr);
          onLoadErrorRef.current?.();
          return;
        }

        if (destroyed) return;

        app.stage.addChild(model as any);

        // Framing: show head to mid-thigh, centered with breathing room
        // Use the actual PIXI resolution (not device DPR) to convert renderer pixels → CSS pixels
        const containerWidth = app.renderer.width / resolution;
        const containerHeight = app.renderer.height / resolution;

        const scaleFactor = 0.9;
        const scale = Math.min(
          containerWidth / model.width,
          containerHeight / model.height
        ) * scaleFactor;
        model.scale.set(scale);
        model.x = containerWidth / 2;
        const yPosition = containerHeight * (isMobile ? 0.59 : 0.56);
        model.y = yPosition;
        model.anchor.set(0.5, 0.5);

        // Store base positioning for zoom math
        baseScaleRef.current = scale;
        baseYRef.current = yPosition;

        // Eye tracking — eyes follow the cursor
        app.stage.interactive = true;
        app.stage.hitArea = app.renderer.screen;
        app.stage.on("pointermove", (e: any) => {
          model.focus(e.global.x, e.global.y);
        });

        modelRef.current = model;
        const loadMs = (performance.now() - loadStart).toFixed(0);
        debugLog(`[Live2D] Model loaded successfully in ${loadMs}ms`);

        // --- Per-frame parameter overrides ---
        // We patch at the coreModel.update() level, which is called INSIDE
        // internalModel.update() AFTER physics has run but BEFORE mesh deformation.
        // Pipeline: motions → physics (writes ears) → coreModel.update() → mesh deform
        // By injecting our values before calling origCoreUpdate(), mesh deformation
        // uses OUR values — physics can no longer overwrite them.
        try {
          const internalModel = model.internalModel;
          const coreModel = internalModel.coreModel as any;

          // Log all param IDs once for future debugging
          try {
            const rawModel = coreModel._model;
            if (rawModel && rawModel.parameters) {
              const allKeys: string[] = [];
              for (let i = 0; i < rawModel.parameters.count; i++) {
                allKeys.push(rawModel.parameters.ids[i]);
              }
              debugLog("[Live2D] ALL PARAMS:", JSON.stringify(allKeys));
            }
          } catch {}

          // Patch coreModel.update — called INSIDE internalModel.update,
          // AFTER physics but the original does mesh deformation.
          const origCoreUpdate = coreModel.update.bind(coreModel);
          let frameCount = 0;

          coreModel.update = function () {
            frameCount++;
            const t = frameCount / 60;

            if (frameCount % 3600 === 1) {
              debugLog(`[Live2D] Per-frame patch running (frame ${frameCount})`);
            }

            try {
              // --- Watermark hide ---
              coreModel.setParameterValueById("Param155", 1);

              // --- Force ears + tail visible ---
              coreModel.setParameterValueById("Param157", 0);

              // --- Breathing (gentle sine wave, ~4.2s cycle) ---
              const breath = (Math.sin(t * 1.5) + 1) * 0.5;
              coreModel.setParameterValueById("ParamBreath", breath);

              // --- Idle body sway (subtle, organic) ---
              const bodyX = Math.sin(t * 0.4) * 2 + Math.sin(t * 0.7) * 1;
              const bodyY = Math.sin(t * 0.3) * 1.5;
              coreModel.setParameterValueById("ParamAngle15", bodyX);
              coreModel.setParameterValueById("ParamAngle16", bodyY);

              // --- Head micro-movements ---
              const headX = Math.sin(t * 0.5) * 1.5 + Math.sin(t * 1.1) * 0.5;
              const headY = Math.sin(t * 0.35) * 1 + Math.cos(t * 0.8) * 0.5;
              const headZ = Math.sin(t * 0.25) * 1;
              coreModel.setParameterValueById("ParamAngle7", headX);
              coreModel.setParameterValueById("ParamAngle8", headY);
              coreModel.setParameterValueById("ParamAngle9", headZ);

              // --- Ear animation (AFTER physics, BEFORE mesh deformation) ---
              // Physics writes to Param68-77 based on eye blinks, but we override
              // with our own animation here. These params MUST be set inside
              // coreModel.update (before origCoreUpdate) to survive the
              // physics→deformation pipeline.
              const earBase = Math.sin(t * 2.0) * 0.8;
              const earSlow = Math.sin(t * 0.8) * 1.5;
              const earBreath = breath * 0.5;

              // Right ear
              coreModel.setParameterValueById("Param68", earSlow + earBase + earBreath);
              coreModel.setParameterValueById("Param69", earBase * 0.7 + Math.sin(t * 2.3) * 0.5);
              coreModel.setParameterValueById("Param70", earSlow * 0.5 + Math.sin(t * 1.7) * 0.4);
              coreModel.setParameterValueById("Param74", earBase * 0.5);
              coreModel.setParameterValueById("Param75", Math.sin(t * 1.5) * 0.5);

              // Left ear (slightly offset phase for organic asymmetry)
              coreModel.setParameterValueById("Param71", earSlow + earBase * 0.9 + earBreath);
              coreModel.setParameterValueById("Param72", earBase * 0.6 + Math.sin(t * 2.5) * 0.5);
              coreModel.setParameterValueById("Param73", earSlow * 0.5 + Math.sin(t * 1.9) * 0.4);
              coreModel.setParameterValueById("Param76", earBase * 0.4);
              coreModel.setParameterValueById("Param77", Math.sin(t * 1.3) * 0.5);
            } catch {}

            // NOW do mesh deformation with our values applied
            origCoreUpdate();
          };

          debugLog("[Live2D] Per-frame patch applied (coreModel.update pre-deformation override)");
        } catch (err2) {
          console.warn("[Live2D] Could not patch per-frame update:", err2);
        }

        // Wait 2 frames for the watermark parameter to take effect before showing
        requestAnimationFrame(() => {
          requestAnimationFrame(() => {
            if (!destroyed) {
              setModelReady(true);
              onModelReadyRef.current?.();
              debugLog(`[Live2D] Model ready — revealing (total ${(performance.now() - loadStart).toFixed(0)}ms)`);

              // Delay expressions/accessories for 2s to let GPU settle
              modelStableTimer.current = setTimeout(() => {
                modelStableRef.current = true;
                debugLog("[Live2D] Model stable — expressions/accessories enabled");

                // Flush any queued emotion
                if (pendingEmotion.current && modelRef.current) {
                  const expr = pendingEmotion.current;
                  pendingEmotion.current = null;
                  try {
                    const mapped = EMOTION_MAP_STATIC[expr];
                    if (mapped) {
                      modelRef.current.expression(mapped);
                      debugLog(`[Live2D] Flushed queued expression: ${mapped}`);
                    }
                  } catch {}
                }

                // Flush any queued accessories
                if (pendingAccessories.current.length > 0 && modelRef.current) {
                  pendingAccessories.current.forEach(acc => {
                    try {
                      modelRef.current.expression(acc);
                      activeAccessoriesRef.current.add(acc);
                      debugLog(`[Live2D] Flushed queued accessory: ${acc}`);
                    } catch {}
                  });
                  pendingAccessories.current = [];
                }

                // Apply random initial hairstyle NOW — model is confirmed stable
                if (modelRef.current) {
                  const initialStyle = HAIRSTYLE_CYCLE[hairstyleIndexRef.current];
                  if (initialStyle) {
                    try {
                      modelRef.current.expression(initialStyle);
                      activeAccessoriesRef.current.add(initialStyle);
                    } catch (e) {
                      console.warn(`[Hair] Failed to apply initial: ${initialStyle}`, e);
                    }
                  }
                  debugLog(`[Hair] Initial style (random): ${initialStyle ?? "default"} (index ${hairstyleIndexRef.current})`);

                  // Start 5-minute cycle timer
                  hairstyleCycleTimerRef.current = setInterval(() => {
                    hairstyleIndexRef.current = (hairstyleIndexRef.current + 1) % HAIRSTYLE_CYCLE.length;
                    const nextStyle = HAIRSTYLE_CYCLE[hairstyleIndexRef.current];
                    // Remove previous hair accessory
                    HAIR_ACCESSORIES.forEach(hair => activeAccessoriesRef.current.delete(hair));
                    if (nextStyle && modelRef.current) {
                      try {
                        modelRef.current.expression(nextStyle);
                        activeAccessoriesRef.current.add(nextStyle);
                      } catch (e) {
                        console.warn(`[Hair] Failed to apply: ${nextStyle}`, e);
                      }
                    }
                    debugLog(`[Hair] Cycled to: ${nextStyle ?? "default"} (index ${hairstyleIndexRef.current})`);
                  }, HAIRSTYLE_CYCLE_INTERVAL);
                }
              }, 2000);
            }
          });
        });
      } // end _initLive2D
    })();

    // Handle resize
    const handleResize = () => {
      if (appRef.current?.renderer && containerRef.current) {
        appRef.current.renderer.resize(
          containerRef.current.clientWidth,
          containerRef.current.clientHeight
        );
        // Re-center model on resize
        if (modelRef.current) {
          const res = pixiResolutionRef.current;
          const w = appRef.current.renderer.width / res;
          const h = appRef.current.renderer.height / res;

          // Recalculate base scale from the model's intrinsic size
          // (model.width/height already factor in scale, so divide it out first)
          const currentScale = modelRef.current.scale.x || 1;
          const rawWidth = modelRef.current.width / currentScale;
          const rawHeight = modelRef.current.height / currentScale;
          const mobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);
          const sf = 0.9;
          const newBaseScale = Math.min(w / rawWidth, h / rawHeight) * sf;
          baseScaleRef.current = newBaseScale;
          baseYRef.current = h * (mobile ? 0.59 : 0.56);

          const z = zoomLevelRef.current;
          modelRef.current.scale.set(newBaseScale * z);
          modelRef.current.x = w / 2;
          modelRef.current.y = baseYRef.current + (z - 1.0) * modelRef.current.height * 0.35;
        }
      }
    };
    window.addEventListener("resize", handleResize);

    return () => {
      destroyed = true;
      window.removeEventListener("resize", handleResize);
      cleanupLive2D.current();
    };
  }, []);

  // Safety net: release GPU resources if the page is being unloaded (tab close,
  // hard navigation, etc.) — React's unmount may not fire in time on mobile Safari.
  useEffect(() => {
    const handleBeforeUnload = () => {
      cleanupLive2D.current();
    };
    window.addEventListener("beforeunload", handleBeforeUnload);
    return () => {
      window.removeEventListener("beforeunload", handleBeforeUnload);
    };
  }, []);

  // Lip sync via LipSyncEngine — smooth attack/release dynamics
  useEffect(() => {
    const model = modelRef.current;
    if (!model) return;

    // Helper: extract RMS amplitude from analyser (speech frequency bins only)
    function getAmplitude(): number {
      if (!analyserNode) return 0;
      const dataArray = new Uint8Array(analyserNode.frequencyBinCount);
      analyserNode.getByteFrequencyData(dataArray);

      // Use lower frequency bins (speech is mostly 85Hz–1kHz)
      // With fftSize=256 at 16kHz sample rate, each bin ≈ 62.5Hz
      // Bins 1-6 ≈ 62–375Hz (fundamental speech range)
      const speechBins = Math.min(40, dataArray.length);
      let sum = 0;
      for (let i = 1; i < speechBins; i++) {
        const normalized = dataArray[i] / 255;
        sum += normalized * normalized;
      }
      return Math.sqrt(sum / (speechBins - 1)); // RMS, 0.0–1.0
    }

    function animateMouth() {
      if (!modelRef.current) return;

      const now = performance.now();
      let smoothMouth: number;

      if (isSpeaking && analyserNode) {
        // Speaking — feed real amplitude through the engine
        const rawAmplitude = getAmplitude();
        smoothMouth = lipSyncEngine.current.update(rawAmplitude, now);
      } else {
        // Not speaking — feed zero so mouth closes via release curve (no snap)
        smoothMouth = lipSyncEngine.current.update(0, now);
      }

      try {
        const core = modelRef.current.internalModel?.coreModel;
        if (core) {
          core.setParameterValueById("ParamMouthOpenY", smoothMouth);
          core.setParameterValueById("ParamMouthForm", 0.15 + smoothMouth * 0.2);

          // Subtle head movement while speaking — nods with speech rhythm
          const speakNod = smoothMouth * 2; // 0 to 2 degrees based on mouth open
          const speakSway = Math.sin(Date.now() / 400) * smoothMouth * 1.5; // Gentle side-to-side
          core.setParameterValueById("ParamAngle8", speakNod);
          core.setParameterValueById("ParamAngle9", speakSway);
        }
      } catch {}

      // Keep running even when not speaking — release curve needs frames to close smoothly
      // Stop only when fully closed and not speaking
      if (!isSpeaking && smoothMouth === 0) {
        animFrameRef.current = 0;
        return;
      }

      animFrameRef.current = requestAnimationFrame(animateMouth);
    }

    // Start the animation loop
    animateMouth();

    return () => {
      if (animFrameRef.current) {
        cancelAnimationFrame(animFrameRef.current);
        animFrameRef.current = 0;
      }
    };
  }, [isSpeaking, analyserNode]);

  // Use module-level emotion map
  const EMOTION_MAP = EMOTION_MAP_STATIC;

  /**
   * Properly reset Live2D expressions by clearing the expression manager state.
   * model.expression() with no args cycles to the NEXT expression in the list
   * (triggering random accessory expressions), so we must clear explicitly.
   */
  function resetExpression(model: any) {
    try {
      const mgr = model.internalModel?.motionManager?.expressionManager;
      if (mgr) {
        // Clear the currently playing expression
        if (mgr._expressions) {
          mgr._expressions.forEach((expr: any) => {
            if (expr && typeof expr.weight !== "undefined") {
              expr.weight = 0;
            }
          });
        }
        // Null out the tracked current expression
        if ("_currentExpression" in mgr) mgr._currentExpression = null;
        if ("currentExpression" in mgr) mgr.currentExpression = null;
        if ("_expressionIndex" in mgr) mgr._expressionIndex = -1;
        if ("expressionIndex" in mgr) mgr.expressionIndex = -1;
        debugLog("[Live2D] Expression cleared via manager");
      }
    } catch (err) {
      console.warn("[Live2D] Failed to reset expression:", err);
    }

    // Re-apply active accessories (they must persist through emotion resets)
    Array.from(activeAccessoriesRef.current).forEach(acc => {
      try {
        model.expression(acc);
      } catch (err) {
        // ignore — accessory may not exist
      }
    });
  }

  // Watch for expression changes
  useEffect(() => {
    const model = modelRef.current;
    if (!model || !emotion) return;

    // Queue if model not yet stable (prevents WebGL crash from early expression)
    if (!modelStableRef.current) {
      debugLog(`[Live2D] Queuing emotion — model not yet stable: ${emotion}`);
      pendingEmotion.current = emotion;
      return;
    }

    // Clear any pending reset
    if (expressionTimeoutRef.current) {
      clearTimeout(expressionTimeoutRef.current);
      expressionTimeoutRef.current = null;
    }

    const expressionName = EMOTION_MAP[emotion];

    if (expressionName) {
      // Trigger the expression
      try {
        model.expression(expressionName);
        debugLog(`[Live2D] Expression: ${expressionName} (emotion: ${emotion})`);
      } catch (err) {
        console.warn(`[Live2D] Failed to set expression: ${expressionName}`, err);
      }

      // Apply emotion-specific parameter overrides (brows, head tilt)
      const paramOverrides = EMOTION_PARAMS[emotion];
      if (paramOverrides) {
        try {
          const core = model.internalModel?.coreModel;
          if (core) {
            for (const [param, value] of Object.entries(paramOverrides)) {
              core.setParameterValueById(param, value);
            }
            debugLog(`[Live2D] Emotion params applied: ${Object.keys(paramOverrides).join(", ")}`);
          }
        } catch (err) {
          console.warn("[Live2D] Failed to set emotion params:", err);
        }
      }

      // Auto-reset to neutral after 4 seconds
      expressionTimeoutRef.current = setTimeout(() => {
        resetExpression(model);
        // Reset param overrides to defaults
        try {
          const core = model.internalModel?.coreModel;
          if (core) {
            core.setParameterValueById("ParamBrowLY", 0);
            core.setParameterValueById("ParamBrowRY", 0);
            core.setParameterValueById("ParamBrowLForm", 0);
            core.setParameterValueById("ParamBrowRForm", 0);
            core.setParameterValueById("ParamEyeRSmile", 0);
            core.setParameterValueById("ParamEyeLSmile", 0);
          }
        } catch {}
      }, 4000);
    } else {
      // neutral/happy — clear any active expression
      resetExpression(model);
      // Apply happy params if happy (subtle difference from neutral)
      if (emotion === "happy") {
        const paramOverrides = EMOTION_PARAMS.happy;
        try {
          const core = model.internalModel?.coreModel;
          if (core && paramOverrides) {
            for (const [param, value] of Object.entries(paramOverrides)) {
              core.setParameterValueById(param, value);
            }
          }
        } catch {}
      }
    }
  }, [emotion]);

  // Watch for accessory changes — accessories persist (unlike emotions which flash)
  // Includes conflict resolution (e.g. headphones replaces earbuds) and 60s auto-removal
  useEffect(() => {
    const model = modelRef.current;
    if (!model) return;

    // Queue if model not yet stable
    if (!modelStableRef.current && accessories) {
      const newItems = accessories.filter(a => !activeAccessoriesRef.current.has(a));
      if (newItems.length > 0) {
        debugLog(`[Live2D] Queuing accessories — model not yet stable: ${newItems.join(", ")}`);
        pendingAccessories.current.push(...newItems);
      }
      return;
    }

    if (accessories) {
      const newSet = new Set(accessories);

      // Turn ON new accessories (with conflict resolution)
      Array.from(newSet).forEach(acc => {
        if (!activeAccessoriesRef.current.has(acc)) {
          // Remove conflicting accessories first
          const conflicts = ACCESSORY_CONFLICTS[acc];
          if (conflicts) {
            conflicts.forEach(conflicting => {
              if (activeAccessoriesRef.current.has(conflicting)) {
                try {
                  // Reset expression to clear it, then re-apply non-conflicting ones
                  activeAccessoriesRef.current.delete(conflicting);
                  debugLog(`[Live2D] Accessory conflict — removed ${conflicting} for ${acc}`);
                  // Clear its auto-remove timer
                  const timer = accessoryTimeoutRefs.current.get(conflicting);
                  if (timer) {
                    clearTimeout(timer);
                    accessoryTimeoutRefs.current.delete(conflicting);
                  }
                } catch {}
              }
            });
          }

          try {
            model.expression(acc);
            debugLog(`[Live2D] Accessory ON: ${acc}`);
          } catch (err) {
            console.warn(`[Live2D] Failed to apply accessory: ${acc}`, err);
          }

          // Auto-remove after 60 seconds
          const timer = setTimeout(() => {
            activeAccessoriesRef.current.delete(acc);
            accessoryTimeoutRefs.current.delete(acc);
            debugLog(`[Live2D] Accessory auto-removed (60s): ${acc}`);
          }, 60000);
          accessoryTimeoutRefs.current.set(acc, timer);
        }
      });

      activeAccessoriesRef.current = newSet;
    }
  }, [accessories]);

  // Watch for action changes — actions are temporary (auto-clear after 10s)
  useEffect(() => {
    const model = modelRef.current;
    if (!model || !action) return;
    if (!modelStableRef.current) return;

    // Remove previous action's visual
    if (activeActionRef.current) {
      try {
        // Re-apply active accessories after clearing
        Array.from(activeAccessoriesRef.current).forEach(acc => {
          try { model.expression(acc); } catch {}
        });
      } catch {}
      activeActionRef.current = null;
    }

    // Clear pending timeout
    if (actionTimeoutRef.current) {
      clearTimeout(actionTimeoutRef.current);
    }

    // Apply new action
    try {
      model.expression(action);
      activeActionRef.current = action;
      debugLog(`[Live2D] Action ON: ${action}`);
    } catch (err) {
      console.warn(`[Live2D] Failed to apply action: ${action}`, err);
    }

    // Auto-remove after 10 seconds
    actionTimeoutRef.current = setTimeout(() => {
      try {
        // Re-apply accessories to clear the action expression
        Array.from(activeAccessoriesRef.current).forEach(acc => {
          try { model.expression(acc); } catch {}
        });
        activeActionRef.current = null;
        debugLog(`[Live2D] Action OFF: ${action}`);
      } catch {}
    }, 10000);

    return () => {
      if (actionTimeoutRef.current) clearTimeout(actionTimeoutRef.current);
    };
  }, [action]);

  // Clean up expression/action/accessory timeouts on unmount
  useEffect(() => {
    return () => {
      if (expressionTimeoutRef.current) {
        clearTimeout(expressionTimeoutRef.current);
      }
      if (actionTimeoutRef.current) {
        clearTimeout(actionTimeoutRef.current);
      }
      accessoryTimeoutRefs.current.forEach(timer => clearTimeout(timer));
      accessoryTimeoutRefs.current.clear();
    };
  }, []);

  // Zoom: scroll wheel (desktop) + pinch (mobile)
  useEffect(() => {
    const container = containerRef.current;
    if (!container) return;

    const MIN_ZOOM = 1.0;
    const MAX_ZOOM = 2.0;
    const ZOOM_STEP = 0.1;

    const handleWheel = (e: WheelEvent) => {
      e.preventDefault();
      setZoomLevel(prev => {
        const next = prev + (e.deltaY < 0 ? ZOOM_STEP : -ZOOM_STEP);
        return Math.min(MAX_ZOOM, Math.max(MIN_ZOOM, Math.round(next * 100) / 100));
      });
    };

    const handleTouchMove = (e: TouchEvent) => {
      if (e.touches.length !== 2) return;
      e.preventDefault();

      const dx = e.touches[0].clientX - e.touches[1].clientX;
      const dy = e.touches[0].clientY - e.touches[1].clientY;
      const distance = Math.sqrt(dx * dx + dy * dy);

      if (lastPinchDistance.current !== null) {
        const delta = distance - lastPinchDistance.current;
        setZoomLevel(prev => {
          const next = prev + delta * 0.005;
          return Math.min(MAX_ZOOM, Math.max(MIN_ZOOM, Math.round(next * 100) / 100));
        });
      }
      lastPinchDistance.current = distance;
    };

    const handleTouchEnd = () => {
      lastPinchDistance.current = null;
    };

    container.addEventListener("wheel", handleWheel, { passive: false });
    container.addEventListener("touchmove", handleTouchMove, { passive: false });
    container.addEventListener("touchend", handleTouchEnd);
    container.addEventListener("touchcancel", handleTouchEnd);

    return () => {
      container.removeEventListener("wheel", handleWheel);
      container.removeEventListener("touchmove", handleTouchMove);
      container.removeEventListener("touchend", handleTouchEnd);
      container.removeEventListener("touchcancel", handleTouchEnd);
    };
  }, []);

  // Apply zoom to model — scale up + shift down to keep face centered
  useEffect(() => {
    zoomLevelRef.current = zoomLevel;
    const model = modelRef.current;
    if (!model || !baseScaleRef.current) return;

    model.scale.set(baseScaleRef.current * zoomLevel);
    const yOffset = (zoomLevel - 1.0) * model.height * 0.35;
    model.y = baseYRef.current + yOffset;
  }, [zoomLevel]);

  return (
    <div style={{ width: "100%", height: "100%", maxWidth: "600px", maxHeight: "90vh", margin: "0 auto", position: "relative", overflow: "hidden" }}>
      <div
        ref={containerRef}
        style={{
          width: "100%",
          height: "100%",
          pointerEvents: "auto",
          overflow: "hidden",
          opacity: modelReady ? 1 : 0,
          transition: "opacity 0.3s ease-in",
        }}
      />
    </div>
  );
}


===== packages/web/src/app/(chat)/chat/[conversationId]/ChatClient.tsx =====
"use client";

import { useAuth, useClerk } from "@clerk/nextjs";
import { useCallback, useEffect, useRef, useState } from "react";
import { useKiraSocket, debugLog } from "@/hooks/useKiraSocket";
import { PhoneOff, Star, User, Mic, MicOff, Eye, EyeOff, Clock, Sparkles, Camera } from "lucide-react";
import ProfileModal from "@/components/ProfileModal";
import KiraOrb from "@/components/KiraOrb";
import { getOrCreateGuestId } from "@/lib/guestId";
import { getVoicePreference, setVoicePreference, VoicePreference } from "@/lib/voicePreference";
import { KiraLogo } from "@/components/KiraLogo";
import dynamic from "next/dynamic";

const Live2DAvatar = dynamic(() => import("@/components/Live2DAvatar"), { ssr: false });
const XOLoader = dynamic(() => import("@/components/XOLoader"), { ssr: false });

export default function ChatClient() {
  const { getToken, userId, isLoaded: clerkLoaded } = useAuth();
  const { openSignIn } = useClerk();
  const [showRatingModal, setShowRatingModal] = useState(false);
  const hasShownRating = useRef(false); // Prevent rating dialog from showing twice
  const [showProfileModal, setShowProfileModal] = useState(false);
  const [rating, setRating] = useState(0);
  const [hoverRating, setHoverRating] = useState(0);
  const [guestId, setGuestId] = useState("");
  const [voicePreference, setVoicePref] = useState<VoicePreference>("anime");
  const [visualMode, setVisualMode] = useState<"avatar" | "orb">("avatar");
  const [live2dReady, setLive2dReady] = useState(false);
  const [live2dFailed, setLive2dFailed] = useState(false);
  const [live2dDismissed, setLive2dDismissed] = useState(false); // set true before WS close to clean up PIXI first
  const isDisconnectingRef = useRef(false); // prevents orb fallback flash during clean shutdown
  const [isMobile, setIsMobile] = useState(false);
  const [deviceDetected, setDeviceDetected] = useState(false);
  const live2dRetryCount = useRef(0);
  const MAX_LIVE2D_RETRIES = 1;

  // --- Debug: track mount/unmount and what triggers remount ---
  useEffect(() => {
    debugLog("[ChatClient] MOUNTED. URL:", window.location.href, "userId:", userId, "clerkLoaded:", clerkLoaded);
    return () => {
      debugLog("[ChatClient] UNMOUNTING. URL:", window.location.href);
    };
  // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  // --- Debug: track Clerk auth state changes (userId flipping can cause subtree re-renders) ---
  const prevUserId = useRef(userId);
  useEffect(() => {
    if (prevUserId.current !== userId) {
      debugLog("[ChatClient] userId changed:", prevUserId.current, "→", userId);
      prevUserId.current = userId;
    }
  }, [userId]);

  // --- Debug: track Clerk isLoaded change ---
  useEffect(() => {
    debugLog("[ChatClient] clerkLoaded changed to:", clerkLoaded, "userId:", userId);
  }, [clerkLoaded, userId]);

  useEffect(() => {
    const checkMobile = () => {
      const mobile =
        /iPhone|iPad|iPod|Android/i.test(navigator.userAgent) ||
        (navigator.maxTouchPoints > 0 && window.innerWidth < 768);
      setIsMobile(mobile);
      setDeviceDetected(true);
    };
    checkMobile();
    window.addEventListener("resize", checkMobile);

    // Fallback re-check: guarantee detection even if the initial check raced
    const fallback = setTimeout(checkMobile, 2000);

    return () => {
      window.removeEventListener("resize", checkMobile);
      clearTimeout(fallback);
    };  
  }, []);

  // If Live2D fails to load (e.g. mobile GPU limits), auto-switch to orb
  // Skip fallback during clean disconnect — just let the component unmount
  useEffect(() => {
    if (live2dFailed && visualMode === "avatar" && !isDisconnectingRef.current) {
      setVisualMode("orb");
      debugLog("[UI] Live2D failed — falling back to orb mode");
    }
  }, [live2dFailed, visualMode]);

  // Load guest ID, voice preference, and chat toggle from localStorage
  useEffect(() => {
    if (!userId) {
      setGuestId(getOrCreateGuestId());
    }
    setVoicePref(getVoicePreference());
  }, [userId]);

  const { 
    connect, 
    disconnect,
    signalVisualReady,
    socketState, 
    kiraState, 
    micVolume, 
    error,
    sendVoiceChange,
    isAudioBlocked, 
    resumeAudio,
    isMuted,
    toggleMute,
    isScreenSharing,
    startScreenShare,
    stopScreenShare,
    isCameraActive,
    cameraStreamRef,
    facingMode,
    startCamera,
    stopCamera,
    flipCamera,
    isPro,
    remainingSeconds,
    isAudioPlaying,
    playerVolume,
    playbackAnalyserNode,
    currentExpression,
    activeAccessories,
    currentAction
  } = useKiraSocket(
    userId ? getToken : null,
    guestId,
    voicePreference
  );

  // Orb mode is always "visually ready" — signal immediately so start_stream isn't blocked
  useEffect(() => {
    if (visualMode === "orb") {
      signalVisualReady();
    }
  }, [visualMode, signalVisualReady]);

  // ─── Camera PIP preview ───
  const previewVideoRef = useRef<HTMLVideoElement>(null);
  const [pipPosition, setPipPosition] = useState({ x: 16, y: 140 }); // offset from bottom-right
  const pipDragRef = useRef<{ startX: number; startY: number; origX: number; origY: number } | null>(null);

  // Attach stream to video element whenever camera becomes active
  useEffect(() => {
    if (!isCameraActive) {
      // Reset PIP position when camera stops
      setPipPosition({ x: 16, y: 140 });
      return;
    }
    const vid = previewVideoRef.current;
    const stream = cameraStreamRef.current;
    if (vid && stream) {
      vid.srcObject = stream;
      vid.setAttribute("playsinline", "true");
      vid.muted = true;
      vid.play().catch(() => {});
    }
  }, [isCameraActive, cameraStreamRef]);

  const handlePipTouchStart = useCallback((e: React.TouchEvent) => {
    const touch = e.touches[0];
    pipDragRef.current = {
      startX: touch.clientX,
      startY: touch.clientY,
      origX: pipPosition.x,
      origY: pipPosition.y,
    };
  }, [pipPosition]);

  const handlePipTouchMove = useCallback((e: React.TouchEvent) => {
    if (!pipDragRef.current) return;
    const touch = e.touches[0];
    const dx = touch.clientX - pipDragRef.current.startX;
    const dy = touch.clientY - pipDragRef.current.startY;
    setPipPosition({
      x: pipDragRef.current.origX - dx, // inverted because offset is from right
      y: pipDragRef.current.origY + dy,  // inverted because offset is from bottom
    });
  }, []);

  const handlePipTouchEnd = useCallback(() => {
    pipDragRef.current = null;
  }, []);

  // ─── start_stream is now sent atomically in the hook's onopen handler ───
  // No more useEffect race — connect() → WS open → start_stream → audio pipeline
  // all happen in the same call stack, immune to React remounts.

  // ─── DO NOT disconnect on unmount ───
  // React can remount this component at any time (Clerk auth, Next.js RSC, etc.).
  // The WebSocket lives in a module-level singleton and survives remounts.
  // Only handleEndCall() → disconnect() closes the WS intentionally.
  useEffect(() => {
    return () => {
      debugLog("[ChatClient] Unmount cleanup — WS stays alive in singleton");
      // Just clean up Live2D visuals, don't touch the WebSocket
      isDisconnectingRef.current = true;
      setLive2dDismissed(true);
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  // --- UI Logic ---

  const handleEndCall = async () => {
    // 1. Mark disconnecting to prevent orb fallback flash
    isDisconnectingRef.current = true;
    // 2. Unmount Live2D first so PIXI can destroy its WebGL context cleanly
    setLive2dDismissed(true);
    // 3. Give the browser time to flush the React unmount + release GPU memory.
    //    500ms is generous but mobile browsers need it to actually free VRAM
    //    after WEBGL_lose_context before a new context can be created.
    await new Promise(r => setTimeout(r, 500));
    // 4. Then close WebSocket
    disconnect();
    if (!hasShownRating.current) {
      hasShownRating.current = true;
      setShowRatingModal(true);
    }
  };

  const handleRate = () => {
    // TODO: Save rating to backend
    debugLog("User rated conversation:", rating);
    setShowRatingModal(false);
    window.location.href = "/"; // Hard nav to guarantee WebGL cleanup
  };

  const handleContinue = () => {
    setShowRatingModal(false);
    window.location.href = "/"; // Hard nav to guarantee WebGL cleanup
  };

  const handleUpgrade = async () => {
    try {
      const res = await fetch("/api/stripe/checkout", { method: "POST" });
      if (res.ok) {
        const data = await res.json();
        window.location.href = data.url;
      } else {
        console.error("Failed to start checkout");
      }
    } catch (error) {
      console.error("Checkout error:", error);
    }
  };

  const isGuest = !userId;

  const handleSignUp = () => {
    // Pass guestId via unsafe_metadata so the Clerk webhook can migrate the conversation
    openSignIn({
      afterSignInUrl: window.location.href,
      afterSignUpUrl: window.location.href,
    });
    // Note: guestId is preserved in localStorage — on next connect as signed-in user,
    // the webhook will have already migrated the buffer
  };

  // --- Local countdown for time remaining ---
  const [localRemaining, setLocalRemaining] = useState<number | null>(null);

  // Sync from server when session_config arrives
  useEffect(() => {
    if (remainingSeconds !== null) {
      setLocalRemaining(remainingSeconds);
    }
  }, [remainingSeconds]);

  // Tick down every second while connected
  useEffect(() => {
    if (socketState !== "connected" || localRemaining === null) return;
    const interval = setInterval(() => {
      setLocalRemaining((prev) => (prev !== null && prev > 0 ? prev - 1 : 0));
    }, 1000);
    return () => clearInterval(interval);
  }, [socketState, localRemaining !== null]);

  // Dump persistent debug logs from sessionStorage (survives page reloads)
  useEffect(() => {
    if (socketState === "idle") {
      try {
        const raw = sessionStorage.getItem('kira-debug');
        if (raw) {
          const logs = JSON.parse(raw) as string[];
          debugLog(`%c[DebugDump] ${logs.length} stored logs:`, 'color: orange; font-weight: bold');
          logs.forEach((l) => debugLog(l));
        }
      } catch {}
    }
  }, [socketState]);

  // Start Screen (Initial State for ALL users)
  if (socketState === "idle") {
    return (
      <div style={{
        minHeight: "100vh",
        background: "#0D1117",
        display: "flex",
        flexDirection: "column",
        alignItems: "center",
        justifyContent: "center",
        fontFamily: "'DM Sans', -apple-system, BlinkMacSystemFont, sans-serif",
        padding: "24px",
        textAlign: "center",
        position: "relative",
      }}>
        {/* Subtle ambient glow */}
        <div style={{
          position: "absolute",
          top: "40%",
          left: "50%",
          transform: "translate(-50%, -50%)",
          width: 400,
          height: 400,
          borderRadius: "50%",
          background: "radial-gradient(circle, rgba(107,125,179,0.05) 0%, transparent 70%)",
          pointerEvents: "none",
        }} />

        {/* Mic icon badge */}
        <div style={{
          width: 64,
          height: 64,
          borderRadius: 16,
          background: "linear-gradient(135deg, rgba(107,125,179,0.12), rgba(107,125,179,0.04))",
          border: "1px solid rgba(107,125,179,0.15)",
          display: "flex",
          alignItems: "center",
          justifyContent: "center",
          marginBottom: 28,
          position: "relative",
        }}>
          <svg width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="rgba(139,157,195,0.7)" strokeWidth="1.8" strokeLinecap="round" strokeLinejoin="round">
            <path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z" />
            <path d="M19 10v2a7 7 0 0 1-14 0v-2" />
            <line x1="12" y1="19" x2="12" y2="22" />
          </svg>
        </div>

        <h2 style={{
          fontSize: 22,
          fontFamily: "'Playfair Display', serif",
          fontWeight: 400,
          color: "#E2E8F0",
          marginBottom: 10,
          marginTop: 0,
          position: "relative",
        }}>
          Enable your microphone
        </h2>

        <p style={{
          fontSize: 15,
          fontWeight: 300,
          color: "rgba(201,209,217,0.45)",
          lineHeight: 1.6,
          maxWidth: 340,
          marginBottom: 32,
          position: "relative",
        }}>
          Kira needs microphone access to hear you.
          Your audio is never stored or recorded.
        </p>

        <button
          onClick={() => { debugLog("[Chat] 🎤 Connect button tapped"); connect(); }}
          style={{
            display: "flex",
            alignItems: "center",
            gap: 10,
            padding: "14px 36px",
            borderRadius: 12,
            background: "linear-gradient(135deg, rgba(107,125,179,0.2), rgba(107,125,179,0.08))",
            border: "1px solid rgba(107,125,179,0.25)",
            color: "#C9D1D9",
            fontSize: 15,
            fontWeight: 500,
            cursor: "pointer",
            transition: "all 0.3s ease",
            fontFamily: "'DM Sans', sans-serif",
            boxShadow: "0 0 30px rgba(107,125,179,0.08)",
            position: "relative",
          }}
          onMouseEnter={(e) => {
            e.currentTarget.style.background = "linear-gradient(135deg, rgba(107,125,179,0.3), rgba(107,125,179,0.15))";
            e.currentTarget.style.boxShadow = "0 0 40px rgba(107,125,179,0.15)";
            e.currentTarget.style.transform = "translateY(-1px)";
          }}
          onMouseLeave={(e) => {
            e.currentTarget.style.background = "linear-gradient(135deg, rgba(107,125,179,0.2), rgba(107,125,179,0.08))";
            e.currentTarget.style.boxShadow = "0 0 30px rgba(107,125,179,0.08)";
            e.currentTarget.style.transform = "translateY(0)";
          }}
        >
          Allow microphone
        </button>
      </div>
    );
  }

  return (
    <div style={{ background: "#0D1117", fontFamily: "'DM Sans', -apple-system, BlinkMacSystemFont, sans-serif", height: "100dvh" }} className="flex flex-col items-center justify-center w-full">
      {/* Header */}
      <div className="absolute top-0 left-0 right-0 p-6 flex justify-between items-center z-20">
        <a href="/">
          <span className="font-medium text-lg flex items-center gap-2" style={{ color: "#C9D1D9" }}>
            <KiraLogo size={24} id="chatXO" />
            Kira
          </span>
        </a>
        
        {/* Profile Link + Timer */}
        <div style={{ display: "flex", alignItems: "center", gap: 12 }}>
          {/* Timer — only shows under 5 min remaining for free users */}
          {!isPro && localRemaining !== null && localRemaining <= 300 && localRemaining > 0 && (
            <span
              style={{
                fontSize: 12,
                fontWeight: 300,
                fontFamily: "'DM Sans', sans-serif",
                color: `rgba(201,209,217,${localRemaining <= 120 ? 0.5 : 0.25})`,
                letterSpacing: "0.06em",
              }}
            >
              {Math.floor(localRemaining / 60)}:{String(localRemaining % 60).padStart(2, "0")}
            </span>
          )}
          {/* Voice selector */}
          <div style={{
            display: "flex",
            borderRadius: 8,
            overflow: "hidden",
            border: "1px solid rgba(201,209,217,0.12)",
          }}>
            {(["anime", "natural"] as const).map((v) => (
              <button
                key={v}
                onClick={() => {
                  setVoicePref(v);
                  setVoicePreference(v);
                  sendVoiceChange(v);
                }}
                style={{
                  padding: "4px 10px",
                  fontSize: 11,
                  fontWeight: voicePreference === v ? 500 : 300,
                  fontFamily: "'DM Sans', sans-serif",
                  background: voicePreference === v ? "rgba(107,125,179,0.25)" : "transparent",
                  color: voicePreference === v ? "#C9D1D9" : "rgba(201,209,217,0.4)",
                  border: "none",
                  cursor: "pointer",
                  letterSpacing: "0.04em",
                  textTransform: "capitalize",
                  transition: "all 0.2s ease",
                }}
              >
                {v}
              </button>
            ))}
          </div>
          {/* Profile icon */}
          <button 
            onClick={() => setShowProfileModal(true)}
            className="p-2 rounded-full transition-colors"
            style={{ background: "none", border: "none", cursor: "pointer" }}
            onMouseEnter={(e) => { e.currentTarget.style.background = "rgba(255,255,255,0.08)"; }}
            onMouseLeave={(e) => { e.currentTarget.style.background = "none"; }}
          >
              <User size={24} style={{ color: "rgba(201,209,217,0.6)" }} />
          </button>
        </div>
      </div>

      {/* Profile Modal */}
      <ProfileModal 
        isOpen={showProfileModal} 
        onClose={() => setShowProfileModal(false)} 
        isPro={isPro}
      />

      {/* Main Content Area — orb/avatar centered */}
      <div className="flex-grow relative w-full max-w-4xl mx-auto" style={{ minHeight: 0, overflow: "hidden", zIndex: 1 }}>
        {/* Visual — absolutely centered */}
        <div className="absolute inset-0 flex items-center justify-center pointer-events-none" style={{ paddingBottom: isMobile ? 140 : 160 }}>
          <div className="pointer-events-auto" style={{ width: visualMode === "avatar" ? "100%" : undefined, height: visualMode === "avatar" ? "100%" : undefined, position: visualMode === "avatar" ? "relative" : undefined, maxHeight: "100%" }}>
            {visualMode === "avatar" ? (
              <>
                {!live2dReady && <XOLoader />}
                {!live2dDismissed && (
                    <Live2DAvatar
                      isSpeaking={isAudioPlaying}
                      analyserNode={playbackAnalyserNode}
                      emotion={currentExpression}
                      accessories={activeAccessories}
                      action={currentAction}
                      onModelReady={() => {
                        setLive2dReady(true);
                        signalVisualReady();
                      }}
                      onLoadError={() => setLive2dFailed(true)}
                    />
                )}
              </>
            ) : (
              <KiraOrb
                state={
                  isAudioPlaying
                    ? "kiraSpeaking"
                    : kiraState === "thinking"
                      ? "thinking"
                      : micVolume > 0.02
                        ? "userSpeaking"
                        : "idle"
                }
                micVolume={micVolume}
                kiraVolume={isAudioPlaying ? playerVolume : 0}
                size="lg"
                enableBreathing={false}
              />
            )}
          </div>
        </div>
      </div>

      {/* ─── Bottom Area: Controls ─── */}
      <div
        className="fixed bottom-0 left-0 right-0 flex flex-col items-center gap-5 pb-9"
        style={{ zIndex: 50, position: "fixed" }}
      >
        {/* Status indicator + errors — sits between avatar and controls */}
        <div style={{ textAlign: "center", minHeight: 28, display: "flex", flexDirection: "column", alignItems: "center", justifyContent: "center", margin: "24px 0 8px 0" }}>
          {error && error !== "limit_reached" && error !== "limit_reached_pro" && error !== "connection_lost" && error !== "heartbeat_timeout" && (
            <div className="mb-2 p-3 rounded relative" style={{
              background: "rgba(200,55,55,0.15)",
              border: "1px solid rgba(200,55,55,0.3)",
              color: "rgba(255,120,120,0.9)",
            }}>
              <span className="block sm:inline">{error}</span>
            </div>
          )}
          {error === "heartbeat_timeout" && (
            <div className="mb-2 p-4 rounded relative text-center" style={{
              background: "rgba(200,150,55,0.15)",
              border: "1px solid rgba(200,150,55,0.3)",
              color: "rgba(255,210,130,0.9)",
            }}>
              <p className="mb-2" style={{ fontSize: 14 }}>Connection timed out. Your conversation ended.</p>
              <button
                onClick={() => window.location.reload()}
                className="px-4 py-2 rounded text-sm font-medium transition-colors"
                style={{
                  background: "rgba(139,157,195,0.2)",
                  border: "1px solid rgba(139,157,195,0.3)",
                  color: "rgba(200,210,230,0.9)",
                }}
              >
                Start New Conversation
              </button>
            </div>
          )}
          {error === "connection_lost" && (
            <div className="mb-2 p-4 rounded relative text-center" style={{
              background: "rgba(200,150,55,0.15)",
              border: "1px solid rgba(200,150,55,0.3)",
              color: "rgba(255,210,130,0.9)",
            }}>
              <p className="mb-2" style={{ fontSize: 14 }}>Connection lost. Your conversation ended.</p>
              <button
                onClick={() => window.location.reload()}
                className="px-4 py-2 rounded text-sm font-medium transition-colors"
                style={{
                  background: "rgba(139,157,195,0.2)",
                  border: "1px solid rgba(139,157,195,0.3)",
                  color: "rgba(200,210,230,0.9)",
                }}
              >
                Start New Conversation
              </button>
            </div>
          )}
        </div>
        {/* Voice Controls */}
        <div className="flex items-center gap-4 relative z-[1]">
        {/* Avatar/Orb Toggle */}
        <button
          onClick={() => {
            if (visualMode === "avatar") {
              // Switching to orb — reset retry count so user can try avatar again later
              live2dRetryCount.current = 0;
              setVisualMode("orb");
            } else {
              // Switching back to avatar — only allow if retries not exhausted
              if (live2dRetryCount.current < MAX_LIVE2D_RETRIES) {
                live2dRetryCount.current++;
                setLive2dFailed(false);
                setLive2dReady(false);
                // Delay remount to let iOS free the previous WebGL context
                setLive2dDismissed(true);
                setTimeout(() => {
                  setLive2dDismissed(false);
                  setVisualMode("avatar");
                }, 800);
              } else {
                debugLog("[UI] Live2D retry limit reached — staying on orb");
              }
            }
          }}
          className="flex items-center justify-center w-12 h-12 rounded-full border-none transition-all duration-200"
          style={{
            background: visualMode === "avatar" ? "rgba(255,255,255,0.12)" : "rgba(255,255,255,0.04)",
            color: visualMode === "avatar" ? "rgba(139,157,195,0.9)" : "rgba(139,157,195,0.45)",
          }}
          title={visualMode === "avatar" ? "Switch to Orb" : "Switch to Avatar"}
        >
          <Sparkles size={18} />
        </button>

        {/* Vision / Camera Button — only rendered after device detection */}
        {deviceDetected && !isMobile && (
          <button
            onClick={isScreenSharing ? stopScreenShare : startScreenShare}
            className="flex items-center justify-center w-12 h-12 rounded-full border-none transition-all duration-200"
            style={{
              background: isScreenSharing ? "rgba(255,255,255,0.12)" : "rgba(255,255,255,0.04)",
              color: isScreenSharing ? "rgba(139,157,195,0.9)" : "rgba(139,157,195,0.45)",
            }}
            title={isScreenSharing ? "Stop screen share" : "Start screen share"}
          >
            {isScreenSharing ? <Eye size={18} /> : <EyeOff size={18} />}
          </button>
        )}

        {/* Camera Button — mobile only, rendered after device detection */}
        {deviceDetected && isMobile && (
          <button
            onClick={() => isCameraActive ? stopCamera() : startCamera()}
            className="flex items-center justify-center w-12 h-12 rounded-full border-none transition-all duration-200"
            style={{
              background: isCameraActive ? "rgba(255,255,255,0.12)" : "rgba(255,255,255,0.04)",
              color: isCameraActive ? "rgba(139,157,195,0.9)" : "rgba(139,157,195,0.45)",
            }}
            title={isCameraActive ? "Stop camera" : "Start camera"}
          >
            <Camera size={18} />
          </button>
        )}

        {/* Mute Button */}
        <button
          onClick={toggleMute}
          className="flex items-center justify-center w-12 h-12 rounded-full border-none transition-all duration-200"
          style={{
            background: isMuted ? "rgba(255,255,255,0.12)" : "rgba(255,255,255,0.04)",
            color: isMuted ? "rgba(139,157,195,0.9)" : "rgba(139,157,195,0.45)",
          }}
        >
          {isMuted ? <MicOff size={18} /> : <Mic size={18} />}
        </button>

        {/* End Call Button */}
        <button
          onClick={handleEndCall}
          className="flex items-center justify-center w-12 h-12 rounded-full border-none transition-all duration-200"
          style={{
            background: "rgba(200,55,55,0.75)",
            color: "rgba(255,255,255,0.9)",
          }}
          title="End Call"
        >
          <PhoneOff size={18} />
        </button>
        </div>
      </div>

      {/* Camera PIP Preview */}
      {isCameraActive && (
        <div
          onTouchStart={handlePipTouchStart}
          onTouchMove={handlePipTouchMove}
          onTouchEnd={handlePipTouchEnd}
          style={{
            position: "fixed",
            bottom: pipPosition.y,
            right: pipPosition.x,
            width: 80,
            height: 107,
            borderRadius: 12,
            overflow: "hidden",
            border: "1px solid rgba(255, 255, 255, 0.15)",
            boxShadow: "0 4px 12px rgba(0, 0, 0, 0.3)",
            zIndex: 30,
            touchAction: "none",
          }}
        >
          <video
            ref={previewVideoRef}
            style={{
              width: "100%",
              height: "100%",
              objectFit: "cover",
              transform: facingMode === "user" ? "scaleX(-1)" : "none",
            }}
            playsInline
            muted
            autoPlay
          />
          <button
            onClick={() => flipCamera()}
            style={{
              position: "absolute",
              top: 4,
              right: 4,
              width: 24,
              height: 24,
              borderRadius: "50%",
              background: "rgba(0, 0, 0, 0.5)",
              border: "none",
              color: "white",
              fontSize: 12,
              display: "flex",
              alignItems: "center",
              justifyContent: "center",
              cursor: "pointer",
            }}
            title="Flip camera"
          >
            ↻
          </button>
        </div>
      )}

      {/* Rating Modal */}
      {showRatingModal && (
        <div className="fixed inset-0 z-50 flex items-center justify-center" style={{ background: "rgba(0,0,0,0.6)", backdropFilter: "blur(12px)" }}>
          <div style={{
            background: "#0D1117",
            border: "1px solid rgba(255,255,255,0.06)",
            borderRadius: 16,
            padding: "32px 28px",
            maxWidth: 360,
            width: "100%",
            fontFamily: "'DM Sans', sans-serif",
            textAlign: "center",
          }}>
            <h2 style={{
              fontSize: 20,
              fontFamily: "'Playfair Display', serif",
              fontWeight: 400,
              color: "#E2E8F0",
              marginBottom: 20,
              marginTop: 0,
            }}>
              Rate your conversation
            </h2>

            <div className="flex gap-2 justify-center mb-6">
              {[1, 2, 3, 4, 5].map((star) => (
                <button
                  key={star}
                  onMouseEnter={() => setHoverRating(star)}
                  onMouseLeave={() => setHoverRating(0)}
                  onClick={() => setRating(star)}
                  className="transition-transform hover:scale-110 focus:outline-none p-1"
                  style={{ background: "none", border: "none", cursor: "pointer" }}
                >
                  <Star
                    size={28}
                    className="transition-colors duration-150"
                    style={{
                      fill: star <= (hoverRating || rating) ? "#8B9DC3" : "transparent",
                      color: star <= (hoverRating || rating) ? "#8B9DC3" : "rgba(201,209,217,0.2)",
                    }}
                  />
                </button>
              ))}
            </div>

            <div className="flex flex-col w-full gap-3">
              <button
                onClick={handleRate}
                disabled={rating === 0}
                style={{
                  width: "100%",
                  padding: "12px 0",
                  borderRadius: 10,
                  border: "none",
                  background: rating > 0 ? "linear-gradient(135deg, rgba(107,125,179,0.3), rgba(107,125,179,0.15))" : "rgba(255,255,255,0.04)",
                  color: rating > 0 ? "#C9D1D9" : "rgba(201,209,217,0.3)",
                  fontSize: 14,
                  fontWeight: 500,
                  cursor: rating > 0 ? "pointer" : "not-allowed",
                  fontFamily: "'DM Sans', sans-serif",
                  transition: "all 0.2s",
                }}
              >
                Rate it
              </button>
              <button
                onClick={handleContinue}
                style={{
                  width: "100%",
                  padding: "12px 0",
                  background: "none",
                  border: "none",
                  color: "rgba(201,209,217,0.35)",
                  fontSize: 14,
                  fontWeight: 400,
                  cursor: "pointer",
                  fontFamily: "'DM Sans', sans-serif",
                  transition: "color 0.2s",
                }}
              >
                Continue
              </button>
            </div>
          </div>
        </div>
      )}

      {/* Limit Reached — Paywall Overlay (Free users & Guests only, never Pro) */}
      {error === "limit_reached" && !isPro && (
        <div
          className="fixed inset-0 z-50 flex items-center justify-center"
          style={{
            background: "rgba(13,17,23,0.85)",
            backdropFilter: "blur(20px)",
            animation: "paywallFadeIn 0.6s ease both",
          }}
        >
          <div style={{
            background: "linear-gradient(135deg, rgba(20,25,35,0.95), rgba(13,17,23,0.98))",
            border: "1px solid rgba(107,125,179,0.12)",
            borderRadius: 20,
            padding: "40px 32px",
            maxWidth: 420,
            width: "100%",
            fontFamily: "'DM Sans', sans-serif",
            textAlign: "center",
            boxShadow: "0 0 80px rgba(107,125,179,0.06)",
          }}>
            {/* Ambient glow */}
            <div style={{
              width: 72,
              height: 72,
              borderRadius: 18,
              background: "linear-gradient(135deg, rgba(107,125,179,0.15), rgba(107,125,179,0.05))",
              border: "1px solid rgba(107,125,179,0.2)",
              display: "flex",
              alignItems: "center",
              justifyContent: "center",
              margin: "0 auto 24px",
            }}>
              <Clock size={28} style={{ color: "rgba(139,157,195,0.7)" }} />
            </div>

            {isGuest ? (
              <>
                <h2 style={{
                  fontSize: 24,
                  fontFamily: "'Playfair Display', serif",
                  fontWeight: 400,
                  color: "#E2E8F0",
                  marginBottom: 10,
                  marginTop: 0,
                }}>
                  This is the beginning of something
                </h2>
                <p style={{
                  fontSize: 15,
                  fontWeight: 300,
                  color: "rgba(201,209,217,0.5)",
                  lineHeight: 1.7,
                  marginBottom: 32,
                }}>
                  Create a free account and Kira keeps building on everything
                  you just talked about — and every conversation after.
                </p>
                <div className="flex flex-col w-full gap-3">
                  <button
                    onClick={handleSignUp}
                    style={{
                      width: "100%",
                      padding: "14px 0",
                      borderRadius: 12,
                      border: "1px solid rgba(107,125,179,0.25)",
                      background: "linear-gradient(135deg, rgba(107,125,179,0.25), rgba(107,125,179,0.1))",
                      color: "#C9D1D9",
                      fontSize: 15,
                      fontWeight: 500,
                      cursor: "pointer",
                      fontFamily: "'DM Sans', sans-serif",
                      transition: "all 0.3s ease",
                      boxShadow: "0 0 30px rgba(107,125,179,0.08)",
                    }}
                  >
                    Create free account
                  </button>
                  <a
                    href="/"
                    style={{
                      display: "block",
                      width: "100%",
                      padding: "12px 0",
                      color: "rgba(201,209,217,0.3)",
                      fontSize: 14,
                      fontWeight: 400,
                      textAlign: "center",
                      textDecoration: "none",
                      transition: "color 0.2s",
                    }}
                  >
                    I&apos;ll come back tomorrow
                  </a>
                </div>
              </>
            ) : (
              <>
                <h2 style={{
                  fontSize: 24,
                  fontFamily: "'Playfair Display', serif",
                  fontWeight: 400,
                  color: "#E2E8F0",
                  marginBottom: 10,
                  marginTop: 0,
                }}>
                  You&apos;ve used your 15 minutes
                </h2>
                <p style={{
                  fontSize: 15,
                  fontWeight: 300,
                  color: "rgba(201,209,217,0.5)",
                  lineHeight: 1.7,
                  marginBottom: 32,
                }}>
                  Upgrade to Pro for unlimited conversations,
                  priority responses, and persistent memory across sessions.
                </p>
                <div className="flex flex-col w-full gap-3">
                  <button
                    onClick={handleUpgrade}
                    style={{
                      width: "100%",
                      padding: "14px 0",
                      borderRadius: 12,
                      border: "1px solid rgba(107,125,179,0.25)",
                      background: "linear-gradient(135deg, rgba(107,125,179,0.25), rgba(107,125,179,0.1))",
                      color: "#C9D1D9",
                      fontSize: 15,
                      fontWeight: 500,
                      cursor: "pointer",
                      fontFamily: "'DM Sans', sans-serif",
                      transition: "all 0.3s ease",
                      boxShadow: "0 0 30px rgba(107,125,179,0.08)",
                    }}
                  >
                    Upgrade to Pro — $9.99/mo
                  </button>
                  <a
                    href="/"
                    style={{
                      display: "block",
                      width: "100%",
                      padding: "12px 0",
                      color: "rgba(201,209,217,0.3)",
                      fontSize: 14,
                      fontWeight: 400,
                      textAlign: "center",
                      textDecoration: "none",
                      transition: "color 0.2s",
                    }}
                  >
                    I&apos;ll come back tomorrow
                  </a>
                </div>
              </>
            )}
          </div>
        </div>
      )}

      {/* Pro Limit Reached — Warm Full-Screen Overlay (no upsell) */}
      {error === "limit_reached_pro" && (
        <div
          className="fixed inset-0 z-50 flex items-center justify-center"
          style={{
            background: "rgba(13,17,23,0.85)",
            backdropFilter: "blur(20px)",
            animation: "paywallFadeIn 0.6s ease both",
          }}
        >
          <div style={{
            background: "linear-gradient(135deg, rgba(20,25,35,0.95), rgba(13,17,23,0.98))",
            border: "1px solid rgba(107,125,179,0.12)",
            borderRadius: 20,
            padding: "40px 32px",
            maxWidth: 420,
            width: "100%",
            fontFamily: "'DM Sans', sans-serif",
            textAlign: "center",
            boxShadow: "0 0 80px rgba(107,125,179,0.06)",
          }}>
            <div style={{
              width: 72,
              height: 72,
              borderRadius: 18,
              background: "linear-gradient(135deg, rgba(107,125,179,0.15), rgba(107,125,179,0.05))",
              border: "1px solid rgba(107,125,179,0.2)",
              display: "flex",
              alignItems: "center",
              justifyContent: "center",
              margin: "0 auto 24px",
            }}>
              <Clock size={28} style={{ color: "rgba(139,157,195,0.7)" }} />
            </div>

            <h2 style={{
              fontSize: 24,
              fontFamily: "'Playfair Display', serif",
              fontWeight: 400,
              color: "#E2E8F0",
              marginBottom: 10,
              marginTop: 0,
            }}>
              You&apos;ve had quite the month
            </h2>
            <p style={{
              fontSize: 15,
              fontWeight: 300,
              color: "rgba(201,209,217,0.5)",
              lineHeight: 1.7,
              marginBottom: 8,
            }}>
              You&apos;ve reached your monthly conversation limit.
              Your conversations and memories are safe — Kira will be
              ready to pick up right where you left off.
            </p>
            <p style={{
              fontSize: 13,
              fontWeight: 300,
              color: "rgba(201,209,217,0.3)",
              marginBottom: 32,
            }}>
              Resets on the 1st of next month
            </p>
            <a
              href="/"
              style={{
                display: "block",
                width: "100%",
                padding: "14px 0",
                borderRadius: 12,
                border: "1px solid rgba(107,125,179,0.15)",
                background: "rgba(107,125,179,0.08)",
                color: "rgba(201,209,217,0.6)",
                fontSize: 15,
                fontWeight: 500,
                textAlign: "center",
                textDecoration: "none",
                fontFamily: "'DM Sans', sans-serif",
                transition: "all 0.3s ease",
              }}
            >
              Back to home
            </a>
          </div>
        </div>
      )}

      {/* Mobile Audio Unlock Overlay */}
      {isAudioBlocked && (
        <div className="fixed inset-0 z-50 flex items-center justify-center" style={{ background: "rgba(0,0,0,0.6)", backdropFilter: "blur(12px)" }}>
          <button
            onClick={resumeAudio}
            style={{
              display: "flex",
              flexDirection: "column",
              alignItems: "center",
              gap: 16,
              padding: "32px 40px",
              borderRadius: 16,
              background: "#0D1117",
              border: "1px solid rgba(255,255,255,0.06)",
              cursor: "pointer",
              fontFamily: "'DM Sans', sans-serif",
              transition: "transform 0.2s",
            }}
            onMouseEnter={(e) => { e.currentTarget.style.transform = "scale(1.02)"; }}
            onMouseLeave={(e) => { e.currentTarget.style.transform = "scale(1)"; }}
          >
            <div style={{
              width: 56,
              height: 56,
              borderRadius: 14,
              background: "linear-gradient(135deg, rgba(107,125,179,0.2), rgba(107,125,179,0.08))",
              border: "1px solid rgba(107,125,179,0.25)",
              display: "flex",
              alignItems: "center",
              justifyContent: "center",
            }}>
              <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="rgba(139,157,195,0.7)" strokeWidth="1.8" strokeLinecap="round" strokeLinejoin="round">
                <path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z" />
                <path d="M19 10v2a7 7 0 0 1-14 0v-2" />
                <line x1="12" y1="19" x2="12" y2="22" />
              </svg>
            </div>
            <span style={{
              fontSize: 16,
              fontWeight: 500,
              color: "#C9D1D9",
            }}>Tap to Start</span>
          </button>
        </div>
      )}
    </div>
  );
}


===== packages/web/src/hooks/useKiraSocket.ts =====
"use client";
import { useState, useEffect, useRef, useCallback } from "react";
import { useSceneDetection } from "./useSceneDetection";

// --- Persistent debug logger (survives page reloads via sessionStorage) ---
// Silent in production unless ?debug is in the URL
const isDebug = typeof window !== 'undefined' && (process.env.NODE_ENV !== 'production' || window.location.search.includes('debug'));
export function debugLog(...args: any[]) {
  if (!isDebug) return;
  const msg = `[${new Date().toISOString().slice(11, 23)}] ${args.map(a => typeof a === 'string' ? a : JSON.stringify(a)).join(' ')}`;
  console.log(...args);
  try {
    const logs = JSON.parse(sessionStorage.getItem('kira-debug') || '[]');
    logs.push(msg);
    if (logs.length > 200) logs.splice(0, logs.length - 200);
    sessionStorage.setItem('kira-debug', JSON.stringify(logs));
  } catch {}
}

// Define the states
type SocketState = "idle" | "connecting" | "connected" | "closing" | "closed";
export type KiraState = "listening" | "thinking" | "speaking";

// ─── Window-level singleton — survives React remounts AND module re-evaluation ───
// Module-level vars can be re-created if Next.js re-evaluates the module during
// code splitting or dynamic imports. window is truly global and survives everything.
interface ConnectionStore {
  ws: WebSocket | null;
  socketState: SocketState;
  audioContext: AudioContext | null;
  playbackContext: AudioContext | null;
  audioStream: MediaStream | null;
  audioWorkletNode: AudioWorkletNode | null;
  audioSource: MediaStreamAudioSourceNode | null;
  playbackGain: GainNode | null;
  playbackAnalyser: AnalyserNode | null;
  isServerReady: boolean;
  conversationActive: boolean;
  reconnectAttempts: number;
}

function getConnectionStore(): ConnectionStore | null {
  if (typeof window === "undefined") return null;
  if (!(window as any).__kiraConnectionStore) {
    (window as any).__kiraConnectionStore = {
      ws: null,
      socketState: "idle",
      audioContext: null,
      playbackContext: null,
      audioStream: null,
      audioWorkletNode: null,
      audioSource: null,
      playbackGain: null,
      playbackAnalyser: null,
      isServerReady: false,
      conversationActive: false,
      reconnectAttempts: 0,
    } as ConnectionStore;
  }
  return (window as any).__kiraConnectionStore as ConnectionStore;
}

// Adaptive EOU: short utterances get snappy response, long utterances get patience for multi-part questions
const EOU_TIMEOUT_MIN = 500;   // 500ms silence for short utterances ("yes", "no", "hi")
const EOU_TIMEOUT_MAX = 1500;  // 1500ms silence for long multi-part questions
const LONG_UTTERANCE_FRAMES = 800; // ~2s of speech = "long utterance" (each frame ≈ 2.67ms at 48kHz)
const MIN_SPEECH_FRAMES_FOR_EOU = 200; // Must have ~200 speech frames (~1-2s real speech) to prevent noise-triggered EOUs
const VAD_STABILITY_FRAMES = 5; // Need 5 consecutive speech frames before considering "speaking"

// Hair accessories managed by cycle timer in Live2DAvatar — block from server-sent changes
const HAIR_ACCESSORIES = new Set(["clip_bangs", "low_twintails", "short_hair"]);

export const useKiraSocket = (getTokenFn: (() => Promise<string | null>) | null, guestId: string, voicePreference: string = "anime") => {
  // ─── Restore state from singleton if a live connection exists ───
  const [socketState, setSocketState] = useState<SocketState>(() => {
    // Use the stored socketState directly — it's authoritative
    if (getConnectionStore()!.socketState === "connected" || getConnectionStore()!.socketState === "connecting") {
      debugLog("[Hook] Restoring socketState →", getConnectionStore()!.socketState, "from singleton. ws:", !!getConnectionStore()!.ws);
      return getConnectionStore()!.socketState;
    }
    return "idle";
  });
  const [kiraState, setKiraState] = useState<KiraState>("listening");
  const kiraStateRef = useRef<KiraState>("listening"); // Ref to track state in callbacks

  // Log hook mount/unmount — DO NOT close WS on unmount (singleton survives remount)
  useEffect(() => {
    debugLog("[Hook] useKiraSocket MOUNTED. Singleton ws:", !!getConnectionStore()!.ws, 
      "readyState:", getConnectionStore()!.ws?.readyState,
      "socketState restored as:", getConnectionStore()!.ws?.readyState === WebSocket.OPEN ? "connected" : "idle");
    return () => {
      debugLog("[Hook] useKiraSocket UNMOUNTING — preserving singleton");
      // Sync current refs TO singleton only if they're alive.
      // NEVER overwrite singleton with null — that destroys the connection for the next mount.
      // The ONLY place that should null out singleton is disconnect() (explicit End Call).
      if (ws.current && ws.current.readyState === WebSocket.OPEN) {
        getConnectionStore()!.ws = ws.current;
        debugLog("[Hook] WebSocket preserved in singleton (readyState:", ws.current.readyState, ")");
      }
      if (audioContext.current) getConnectionStore()!.audioContext = audioContext.current;
      if (playbackContext.current) getConnectionStore()!.playbackContext = playbackContext.current;
      if (audioStream.current) getConnectionStore()!.audioStream = audioStream.current;
      if (audioWorkletNode.current) getConnectionStore()!.audioWorkletNode = audioWorkletNode.current;
      if (audioSource.current) getConnectionStore()!.audioSource = audioSource.current;
      if (playbackGain.current) getConnectionStore()!.playbackGain = playbackGain.current;
      if (playbackAnalyser.current) getConnectionStore()!.playbackAnalyser = playbackAnalyser.current;
      // Always sync these non-nullable flags
      getConnectionStore()!.isServerReady = isServerReady.current;
      getConnectionStore()!.conversationActive = conversationActive.current;
      getConnectionStore()!.reconnectAttempts = reconnectAttempts.current;
    };
  }, []);

  // ─── Handler refs: these always point to the latest closure ───
  // The actual WS handlers call through these refs, so remounts get fresh state setters.
  const onMessageRef = useRef<((event: MessageEvent) => void) | null>(null);
  const onCloseRef = useRef<((event: CloseEvent) => void) | null>(null);
  const onErrorRef = useRef<((event: Event) => void) | null>(null);

  // ─── Visual-ready gating: don't send start_stream until Live2D is loaded (or timeout) ───
  const visualReadyRef = useRef(false);
  const wsOpenRef = useRef(false); // true once ws.onopen fires

  // Sync ref with state
  useEffect(() => {
    kiraStateRef.current = kiraState;
  }, [kiraState]);

  const [micVolume, setMicVolume] = useState(0);
  const [transcript, setTranscript] = useState<{ role: "user" | "ai"; text: string } | null>(null);

  const [currentExpression, setCurrentExpression] = useState<string>("neutral");
  const [activeAccessories, setActiveAccessories] = useState<string[]>([]);
  const [currentAction, setCurrentAction] = useState<string | null>(null);
  const [error, setError] = useState<string | null>(null);
  const [isAudioBlocked, setIsAudioBlocked] = useState(false);
  const [isMuted, setIsMuted] = useState(false);
  const [isScreenSharing, setIsScreenSharing] = useState(false);
  const [isCameraActive, setIsCameraActive] = useState(false);
  const [facingMode, setFacingMode] = useState<"environment" | "user">("environment");
  const [isPro, setIsPro] = useState(false);
  const isProRef = useRef(false); // Ref mirror of isPro for use in onclose callback
  const [remainingSeconds, setRemainingSeconds] = useState<number | null>(null);
  const [isAudioPlaying, setIsAudioPlaying] = useState(false);
  const audioPlayingTimeout = useRef<ReturnType<typeof setTimeout> | null>(null);

  // --- Visualizer: read AnalyserNode while audio is playing ---
  useEffect(() => {
    if (!isAudioPlaying) {
      setPlayerVolume(0);
      return;
    }

    let frame: number;
    const tick = () => {
      if (playbackAnalyser.current) {
        const data = new Uint8Array(playbackAnalyser.current.frequencyBinCount);
        playbackAnalyser.current.getByteFrequencyData(data);
        let sum = 0;
        for (let i = 0; i < data.length; i++) sum += data[i];
        const avg = sum / data.length / 255; // normalize 0-1
        setPlayerVolume(avg);
      }
      frame = requestAnimationFrame(tick);
    };
    tick();

    return () => cancelAnimationFrame(frame);
  }, [isAudioPlaying]);
  const ws = useRef<WebSocket | null>(getConnectionStore()!.ws);
  const isServerReady = useRef(getConnectionStore()!.isServerReady); // Gate for sending audio

  // --- Audio Pipeline Refs (restore from singleton if present) ---
  const audioContext = useRef<AudioContext | null>(getConnectionStore()!.audioContext);
  const audioWorkletNode = useRef<AudioWorkletNode | null>(getConnectionStore()!.audioWorkletNode);
  const audioSource = useRef<MediaStreamAudioSourceNode | null>(getConnectionStore()!.audioSource);
  const audioStream = useRef<MediaStream | null>(getConnectionStore()!.audioStream);

  // --- Screen Share Refs ---
  const screenStream = useRef<MediaStream | null>(null);
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);
  const isScreenSharingRef = useRef(false); // Ref to track screen share state in callbacks

  // --- Camera Refs ---
  const cameraStreamRef = useRef<MediaStream | null>(null);
  const cameraVideoRef = useRef<HTMLVideoElement | null>(null);
  const cameraIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const isCameraActiveRef = useRef(false);

  // --- Scene Detection ---
  const lastSceneUpdateSent = useRef(0);
  const SCENE_UPDATE_COOLDOWN = 30000; // Don't send scene updates more than once per 30 seconds

  const handleSceneChange = useCallback((frames: string[]) => {
    const now = Date.now();
    if (
      now - lastSceneUpdateSent.current > SCENE_UPDATE_COOLDOWN &&
      ws.current?.readyState === WebSocket.OPEN &&
      isScreenSharingRef.current &&
      kiraStateRef.current === "listening"
    ) {
      lastSceneUpdateSent.current = now;
      ws.current.send(JSON.stringify({
        type: "scene_update",
        images: frames,
      }));
    }
  }, []);

  const sceneBuffer = useSceneDetection({
    videoRef,
    enabled: isScreenSharing,
    checkInterval: 2000,
    threshold: 15,
    onSceneChange: handleSceneChange,
  });
  const sceneBufferRef = useRef<string[]>([]);

  // Sync sceneBuffer to ref for access in callbacks
  useEffect(() => {
    sceneBufferRef.current = sceneBuffer;
  }, [sceneBuffer]);

  // --- Audio Playback Refs ---
  const audioQueue = useRef<ArrayBuffer[]>([]);
  const nextStartTime = useRef(0); // Track where the next chunk should start
  const isProcessingQueue = useRef(false); // Lock for the processing loop
  const scheduledSources = useRef<AudioBufferSourceNode[]>([]); // Track all scheduled sources
  const ttsChunksDone = useRef(true); // Whether server has finished sending audio for this turn

  const playbackContext = useRef<AudioContext | null>(getConnectionStore()!.playbackContext);
  const playbackSource = useRef<AudioBufferSourceNode | null>(null);
  const playbackGain = useRef<GainNode | null>(getConnectionStore()!.playbackGain);
  const playbackAnalyser = useRef<AnalyserNode | null>(getConnectionStore()!.playbackAnalyser);
  const playerVolumeFrame = useRef<number>(0);
  const [playerVolume, setPlayerVolume] = useState(0);

  // --- "Ramble Bot" EOU Timer ---
  const eouTimer = useRef<NodeJS.Timeout | null>(null);
  const maxUtteranceTimer = useRef<NodeJS.Timeout | null>(null);
  const speechFrameCount = useRef(0); // Track consecutive speech frames for VAD stability
  const totalSpeechFrames = useRef(0); // Total speech frames in current utterance (reset on EOU)
  const hasSpoken = useRef(false); // Whether user has spoken enough to trigger EOU

  // --- Latency Tracking ---
  const eouSentAt = useRef(0);
  const firstAudioLogged = useRef(false);

  // --- Vision: Snapshot Cooldown ---
  const lastSnapshotTime = useRef(0);
  const SNAPSHOT_COOLDOWN_MS = 5000; // One snapshot per 5 seconds max
  const periodicCaptureTimer = useRef<ReturnType<typeof setInterval> | null>(null);

  // --- WebSocket Auto-Reconnect ---
  const reconnectAttempts = useRef(getConnectionStore()!.reconnectAttempts);
  const MAX_RECONNECT_ATTEMPTS = 5;
  const conversationActive = useRef(getConnectionStore()!.conversationActive); // True once start_stream sent — prevents reconnect loops

  /**
   * Calculates adaptive EOU timeout based on how long the user has been speaking.
   * Short utterances ("yes") → fast 500ms cutoff for snappy responses.
   * Long utterances (multi-part questions) → patient 1500ms to allow thinking pauses.
   */
  const getAdaptiveEOUTimeout = () => {
    const ratio = Math.min(totalSpeechFrames.current / LONG_UTTERANCE_FRAMES, 1.0);
    return Math.round(EOU_TIMEOUT_MIN + (EOU_TIMEOUT_MAX - EOU_TIMEOUT_MIN) * ratio);
  };

  /**
   * Stops current audio playback and clears the queue.
   */
  const stopAudioPlayback = useCallback(() => {
    // 1. Clear the queue so no new chunks are scheduled
    audioQueue.current = [];
    
    // 2. Stop ALL scheduled sources
    scheduledSources.current.forEach((source) => {
      try {
        source.stop();
      } catch (e) {
        // Ignore errors if already stopped
      }
    });
    scheduledSources.current = []; // Clear the list
    playbackSource.current = null;

    // 3. Reset scheduling time
    if (playbackContext.current) {
        nextStartTime.current = playbackContext.current.currentTime;
    } else {
        nextStartTime.current = 0;
    }

    // 4. Reset for next turn
    ttsChunksDone.current = true;

    // 5. Audio is no longer playing
    if (audioPlayingTimeout.current) {
      clearTimeout(audioPlayingTimeout.current);
      audioPlayingTimeout.current = null;
    }
    setIsAudioPlaying(false);
  }, []);

  /**
   * Processes the audio queue and schedules chunks to play back-to-back.
   * This eliminates gaps/pops caused by waiting for onended events.
   */
  const processAudioQueue = useCallback(async () => {
    if (isProcessingQueue.current) return;
    isProcessingQueue.current = true;

    // Ensure the playback audio context is running (and is 16kHz for Azure's output)
    if (
      !playbackContext.current ||
      playbackContext.current.state === "closed"
    ) {
      playbackContext.current = new AudioContext({ sampleRate: 16000 });
      // Reset persistent audio chain when context is recreated
      playbackGain.current = null;
      playbackAnalyser.current = null;
    }
    if (playbackContext.current.state === "suspended") {
      await playbackContext.current.resume();
    }

    // Build persistent audio chain once: Source → GainNode → AnalyserNode → Destination
    if (!playbackGain.current) {
      playbackGain.current = playbackContext.current.createGain();
      playbackAnalyser.current = playbackContext.current.createAnalyser();
      playbackAnalyser.current.fftSize = 256;
      playbackAnalyser.current.smoothingTimeConstant = 0.3; // Moderate pre-smoothing — LipSyncEngine handles the rest
      playbackAnalyser.current.minDecibels = -90;
      playbackAnalyser.current.maxDecibels = -10;
      playbackGain.current.connect(playbackAnalyser.current);
      playbackAnalyser.current.connect(playbackContext.current.destination);
    }

    while (audioQueue.current.length > 0) {
      const buffer = audioQueue.current.shift();
      if (!buffer) continue;

      try {
        // 1. Decode the raw PCM buffer
        const wavBuffer = createWavHeader(buffer, 16000, 16);
        const audioBuffer = await playbackContext.current.decodeAudioData(
          wavBuffer
        );

        // 2. Create a source node and route through persistent gain
        const source = playbackContext.current.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(playbackGain.current!);

        // 3. Schedule playback
        const currentTime = playbackContext.current.currentTime;
        // If nextStartTime is in the past (gap in stream), reset to now + small buffer
        if (nextStartTime.current < currentTime) {
          nextStartTime.current = currentTime + 0.02;
        }

        source.start(nextStartTime.current);
        nextStartTime.current += audioBuffer.duration;

        // Signal that audio is actively playing
        if (audioPlayingTimeout.current) {
          clearTimeout(audioPlayingTimeout.current);
          audioPlayingTimeout.current = null;
        }
        setIsAudioPlaying(true);

        // Keep track of the source so we can stop it later
        scheduledSources.current.push(source);
        source.onended = () => {
          // Remove from list when done to keep memory clean
          scheduledSources.current = scheduledSources.current.filter(s => s !== source);

          // When last source finishes and no more chunks coming, debounce isAudioPlaying off
          if (scheduledSources.current.length === 0 && audioQueue.current.length === 0) {
            audioPlayingTimeout.current = setTimeout(() => {
              // Double-check nothing new arrived in the gap
              if (scheduledSources.current.length === 0 && audioQueue.current.length === 0) {
                setIsAudioPlaying(false);
              }
            }, 300);
          }
        };

        // Keep track of the last source if we need to stop it manually later
        playbackSource.current = source;

      } catch (e) {
        console.error("[AudioPlayer] Error decoding or playing audio:", e);
      }
    }

    isProcessingQueue.current = false;
  }, []);

  const stopAudioPipeline = useCallback(() => {
    if (eouTimer.current) clearTimeout(eouTimer.current);

    audioWorkletNode.current?.port.close();
    audioSource.current?.disconnect();
    audioStream.current?.getTracks().forEach((track) => track.stop());
    screenStream.current?.getTracks().forEach((track) => track.stop()); // Stop screen share
    // Stop camera if active
    if (cameraStreamRef.current) {
      cameraStreamRef.current.getTracks().forEach(track => track.stop());
      cameraStreamRef.current = null;
    }
    if (cameraIntervalRef.current) {
      clearInterval(cameraIntervalRef.current);
      cameraIntervalRef.current = null;
    }
    if (cameraVideoRef.current) {
      cameraVideoRef.current.pause();
      cameraVideoRef.current.srcObject = null;
      cameraVideoRef.current = null;
    }
    setIsCameraActive(false);
    isCameraActiveRef.current = false;
    audioContext.current?.close().catch(console.error);
    playbackContext.current?.close().catch(console.error);

    audioWorkletNode.current = null;
    audioSource.current = null;
    audioStream.current = null;
    audioContext.current = null;
    playbackContext.current = null;
    playbackGain.current = null;

    // ─── Clear audio from singleton ───
    getConnectionStore()!.audioContext = null;
    getConnectionStore()!.playbackContext = null;
    getConnectionStore()!.audioStream = null;
    getConnectionStore()!.audioWorkletNode = null;
    getConnectionStore()!.audioSource = null;
    getConnectionStore()!.playbackGain = null;
    getConnectionStore()!.playbackAnalyser = null;

    debugLog("[Audio] 🛑 Audio pipeline stopped.");
  }, []);

  /**
   * Initializes audio contexts and requests mic permission.
   * Must be called from a user gesture.
   */
  const initializeAudio = useCallback(async () => {
    try {
      debugLog("[Audio] Initializing audio contexts...");
      
      // 1. Create/Resume AudioContext
      if (!audioContext.current || audioContext.current.state === "closed") {
        audioContext.current = new AudioContext();
        debugLog(`[Audio] Created capture AudioContext (sampleRate: ${audioContext.current.sampleRate})`);
      }
      if (audioContext.current.state === "suspended") {
        debugLog("[Audio] Capture AudioContext is suspended, resuming...");
        await audioContext.current.resume();
      }
      debugLog(`[Audio] Capture AudioContext state: ${audioContext.current.state}`);

      // 2. Create/Resume PlaybackContext
      if (!playbackContext.current || playbackContext.current.state === "closed") {
        playbackContext.current = new AudioContext({ sampleRate: 16000 });
        debugLog("[Audio] Created playback AudioContext (sampleRate: 16000)");
      }
      if (playbackContext.current.state === "suspended") {
        debugLog("[Audio] Playback AudioContext is suspended, resuming...");
        await playbackContext.current.resume();
      }
      debugLog(`[Audio] Playback AudioContext state: ${playbackContext.current.state}`);

      // 3. Request Mic Permission (if not already)
      if (!audioStream.current) {
        debugLog("[Audio] Requesting mic permission...");
        audioStream.current = await navigator.mediaDevices.getUserMedia({
          audio: {
            channelCount: 1,
            echoCancellation: true,
            autoGainControl: true,
            noiseSuppression: true,
          },
        });
        debugLog(`[Audio] Mic permission granted. Tracks: ${audioStream.current.getAudioTracks().length}, active: ${audioStream.current.active}`);
      } else {
        debugLog(`[Audio] Mic stream already exists. active: ${audioStream.current.active}`);
      }

      setIsAudioBlocked(false);
      return true;
    } catch (err) {
      debugLog("[Audio] ❌ Failed to initialize audio:", err);
      setIsAudioBlocked(true);
      return false;
    }
  }, []);

  /**
   * Toggles microphone mute state
   */
  const toggleMute = useCallback(() => {
    if (audioStream.current) {
      const audioTracks = audioStream.current.getAudioTracks();
      audioTracks.forEach(track => {
        track.enabled = !track.enabled;
      });
      setIsMuted(prev => !prev);
    }
  }, []);

  /**
   * Starts screen sharing
   */
  const startScreenShare = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getDisplayMedia({
        video: {
          width: { ideal: 1280 },
          height: { ideal: 720 },
          frameRate: { ideal: 10 } // Low framerate is fine for snapshots
        },
        audio: false
      });

      screenStream.current = stream;
      setIsScreenSharing(true);
      isScreenSharingRef.current = true;

      // Setup hidden video element for capturing frames
      if (!videoRef.current) {
        videoRef.current = document.createElement("video");
        videoRef.current.autoplay = true;
        videoRef.current.muted = true;
        videoRef.current.playsInline = true;
        // Ensure it's in the DOM so it processes frames
        videoRef.current.style.position = "absolute";
        videoRef.current.style.top = "-9999px";
        videoRef.current.style.left = "-9999px";
        videoRef.current.style.width = "1px";
        videoRef.current.style.height = "1px";
        videoRef.current.style.opacity = "0";
        videoRef.current.style.pointerEvents = "none";
        document.body.appendChild(videoRef.current);
      }
      videoRef.current.srcObject = stream;
      await videoRef.current.play();

      // Handle user stopping share via browser UI
      stream.getVideoTracks()[0].onended = () => {
        stopScreenShare();
      };

      debugLog("[Vision] Screen share started");
      
      // Send an initial snapshot immediately to establish context
      setTimeout(() => {
          const snapshot = captureScreenSnapshot();
          if (snapshot && ws.current?.readyState === WebSocket.OPEN) {
              debugLog("[Vision] Sending initial snapshot...");
              // Send buffer + current frame
              const payload = {
                  type: "image",
                  images: [...sceneBufferRef.current, snapshot]
              };
              ws.current.send(JSON.stringify(payload));
          } else {
              console.warn("[Vision] Failed to capture initial snapshot.");
          }
      }, 1000);

      // Start periodic captures every 15 seconds so the server always has fresh images
      if (periodicCaptureTimer.current) clearInterval(periodicCaptureTimer.current);
      periodicCaptureTimer.current = setInterval(() => {
        if (!isScreenSharingRef.current || !ws.current || ws.current.readyState !== WebSocket.OPEN) {
          if (periodicCaptureTimer.current) clearInterval(periodicCaptureTimer.current);
          periodicCaptureTimer.current = null;
          return;
        }
        const snapshot = captureScreenSnapshot();
        if (snapshot) {
          ws.current.send(JSON.stringify({
            type: "image",
            images: [snapshot],
          }));
          debugLog("[Vision] Periodic snapshot sent.");
        }
      }, 15000);

    } catch (err) {
      console.error("[Vision] Failed to start screen share:", err);
      setIsScreenSharing(false);
    }
  }, []);

  /**
   * Stops screen sharing
   */
  const stopScreenShare = useCallback(() => {
    if (periodicCaptureTimer.current) {
      clearInterval(periodicCaptureTimer.current);
      periodicCaptureTimer.current = null;
    }
    if (screenStream.current) {
      screenStream.current.getTracks().forEach(track => track.stop());
      screenStream.current = null;
    }
    if (videoRef.current) {
      videoRef.current.srcObject = null;
      // Remove from DOM
      if (videoRef.current.parentNode) {
          videoRef.current.parentNode.removeChild(videoRef.current);
      }
      videoRef.current = null; // Reset ref
    }
    setIsScreenSharing(false);
    isScreenSharingRef.current = false;

    // Tell server to stop vision reactions
    if (ws.current && ws.current.readyState === WebSocket.OPEN) {
      ws.current.send(JSON.stringify({ type: "vision_stop" }));
    }

    debugLog("[Vision] Screen share stopped");
  }, []);

  /**
   * Captures a frame from the camera and sends it over WebSocket
   */
  const captureAndSendCameraFrame = useCallback(() => {
    const video = cameraVideoRef.current;
    if (!video || video.readyState < 2) return;

    const canvas = document.createElement("canvas");
    // Downscale to max 512px on longest side (same as screen share)
    const MAX_DIM = 512;
    const scale = Math.min(MAX_DIM / video.videoWidth, MAX_DIM / video.videoHeight, 1);
    canvas.width = Math.round(video.videoWidth * scale);
    canvas.height = Math.round(video.videoHeight * scale);

    const ctx = canvas.getContext("2d");
    if (!ctx) return;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    const jpeg = canvas.toDataURL("image/jpeg", 0.5);
    const base64 = jpeg.split(",")[1];

    if (ws.current && ws.current.readyState === WebSocket.OPEN) {
      ws.current.send(JSON.stringify({
        type: "image",
        images: [base64],
      }));
    }
  }, []);

  /**
   * Starts camera capture (mobile vision)
   */
  const startCamera = useCallback(async (mode?: "environment" | "user") => {
    const useFacing = mode || facingMode;
    debugLog("[Camera] startCamera called, facingMode:", useFacing);

    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      console.error("[Camera] getUserMedia not available — requires HTTPS");
      setError("Camera not available — HTTPS required");
      return;
    }

    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          facingMode: useFacing,
          width: { ideal: 1280 },
          height: { ideal: 720 },
        },
        audio: false,
      });
      debugLog("[Camera] Got stream:", stream.getVideoTracks().length, "video tracks");

      cameraStreamRef.current = stream;

      const video = document.createElement("video");
      video.srcObject = stream;
      video.setAttribute("playsinline", "true");
      video.muted = true;
      await video.play();
      cameraVideoRef.current = video;

      setIsCameraActive(true);
      isCameraActiveRef.current = true;
      debugLog("[Camera] Camera started, facing:", useFacing);

      // Send initial snapshot
      setTimeout(() => {
        captureAndSendCameraFrame();
        debugLog("[Camera] Initial snapshot sent.");
      }, 500);

      // Start periodic captures — same 15s interval as screen share
      if (cameraIntervalRef.current) clearInterval(cameraIntervalRef.current);
      cameraIntervalRef.current = setInterval(() => {
        if (!isCameraActiveRef.current || !ws.current || ws.current.readyState !== WebSocket.OPEN) {
          if (cameraIntervalRef.current) clearInterval(cameraIntervalRef.current);
          cameraIntervalRef.current = null;
          return;
        }
        captureAndSendCameraFrame();
        debugLog("[Camera] Periodic snapshot sent.");
      }, 15000);

    } catch (err) {
      console.error("[Camera] Failed to start:", err);
      const msg = (err as Error).message || "Unknown camera error";
      if (msg.includes("NotAllowedError") || msg.includes("Permission")) {
        setError("Camera permission denied");
      } else {
        setError("Camera failed: " + msg);
      }
    }
  }, [facingMode, captureAndSendCameraFrame]);

  /**
   * Stops camera capture
   */
  const stopCamera = useCallback(() => {
    if (cameraIntervalRef.current) {
      clearInterval(cameraIntervalRef.current);
      cameraIntervalRef.current = null;
    }
    if (cameraStreamRef.current) {
      cameraStreamRef.current.getTracks().forEach(track => track.stop());
      cameraStreamRef.current = null;
    }
    if (cameraVideoRef.current) {
      cameraVideoRef.current.pause();
      cameraVideoRef.current.srcObject = null;
      cameraVideoRef.current = null;
    }
    setIsCameraActive(false);
    isCameraActiveRef.current = false;
    debugLog("[Camera] Camera stopped.");
  }, []);

  /**
   * Flips from front to rear camera (or vice versa)
   */
  const flipCamera = useCallback(() => {
    const newMode = facingMode === "environment" ? "user" : "environment";
    setFacingMode(newMode);
    if (isCameraActiveRef.current) {
      stopCamera();
      setTimeout(() => startCamera(newMode), 300);
    }
  }, [facingMode, stopCamera, startCamera]);

  const captureScreenSnapshot = useCallback(() => {
    if (!videoRef.current || !screenStream.current) {
        console.warn("[Vision] Capture failed: No video or stream.");
        return null;
    }

    if (!canvasRef.current) {
      canvasRef.current = document.createElement("canvas");
    }

    const video = videoRef.current;
    const canvas = canvasRef.current;
    
    if (video.videoWidth === 0 || video.videoHeight === 0) {
        console.warn("[Vision] Capture failed: Video dimensions are 0.");
        return null;
    }

    // Downscale to max 512px on longest side (matches GPT-4o "low" detail)
    const MAX_DIM = 512;
    const scale = Math.min(MAX_DIM / video.videoWidth, MAX_DIM / video.videoHeight, 1);
    canvas.width = Math.round(video.videoWidth * scale);
    canvas.height = Math.round(video.videoHeight * scale);

    const ctx = canvas.getContext("2d");
    if (!ctx) return null;

    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    
    // Lower quality since images are already small
    return canvas.toDataURL("image/jpeg", 0.5);
  }, []);

  /**
   * Initializes and starts the audio capture pipeline (Mic -> Worklet -> WebSocket)
   */
  const startAudioPipeline = useCallback(async () => {
    if (!ws.current || ws.current.readyState !== WebSocket.OPEN) {
      debugLog("[Audio] ❌ WebSocket not open, cannot start pipeline.");
      return;
    }

    try {
      // Ensure audio is initialized (should be done by connect/initializeAudio already)
      if (!audioStream.current) {
         const success = await initializeAudio();
         if (!success) throw new Error("Audio initialization failed");
      }

      // 2. Load AudioWorklet module
      if (!audioContext.current) throw new Error("AudioContext is null");
      
      debugLog("[Audio] Loading AudioWorklet module...");
      try {
        // Use a robust path for the worklet
        const workletUrl = "/worklets/AudioWorkletProcessor.js";
        // Check if module is already added (not directly possible, but addModule is idempotent-ish or throws)
        // We'll just try adding it.
        await audioContext.current.audioWorklet.addModule(workletUrl);
        debugLog("[Audio] AudioWorklet module loaded.");
      } catch (e) {
        // Ignore error if module already added (DOMException)
        debugLog("[Audio] Worklet might already be loaded:", e);
      }

      // 3. Create the Worklet Node
      if (!audioWorkletNode.current) {
          audioWorkletNode.current = new AudioWorkletNode(
            audioContext.current,
            "audio-worklet-processor",
            {
              processorOptions: {
                targetSampleRate: 16000,
              },
            }
          );
          
          audioWorkletNode.current.onprocessorerror = (err) => {
            console.error("[Audio] Worklet processor error:", err);
          };

          // 5. Connect the Worklet to the main app (this hook)
          audioWorkletNode.current.port.onmessage = (event) => {
            // ... (Existing message handler logic) ...
            // Handle Debug Messages from Worklet
            if (event.data && event.data.type === "debug") {
               debugLog("[AudioWorklet]", event.data.message);
               return;
            }
    
            // We received a 16-bit PCM buffer from the worklet
            const pcmBuffer = event.data as ArrayBuffer;
    
            // Safety: skip empty/detached buffers
            if (!pcmBuffer || pcmBuffer.byteLength === 0) return;

            // Calculate Mic Volume (RMS)
            const pcmData = new Int16Array(pcmBuffer);
            if (pcmData.length === 0) return;

            let sum = 0;
            for (let i = 0; i < pcmData.length; i++) {
              sum += pcmData[i] * pcmData[i];
            }
            const rms = Math.sqrt(sum / pcmData.length);
            // Normalize (16-bit max is 32768)
            // Multiply by a factor to make it more sensitive visually
            const rawVolume = Math.min(1, (rms / 32768) * 5);
            
            setMicVolume((prev) => {
                const smoothingFactor = 0.3; 
                return prev * (1 - smoothingFactor) + rawVolume * smoothingFactor;
            });
    
            if (
              ws.current?.readyState === WebSocket.OPEN &&
              isServerReady.current
            ) {
              // Only send audio when listening — no interrupt feature
              if (kiraStateRef.current === "listening") {
                ws.current.send(pcmBuffer);
              }
    
              // VAD & EOU Logic — only runs in listening state
              // (no interrupt feature; Kira finishes her response before we process speech)
              if (kiraStateRef.current === "listening") {
              const VAD_THRESHOLD = 300; 
              const isSpeakingFrame = rms > VAD_THRESHOLD;
    
              if (isSpeakingFrame) {
                speechFrameCount.current++;
                totalSpeechFrames.current++;
              } else {
                speechFrameCount.current = 0;
              }
    
              const isSpeaking = speechFrameCount.current > VAD_STABILITY_FRAMES;

              // Mark that the user has spoken enough to warrant an EOU
              if (totalSpeechFrames.current >= MIN_SPEECH_FRAMES_FOR_EOU) {
                hasSpoken.current = true;
              }
    
              if (isSpeaking) {
                // --- VISION: Snapshot-on-Speech ---
                // If this is the START of speech (transition from silence), capture a frame
                // Cooldown prevents re-triggering from micro-dips in natural speech
                if (speechFrameCount.current === (VAD_STABILITY_FRAMES + 1) && totalSpeechFrames.current >= 100) {
                    const now = Date.now();
                    if (now - lastSnapshotTime.current > SNAPSHOT_COOLDOWN_MS) {
                        // Screen share path
                        if (isScreenSharingRef.current) {
                            lastSnapshotTime.current = now;
                            debugLog("[Vision] Speech start detected while screen sharing. Attempting capture...");
                            const snapshot = captureScreenSnapshot();
                            if (snapshot) {
                                debugLog("[Vision] Sending snapshot on speech start...");
                                const payload = {
                                    type: "image",
                                    images: [...sceneBufferRef.current, snapshot]
                                };
                                ws.current.send(JSON.stringify(payload));
                            } else {
                                console.warn("[Vision] Snapshot capture returned null.");
                            }
                        }
                        // Camera path (mobile)
                        if (isCameraActiveRef.current) {
                            lastSnapshotTime.current = now;
                            debugLog("[Camera] Sending snapshot on speech start...");
                            captureAndSendCameraFrame();
                        }
                    }
                }

                // User is speaking — cancel any pending EOU timer
                if (eouTimer.current) {
                  clearTimeout(eouTimer.current);
                  eouTimer.current = null;
                }
    
                if (!maxUtteranceTimer.current) {
                  maxUtteranceTimer.current = setTimeout(() => {
                    debugLog("[EOU] Max utterance length reached. Forcing EOU.");
                    if (ws.current?.readyState === WebSocket.OPEN) {
                      eouSentAt.current = Date.now();
                      firstAudioLogged.current = false;
                      debugLog(`[Latency] EOU sent at ${eouSentAt.current}`);
                      ws.current.send(JSON.stringify({ type: "eou", forced: true }));
                    }
                    if (eouTimer.current) clearTimeout(eouTimer.current);
                    eouTimer.current = null;
                    maxUtteranceTimer.current = null;
                    // Reset speech tracking for next utterance
                    totalSpeechFrames.current = 0;
                    hasSpoken.current = false;
                  }, 60000); 
                }
              } else {
                // Silence detected — start EOU timer if user has spoken enough
                if (!eouTimer.current && hasSpoken.current) {
                  const adaptiveTimeout = getAdaptiveEOUTimeout();
                  eouTimer.current = setTimeout(() => {
                    debugLog(`[EOU] Silence detected after speech (${totalSpeechFrames.current} speech frames, timeout: ${adaptiveTimeout}ms), sending End of Utterance.`);
                    if (ws.current?.readyState === WebSocket.OPEN) {
                      eouSentAt.current = Date.now();
                      firstAudioLogged.current = false;
                      debugLog(`[Latency] EOU sent at ${eouSentAt.current}`);
                      ws.current.send(JSON.stringify({ type: "eou" }));
                    }
                    eouTimer.current = null;
                    if (maxUtteranceTimer.current) {
                      clearTimeout(maxUtteranceTimer.current);
                      maxUtteranceTimer.current = null;
                    }
                    // Reset speech tracking for next utterance
                    totalSpeechFrames.current = 0;
                    hasSpoken.current = false;
                  }, adaptiveTimeout);
                }
              }
              } // end if (kiraStateRef.current === "listening")
            }
          };
      }

      // 4. Connect the Mic to the Worklet (if not already)
      if (audioSource.current) audioSource.current.disconnect();
      
      debugLog("[Audio] Connecting mic to worklet...");
      if (audioStream.current) {
        audioSource.current = audioContext.current.createMediaStreamSource(
          audioStream.current
        );
        audioSource.current.connect(audioWorkletNode.current);
      } else {
        console.error("[Audio] No audio stream available to connect.");
      }

      // WORKAROUND: Connect worklet to a silent destination
      const silentGain = audioContext.current.createGain();
      silentGain.gain.value = 0;
      audioWorkletNode.current.connect(silentGain);
      silentGain.connect(audioContext.current.destination);

      debugLog("[Audio] ✅ Audio pipeline started.");
    } catch (err) {
      console.error("[Audio] ❌ Failed to start audio pipeline:", err);
      setError("Microphone access denied or failed. Please check permissions.");
    }
  }, [stopAudioPlayback, initializeAudio, captureScreenSnapshot]);

  /**
   * Explicitly start the conversation: send start_stream and start mic pipeline.
   * Adds detailed logs to trace user action and pipeline startup.
   */
  const startConversation = useCallback(() => {
    debugLog("[StartConvo] startConversation called. ws exists:", !!ws.current, "readyState:", ws.current?.readyState, "conversationActive:", conversationActive.current);
    
    // Idempotent — if conversation is already active (e.g. restored from singleton after remount), skip
    if (conversationActive.current) {
      debugLog("[StartConvo] Already active — skipping duplicate start_stream");
      return;
    }
    
    if (ws.current && ws.current.readyState === WebSocket.OPEN) {
      debugLog("[StartConvo] Sending 'start_stream' message...");
      try {
        ws.current.send(JSON.stringify({ type: "start_stream" }));
        conversationActive.current = true; // Mark session as live — no more auto-reconnect
        getConnectionStore()!.conversationActive = true;
        debugLog("[StartConvo] start_stream sent, conversationActive=true");
      } catch (err) {
        debugLog("[StartConvo] ❌ Failed to send start_stream:", err);
      }
      
      // Start mic immediately to satisfy browser user-gesture requirements
      debugLog("[StartConvo] Starting local audio pipeline...");
      startAudioPipeline();
    } else {
      debugLog(
        "[StartConvo] ❌ Cannot start: WebSocket not open. ws:", !!ws.current, "readyState:", ws.current?.readyState
      );
    }
  }, [startAudioPipeline]);

  /**
   * Signal that the visual layer (Live2D avatar or orb) is ready.
   * If the WebSocket is already open but waiting, this triggers start_stream + mic pipeline.
   */
  const signalVisualReady = useCallback(() => {
    if (visualReadyRef.current) return; // Already signaled
    visualReadyRef.current = true;
    debugLog("[VisualReady] Visual layer ready. wsOpen:", wsOpenRef.current, "conversationActive:", conversationActive.current);

    if (wsOpenRef.current && !conversationActive.current) {
      debugLog("[VisualReady] WS already open — sending start_stream now");
      startConversation();
    }
  }, [startConversation]);

  /**
   * Explicitly resume audio contexts.
   * Call this from a user gesture (click/tap) if audio is blocked.
   */
  const resumeAudio = useCallback(async () => {
    await initializeAudio();
  }, [initializeAudio]);

  /**
   * Main connection logic
   */
  const connect = useCallback(async () => {
    if (ws.current) {
      debugLog("[Connect] Aborted — WebSocket already exists");
      return;
    }

    debugLog("[Connect] Starting connection attempt...");

    // Initialize Audio IMMEDIATELY (Synchronously inside gesture if possible)
    const audioOk = await initializeAudio();
    debugLog(`[Connect] Audio initialized: ${audioOk}`);
    if (!audioOk) {
      debugLog("[Connect] ❌ Failed: audio initialization returned false (mic denied or AudioContext failed)");
      return;
    }

    // Fetch a FRESH auth token right before connecting — prevents stale JWT race conditions
    // (token fetched at mount time can expire before the user clicks "start")
    let freshToken: string | null = null;
    if (getTokenFn) {
      try {
        freshToken = await getTokenFn();
        debugLog("[Connect] Auth token fetched successfully");
      } catch (err) {
        debugLog("[Connect] ❌ Failed to get fresh token:", err);
      }
    }

    const wsUrl = process.env.NEXT_PUBLIC_WEBSOCKET_URL!;
    const authParam = freshToken ? `token=${freshToken}` : `guestId=${guestId}`;
    const voiceParam = `&voice=${voicePreference}`;
    debugLog(`[Connect] Opening WS: ${wsUrl}?${authParam}${voiceParam}`);

    debugLog("[State] socketState → connecting");
    setSocketState("connecting");
    getConnectionStore()!.socketState = "connecting";
    isServerReady.current = false;
    ws.current = new WebSocket(`${wsUrl}?${authParam}${voiceParam}`);
    ws.current.binaryType = "arraybuffer"; // We are sending and receiving binary

    // ─── Store to singleton immediately so remounts can find it ───
    debugLog("[Singleton] getConnectionStore()!.ws → WebSocket (from connect, pre-open)");
    getConnectionStore()!.ws = ws.current;

    ws.current.onopen = () => {
      debugLog("[State] socketState → connected");
      setSocketState("connected");
      getConnectionStore()!.socketState = "connected";
      reconnectAttempts.current = 0; // Reset on successful connection
      getConnectionStore()!.reconnectAttempts = 0;
      setError(null); // Clear any error banner from a previous disconnect
      debugLog("[Connect] ✅ WebSocket connected. Singleton stored immediately.");
      // Store audio refs to singleton now that connection is live
      getConnectionStore()!.audioContext = audioContext.current;
      getConnectionStore()!.playbackContext = playbackContext.current;
      getConnectionStore()!.audioStream = audioStream.current;
      
      // ─── Visual-ready gating ───
      // Don't send start_stream until the visual layer (Live2D / orb) signals ready.
      // This prevents the server from sending audio before the user sees anything.
      // Fallback: 15s timeout so we never hang forever if Live2D fails silently.
      wsOpenRef.current = true;

      if (visualReadyRef.current) {
        // Visual is already loaded (orb mode, or Live2D preloaded fast) — start immediately
        debugLog("[Connect] Visual already ready — sending start_stream now");
        if (!conversationActive.current) {
          startConversation();
        }
      } else {
        // Visual not ready yet — wait for signalVisualReady() or 15s timeout
        debugLog("[Connect] Waiting for visual ready signal (15s timeout)...");
        setTimeout(() => {
          if (!conversationActive.current && wsOpenRef.current) {
            debugLog("[Connect] ⏱ Visual-ready timeout (15s) — sending start_stream anyway");
            startConversation();
          }
        }, 15000);
      }
    };

    // ─── Wire handlers through refs so remounts get fresh closures ───
    onMessageRef.current = (event: MessageEvent) => {
      if (typeof event.data === "string") {
        // This is a JSON control message
        const msg = JSON.parse(event.data);
        debugLog("[WS] ← message:", msg.type, msg.type === "session_config" ? JSON.stringify(msg).slice(0, 200) : "");

        switch (msg.type) {
          case "session_config":
            debugLog("[WS] Received session_config:", JSON.stringify(msg));
            setIsPro(msg.isPro);
            isProRef.current = msg.isPro;
            if (msg.remainingSeconds !== undefined) {
              setRemainingSeconds(msg.remainingSeconds);
            }
            break;
          case "stream_ready":
            debugLog("[WS] Received stream_ready — setting kiraState to listening");
            setKiraState("listening");
            isServerReady.current = true;
            getConnectionStore()!.isServerReady = true;
            break;
          case "ping":
            // Respond to server heartbeat to keep connection alive
            if (ws.current?.readyState === WebSocket.OPEN) {
                ws.current.send(JSON.stringify({ type: "pong" }));
            }
            break;
          case "state_thinking":
            kiraStateRef.current = "thinking";
            if (eouTimer.current) clearTimeout(eouTimer.current); // Stop EOU timer
            setKiraState("thinking");
            break;
          case "state_speaking":
            kiraStateRef.current = "speaking";
            setKiraState("speaking");
            // CRITICAL: Stop any audio still playing from a previous turn
            // This prevents double-speak when a proactive comment overlaps with a user response
            scheduledSources.current.forEach((source) => {
              try { source.stop(); } catch (e) { /* already stopped */ }
            });
            scheduledSources.current = [];
            playbackSource.current = null;
            audioQueue.current = [];
            if (playbackContext.current) {
              nextStartTime.current = playbackContext.current.currentTime;
            } else {
              nextStartTime.current = 0;
            }
            if (audioPlayingTimeout.current) {
              clearTimeout(audioPlayingTimeout.current);
              audioPlayingTimeout.current = null;
            }
            ttsChunksDone.current = false;
            break;
          case "state_listening":
            kiraStateRef.current = "listening";
            setKiraState("listening");
            break;
          case "transcript":
            setTranscript({ role: msg.role, text: msg.text });
            break;
          case "expression":
            setCurrentExpression(msg.expression || "neutral");
            // Handle action/accessory fields from context detection
            if (msg.action) setCurrentAction(msg.action);
            if (msg.accessory && !HAIR_ACCESSORIES.has(msg.accessory)) {
              setActiveAccessories(prev =>
                prev.includes(msg.accessory) ? prev : [...prev, msg.accessory]
              );
            }
            if (msg.removeAccessory && !HAIR_ACCESSORIES.has(msg.removeAccessory)) {
              setActiveAccessories(prev =>
                prev.filter((a: string) => a !== msg.removeAccessory)
              );
            }
            break;
          case "accessory": {
            const { accessory, action } = msg;
            // Hair accessories are managed by the cycle timer — ignore server commands
            if (HAIR_ACCESSORIES.has(accessory)) break;
            setActiveAccessories(prev => {
              if (action === "on") {
                return prev.includes(accessory) ? prev : [...prev, accessory];
              } else {
                return prev.filter(a => a !== accessory);
              }
            });
            break;
          }
          case "tts_chunk_starts":
            ttsChunksDone.current = false; // More audio chunks incoming
            break;
          case "tts_chunk_ends":
            // The server is done sending audio for this turn
            ttsChunksDone.current = true; // Visualizer can now self-terminate when queue drains
            break;
          case "interrupt":
            // Server detected barge-in — immediately stop all audio playback
            scheduledSources.current.forEach((source) => {
              try { source.stop(); } catch (e) { /* already stopped */ }
            });
            scheduledSources.current = [];
            playbackSource.current = null;
            audioQueue.current = [];
            if (playbackContext.current) {
              nextStartTime.current = playbackContext.current.currentTime;
            }
            if (audioPlayingTimeout.current) {
              clearTimeout(audioPlayingTimeout.current);
              audioPlayingTimeout.current = null;
            }
            ttsChunksDone.current = true;
            console.log("[WS] Interrupt received — audio stopped");
            break;
          case "text_response":
            setTranscript({ role: "ai", text: msg.text });
            // Orb goes to "speaking" briefly to visually acknowledge
            kiraStateRef.current = "speaking";
            setKiraState("speaking");
            setTimeout(() => {
              kiraStateRef.current = "listening";
              setKiraState("listening");
            }, 1500);
            break;
          case "error":
            if (msg.code === "limit_reached") {
              if (msg.tier === "pro") {
                debugLog("[WS] ⚠️ Pro monthly limit reached.");
                setError("limit_reached_pro");
              } else {
                debugLog("[WS] ⚠️ Daily limit reached.");
                setError("limit_reached");
              }
            } else {
              debugLog("[WS] ❌ Server error:", msg.message);
              setError(msg.message);
            }
            break;
        }
      } else if (event.data instanceof ArrayBuffer) {
        // This is a raw PCM audio chunk from Azure
        // Only process audio if we are in 'speaking' state.
        // If we are 'listening' (e.g. due to interruption), we drop these packets.
        if (kiraStateRef.current === "speaking") {
            if (!firstAudioLogged.current && eouSentAt.current > 0) {
              firstAudioLogged.current = true;
              debugLog(`[Latency] Client: EOU → first audio: ${Date.now() - eouSentAt.current}ms`);
            }
            audioQueue.current.push(event.data);
            processAudioQueue();
        }
      }
    };

    onCloseRef.current = (event: CloseEvent) => {
      debugLog("[WS] 🔌 Connection closed. Code:", event.code, "Reason:", event.reason, "Clean:", event.wasClean);
      debugLog("[Singleton] getConnectionStore()!.ws → null (from onclose). Caller:", new Error().stack?.split('\n')[1]?.trim());
      debugLog("[State] socketState → closed (from onclose)");
      setSocketState("closed");
      getConnectionStore()!.socketState = "closed";
      wsOpenRef.current = false; // WS is no longer open
      
      // ─── Clear singleton ───
      getConnectionStore()!.ws = null;
      getConnectionStore()!.isServerReady = false;
      getConnectionStore()!.conversationActive = false;

      if (event.code === 1008) {
        // Don't overwrite a more specific error (e.g. "limit_reached_pro")
        // If user is Pro, always use "limit_reached_pro" — never show the free-tier paywall
        setError((prev) => {
          if (prev?.startsWith("limit_reached")) return prev;
          return isProRef.current ? "limit_reached_pro" : "limit_reached";
        });
      } else if (event.code === 4000) {
        // Heartbeat timeout — connection went stale (network issue, suspended tab, etc.)
        debugLog("[WS] Heartbeat timeout — connection went stale");
        setError("heartbeat_timeout");
      }

      stopAudioPipeline();
      ws.current = null;
      isServerReady.current = false; // Prevent stale audio sends on reconnect

      // Auto-reconnect logic:
      // ONLY reconnect if the conversation hasn't started yet (pre-stream connection flakiness).
      // Once a live voice session is active, reconnecting would create a fresh server session
      // (new chatHistory, new usage timer, new opener) — causing the "conversation loop" bug
      // where the same exchange replays and the usage counter goes backwards.
      if (event.code !== 1000 && event.code !== 1008 && event.code !== 4000) {
        if (conversationActive.current) {
          // Live session was interrupted — don't reconnect, show error
          debugLog("[WS] Connection lost during active conversation — not reconnecting (would create duplicate session)");
          setError("connection_lost");
        } else if (reconnectAttempts.current < MAX_RECONNECT_ATTEMPTS) {
          // Pre-conversation connection drop — safe to retry
          const delay = Math.min(1000 * Math.pow(2, reconnectAttempts.current), 10000);
          reconnectAttempts.current++;
          getConnectionStore()!.reconnectAttempts = reconnectAttempts.current;
          debugLog(`[WS] Reconnecting in ${delay}ms (attempt ${reconnectAttempts.current}/${MAX_RECONNECT_ATTEMPTS})...`);
          setTimeout(() => {
            connect();
          }, delay);
        } else {
          // All reconnect attempts exhausted
          setError("Connection lost. Please refresh the page.");
        }
      }
    };

    onErrorRef.current = (err: Event) => {
      debugLog("[WS] ❌ WebSocket error event fired:", err);
      // Don't null getConnectionStore()!.ws here — onclose ALWAYS fires after onerror
      // and handles singleton cleanup + reconnect logic. Nulling here would race.
      // Don't set socketState or call stopAudioPipeline — let onclose handle it all.
    };

    // ─── Wire WS events through refs (so remounts refresh closures) ───
    ws.current.onmessage = (e) => onMessageRef.current?.(e);
    ws.current.onclose = (e) => onCloseRef.current?.(e);
    ws.current.onerror = (e) => onErrorRef.current?.(e);

  }, [getTokenFn, guestId, startConversation, startAudioPipeline, processAudioQueue, stopAudioPipeline, initializeAudio]);

  const disconnect = useCallback(() => {
    debugLog("[WS] disconnect() called. ws.current exists:", !!ws.current);
    if (eouTimer.current) clearTimeout(eouTimer.current);
    reconnectAttempts.current = MAX_RECONNECT_ATTEMPTS; // Prevent any reconnection
    conversationActive.current = false; // Clean shutdown — not a crash
    visualReadyRef.current = false; // Reset for next session
    wsOpenRef.current = false;
    // ─── Clear singleton — this is an intentional disconnect ───
    debugLog("[Singleton] getConnectionStore()!.ws → null (from disconnect)");
    getConnectionStore()!.ws = null;
    getConnectionStore()!.socketState = "closing";
    getConnectionStore()!.isServerReady = false;
    getConnectionStore()!.conversationActive = false;
    getConnectionStore()!.reconnectAttempts = 0;
    if (ws.current) {
      debugLog("[State] socketState → closing (from disconnect)");
      setSocketState("closing");
      ws.current.close(1000, "User ended call"); // Code 1000 = intentional close, won't trigger reconnect
    }
  }, []);

  /**
   * Helper function to create a WAV header for raw PCM data
   */
  const createWavHeader = (
    data: ArrayBuffer,
    sampleRate: number,
    sampleBits: number
  ): ArrayBuffer => {
    const dataLength = data.byteLength;
    const buffer = new ArrayBuffer(44 + dataLength);
    const view = new DataView(buffer);

    const writeString = (offset: number, str: string) => {
      for (let i = 0; i < str.length; i++) {
        view.setUint8(offset + i, str.charCodeAt(i));
      }
    };

    const channels = 1;
    const byteRate = (sampleRate * channels * sampleBits) / 8;
    const blockAlign = (channels * sampleBits) / 8;

    writeString(0, "RIFF");
    view.setUint32(4, 36 + dataLength, true);
    writeString(8, "WAVE");
    writeString(12, "fmt ");
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, channels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, byteRate, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, sampleBits, true);
    writeString(36, "data");
    view.setUint32(40, dataLength, true);

    // Copy the PCM data
    const pcm = new Uint8Array(data);
    const dataView = new Uint8Array(buffer, 44);
    dataView.set(pcm);

    return buffer;
  };

  /**
   * Send a text message (text chat mode — skips STT/TTS)
   */
  const sendText = useCallback((text: string) => {
    if (ws.current?.readyState === WebSocket.OPEN) {
      ws.current.send(JSON.stringify({ type: "text_message", text }));
      setTranscript({ role: "user", text });
    }
  }, []);

  const sendVoiceChange = useCallback((voice: string) => {
    if (ws.current?.readyState === WebSocket.OPEN) {
      ws.current.send(JSON.stringify({ type: "voice_change", voice }));
      debugLog(`[WS] Sent voice_change: ${voice}`);
    }
  }, []);

  return {
    connect,
    disconnect,
    startConversation,
    signalVisualReady,
    socketState,
    kiraState,
    micVolume,
    transcript,
    sendText,
    sendVoiceChange,
    error,
    isAudioBlocked,
    resumeAudio,
    isMuted,
    toggleMute,
    isScreenSharing,
    startScreenShare,
    stopScreenShare,
    isCameraActive,
    cameraStreamRef,
    facingMode,
    startCamera,
    stopCamera,
    flipCamera,
    isPro,
    remainingSeconds,
    isAudioPlaying,
    playerVolume,
    playbackAnalyserNode: playbackAnalyser.current,
    currentExpression,
    activeAccessories,
    currentAction,
  };
};


===== packages/server/src/server.ts =====
import { WebSocketServer } from "ws";
import type { IncomingMessage } from "http";
import { createServer } from "http";
import { URL } from "url";
import prisma from "./prismaClient.js";
import { createClerkClient, verifyToken } from "@clerk/backend";
import { OpenAI } from "openai";
import { DeepgramSTTStreamer } from "./DeepgramSTTStreamer.js";
import { AzureTTSStreamer } from "./AzureTTSStreamer.js";
import type { AzureVoiceConfig } from "./AzureTTSStreamer.js";
import { KIRA_SYSTEM_PROMPT } from "./personality.js";
import { extractAndSaveMemories } from "./memoryExtractor.js";
import { loadUserMemories } from "./memoryLoader.js";
import { bufferGuestConversation, getGuestBuffer, clearGuestBuffer } from "./guestMemoryBuffer.js";
import { getGuestUsage, getGuestUsageInfo, saveGuestUsage } from "./guestUsage.js";
import { getProUsage, saveProUsage } from "./proUsage.js";

// --- VISION CONTEXT PROMPT (injected dynamically when screen share is active) ---
const VISION_CONTEXT_PROMPT = `

[VISUAL FEED ACTIVE]
You can see the user's world right now through shared images. These may come from screen share (desktop) or camera (mobile). You have FULL ability to:
- Read any text on screen (titles, subtitles, UI elements, chat messages, code, articles, etc.)
- Identify what app, website, game, or media is being shown
- See visual details like colors, characters, scenes, layouts, faces, objects, environments
- Understand context from what's visible

When the user asks you about what you see, look carefully at the images and give specific, detailed answers. You CAN read text — describe exactly what you see. If they ask "what does it say?" or "can you read that?" — read it word for word.

CONTEXT DETECTION — Adapt your unprompted behavior based on what's happening:
- MEDIA (anime, movies, TV, YouTube, streams): Be a quiet co-watcher. Keep unprompted reactions to 1-8 words.
- CREATIVE WORK (coding, writing, design): Don't comment unless asked. When asked, reference specifics.
- BROWSING (social media, shopping, articles): Light commentary okay. Don't narrate.
- GAMING: React like a friend watching. Keep it short unless asked.
- CONVERSATION (Discord, messages, calls): Stay quiet unless addressed.
- CAMERA (seeing the user's face or surroundings): Be warm and natural. You might see their room, their face, something they're showing you. React like a friend on a video call. Be thoughtful about personal appearance — compliment genuinely but don't critique. If they're showing you something specific, focus on that.

UNPROMPTED BEHAVIOR (when the user is NOT talking to you):
- Keep unprompted reactions brief (1-2 sentences max)
- React like a friend in the room, not a narrator
- React to standout moments — interesting visuals, mood shifts, cool details
- Match the energy: quiet during emotional scenes, excited during hype moments
- You should react to something every so often — your presence matters. Being totally silent makes the user feel alone.

WHEN THE USER ASKS YOU SOMETHING:
- Give full, specific answers. Reference what you see in detail.
- Read text on screen if asked. You have full OCR-level ability.
- Help with code, explain what's on screen, identify characters — whatever they need.
- Don't be artificially brief when the user wants information. Answer thoroughly.
- Your awareness of the screen should feel natural, like a friend in the same room.`;

// --- CONFIGURATION ---
const PORT = process.env.PORT ? parseInt(process.env.PORT, 10) : 10000;
const CLERK_SECRET_KEY = process.env.CLERK_SECRET_KEY!;
const OPENAI_API_KEY = process.env.OPENAI_API_KEY!;
const OPENAI_MODEL = process.env.OPENAI_MODEL || "gpt-4o-mini";

// --- INLINE LLM EMOTION TAGGING ---
// The LLM prefixes every response with [EMO:emotion] (optionally |ACT:action|ACC:accessory).
// We parse this tag from the first tokens of the stream, send expression data to the client,
// then strip the tag before TTS/history/transcript.

const VALID_EMOTIONS = new Set([
  "neutral", "happy", "excited", "love", "blush", "sad", "angry",
  "playful", "thinking", "speechless", "eyeroll", "sleepy",
  "frustrated", "confused", "surprised"
]);

const VALID_ACTIONS = new Set([
  "hold_phone", "hold_lollipop", "hold_pen", "hold_drawing_board",
  "gaming", "hold_knife"
]);

const VALID_ACCESSORIES = new Set([
  "glasses", "headphones_on", "cat_mic"
]);

interface ParsedExpression {
  emotion: string;
  action?: string;
  accessory?: string;
}

/** Parse an [EMO:...] tag string into structured expression data.
 *  Lenient: case-insensitive, flexible whitespace, ignores unknown fields. */
function parseExpressionTag(raw: string): ParsedExpression | null {
  const match = raw.match(/\[\s*EMO\s*:\s*(\w+)(?:\s*\|\s*ACT\s*:\s*(\w+))?(?:\s*\|\s*ACC\s*:\s*(\w+))?[^\]]*\]/i);
  if (!match) return null;

  const emotion = match[1].toLowerCase();
  if (!VALID_EMOTIONS.has(emotion)) return null;

  const action = match[2] ? (VALID_ACTIONS.has(match[2].toLowerCase()) ? match[2].toLowerCase() : undefined) : undefined;
  const accessory = match[3] ? (VALID_ACCESSORIES.has(match[3].toLowerCase()) ? match[3].toLowerCase() : undefined) : undefined;

  return { emotion, action, accessory };
}

/** Strip an [EMO:...] tag from the beginning of a response string. Returns clean text.
 *  Lenient: case-insensitive, flexible whitespace, handles unknown fields. */
function stripExpressionTag(text: string): string {
  return text.replace(/^\[\s*EMO\s*:\s*\w+(?:\s*\|[^\]]*)*\]\s*\n?/i, "").trim();
}

/** Strip any stray bracketed emotion words from response text (safety net). */
function stripEmotionTags(text: string): string {
  return text
    .replace(/\s*\[(neutral|happy|excited|love|blush|sad|angry|playful|thinking|speechless|eyeroll|sleepy|frustrated|confused|surprised)\]\s*$/gi, "")
    .replace(/^\[\s*EMO\s*:\s*\w+(?:\s*\|[^\]]*)*\]\s*\n?/i, "")
    .trim();
}

// --- Expression tag reminder (injected as last system message before user message) ---
// This is sent as a SEPARATE system message right at the end of the messages array,
// close to the model's attention window, to maximize tag compliance with smaller models.
const EXPRESSION_TAG_REMINDER = `IMPORTANT: Your VERY FIRST line must be an expression tag. Do NOT skip this.
Format: [EMO:<emotion>] or [EMO:<emotion>|ACT:<action>] or [EMO:<emotion>|ACC:<accessory>]

Emotions: neutral, happy, excited, love, blush, sad, angry, playful, thinking, speechless, eyeroll, sleepy, frustrated, confused, surprised
Actions (optional, only when relevant): hold_phone, hold_lollipop, hold_pen, hold_drawing_board, gaming, hold_knife
Accessories (optional, only when shifting mode): glasses, headphones_on, cat_mic

Example — if user says something sad:
[EMO:sad]
Oh no, that sounds rough...

Example — if user asks about games:
[EMO:excited|ACT:gaming]
Yes! Which game?

You MUST start with the tag. The user cannot see it.`;

const clerkClient = createClerkClient({ secretKey: CLERK_SECRET_KEY });
const openai = new OpenAI({ apiKey: OPENAI_API_KEY });

const server = createServer((req, res) => {
  if (req.url === "/health" || req.url === "/healthz") {
    res.writeHead(200, { "Content-Type": "text/plain" });
    res.end("ok");
    return;
  }

  // --- Guest buffer retrieval endpoint (called by Clerk webhook) ---
  if (req.url?.startsWith("/api/guest-buffer/") && req.method === "DELETE") {
    const authHeader = req.headers.authorization;
    if (!process.env.INTERNAL_API_SECRET || authHeader !== `Bearer ${process.env.INTERNAL_API_SECRET}`) {
      res.writeHead(401, { "Content-Type": "application/json" });
      res.end(JSON.stringify({ error: "Unauthorized" }));
      return;
    }
    const guestId = decodeURIComponent(req.url.split("/api/guest-buffer/")[1]);
    const buffer = getGuestBuffer(guestId);
    if (buffer) {
      clearGuestBuffer(guestId);
      res.writeHead(200, { "Content-Type": "application/json" });
      res.end(JSON.stringify(buffer));
    } else {
      res.writeHead(404, { "Content-Type": "application/json" });
      res.end(JSON.stringify({ error: "No buffer found" }));
    }
    return;
  }

  res.writeHead(404);
  res.end();
});
const wss = new WebSocketServer({ server, maxPayload: 5 * 1024 * 1024 });

  // --- Per-IP connection tracking ---
  const connectionsPerIp = new Map<string, number>();
  const MAX_CONNECTIONS_PER_IP = 5;

  console.log("[Server] Starting...");

wss.on("connection", (ws: any, req: IncomingMessage) => {
  // --- PER-IP CONNECTION LIMIT ---
  const clientIp = (req.headers["x-forwarded-for"] as string)?.split(",")[0]?.trim() || req.socket.remoteAddress || "unknown";
  const currentCount = connectionsPerIp.get(clientIp) || 0;
  if (currentCount >= MAX_CONNECTIONS_PER_IP) {
    console.warn(`[WS] Rejected connection from ${clientIp} — ${currentCount} active connections`);
    ws.close(1008, "Too many connections");
    return;
  }
  connectionsPerIp.set(clientIp, currentCount + 1);

  // --- ORIGIN VALIDATION ---
  const origin = req.headers.origin;
  const allowedOrigins = [
    "https://www.xoxokira.com",
    "https://xoxokira.com",
  ];
  // Allow localhost only in development
  if (process.env.NODE_ENV !== "production") {
    allowedOrigins.push("http://localhost:3000");
  }

  if (origin && !allowedOrigins.includes(origin)) {
    console.warn(`[WS] Rejected connection from origin: ${origin}`);
    ws.close(1008, "Origin not allowed");
    return;
  }

  console.log("[WS] New client connecting...");
  const url = new URL(req.url!, `wss://${req.headers.host}`);
  const token = url.searchParams.get("token");
  const guestId = url.searchParams.get("guestId");

  // Validate guestId format (must be guest_<uuid>)
  if (guestId && !/^guest_[a-f0-9-]{36}$/.test(guestId)) {
    console.warn(`[Auth] Rejected invalid guestId format: ${guestId}`);
    ws.close(1008, "Invalid guest ID format");
    return;
  }

  const voicePreference = (url.searchParams.get("voice") === "natural" ? "natural" : "anime") as "anime" | "natural";

  // Dual Azure voice configs — both go through the same AzureTTSStreamer pipeline
  const VOICE_CONFIGS: Record<string, AzureVoiceConfig> = {
    anime: {
      voiceName: process.env.AZURE_VOICE_ANIME || process.env.AZURE_TTS_VOICE || "en-US-AshleyNeural",
      style: process.env.AZURE_VOICE_ANIME_STYLE || undefined,
      rate: process.env.AZURE_TTS_RATE || "+25%",
      pitch: process.env.AZURE_TTS_PITCH || "+25%",
    },
    natural: {
      voiceName: process.env.AZURE_VOICE_NATURAL || "en-US-JennyNeural",
      style: process.env.AZURE_VOICE_NATURAL_STYLE || "soft voice",
      rate: process.env.AZURE_VOICE_NATURAL_RATE || undefined,
      pitch: process.env.AZURE_VOICE_NATURAL_PITCH || undefined,
      temperature: process.env.AZURE_VOICE_NATURAL_TEMP || "0.85",
      topP: process.env.AZURE_VOICE_NATURAL_TOP_P || "0.85",
    },
  };
  let currentVoiceConfig = VOICE_CONFIGS[voicePreference] || VOICE_CONFIGS.anime;
  console.log(`[Voice] Preference: "${voicePreference}", voice: ${currentVoiceConfig.voiceName} (style: ${currentVoiceConfig.style || "default"})`);

  // --- KEEP-ALIVE HEARTBEAT ---
  // Send a ping every 30 seconds to prevent load balancer timeouts (e.g. Render, Nginx)
  // If client doesn't respond with pong within 45s, close the connection gracefully
  let pongTimeoutTimer: NodeJS.Timeout | null = null;

  const keepAliveInterval = setInterval(() => {
    if (ws.readyState === ws.OPEN) {
      ws.send(JSON.stringify({ type: "ping" }));

      // Set a 45s timeout to receive pong (30s ping interval + 15s grace period)
      // If no pong received, the connection is likely stale (network issue, suspended tab, etc.)
      if (pongTimeoutTimer) clearTimeout(pongTimeoutTimer);
      pongTimeoutTimer = setTimeout(() => {
        console.warn(`[WS] No pong received for 45s from ${userId || 'guest'} — closing stale connection`);
        clientDisconnected = true;
        // Use 4000 (custom code) so client can handle heartbeat timeouts distinctly
        ws.close(4000, "Heartbeat timeout");
      }, 45000);
    }
  }, 30000);

  let userId: string | null = null;
  let isGuest = false;

  // --- 1. AUTH & USER SETUP ---
  if (!token && !guestId) {
    console.error("[Auth] ❌ No authentication provided. Closing connection.");
    ws.close(1008, "No authentication provided");
    return;
  }

  const authPromise = (async () => {
    try {
      if (token) {
        const payload = await verifyToken(token, { secretKey: CLERK_SECRET_KEY });
        if (!payload?.sub) {
          throw new Error("Unable to resolve user id from token");
        }
        userId = payload.sub;
        isGuest = false;
        console.log(`[Auth] ✅ Authenticated user: ${userId}`);
        return true;
      } else if (guestId) {
        userId = guestId; // Client already sends "guest_<uuid>"
        isGuest = true;
        console.log(`[Auth] - Guest user: ${userId}`);
        return true;
      } else {
        throw new Error("No auth provided.");
      }
    } catch (err) {
      console.error("[Auth] ❌ Failed:", (err as Error).message);
      ws.close(1008, "Authentication failed");
      return false;
    }
  })();

  // --- RATE LIMITING (control messages only — binary audio is exempt) ---
  const MAX_CONTROL_MESSAGES_PER_SECOND = 50;
  let messageCount = 0;
  const messageCountResetInterval = setInterval(() => { messageCount = 0; }, 1000);

  // --- LLM CALL RATE LIMITING (prevent abuse via rapid EOU/text_message spam) ---
  const LLM_MAX_CALLS_PER_MINUTE = 12;
  let llmCallCount = 0;
  const llmRateLimitInterval = setInterval(() => { llmCallCount = 0; }, 60000);

  // --- 2. PIPELINE SETUP ---
  let state: string = "listening";
  let stateTimeoutTimer: NodeJS.Timeout | null = null;
  let pendingEOU: string | null = null;

  function setState(newState: string) {
    state = newState;

    // Clear any existing safety timer
    if (stateTimeoutTimer) { clearTimeout(stateTimeoutTimer); stateTimeoutTimer = null; }

    // If not listening, set a 30s safety timeout
    if (newState !== "listening") {
      stateTimeoutTimer = setTimeout(() => {
        console.error(`[STATE] ⚠️ Safety timeout! Stuck in "${state}" for 30s. Forcing reset to listening.`);
        state = "listening";
        stateTimeoutTimer = null;
        // Notify client so UI stays in sync
        try { ws.send(JSON.stringify({ type: "state_listening" })); } catch (_) {}
        // Process any queued EOU
        if (pendingEOU) {
          const queued = pendingEOU;
          pendingEOU = null;
          console.log(`[EOU] Processing queued EOU after safety timeout: "${queued}"`);
          processEOU(queued);
        }
      }, 30000);
    } else {
      // Returning to listening — check for pending EOUs
      if (pendingEOU) {
        const queued = pendingEOU;
        pendingEOU = null;
        console.log(`[EOU] Processing queued EOU: "${queued}"`);
        // Use setImmediate to avoid re-entrancy
        setImmediate(() => processEOU(queued));
      }
    }
  }

  /** Re-inject a queued EOU transcript into the pipeline by simulating an eou message. */
  function processEOU(transcript: string) {
    if (state !== "listening") {
      console.warn(`[EOU] processEOU called but state is "${state}". Re-queuing.`);
      pendingEOU = transcript;
      return;
    }
    // Set the transcript so the EOU handler picks it up
    currentTurnTranscript = transcript;
    currentInterimTranscript = "";
    // Emit a synthetic EOU message through the ws handler
    ws.emit("message", Buffer.from(JSON.stringify({ type: "eou" })), false);
  }

  let sttStreamer: DeepgramSTTStreamer | null = null;
  let currentTurnTranscript = "";
  let currentInterimTranscript = "";
  let transcriptClearedAt = 0;
  let lastProcessedTranscript = "";
  let latestImages: string[] | null = null;
  let lastImageTimestamp = 0;
  let viewingContext = ""; // Track the current media context
  let lastEouTime = 0;
  const EOU_DEBOUNCE_MS = 600; // Ignore EOU if within 600ms of last one
  let consecutiveEmptyEOUs = 0;
  let lastTranscriptReceivedAt = Date.now();
  let isReconnectingDeepgram = false;
  let clientDisconnected = false;
  let timeWarningPhase: 'normal' | 'final_goodbye' | 'done' = 'normal';
  let goodbyeTimeout: NodeJS.Timeout | null = null;
  let isAcceptingAudio = false;
  let lastSceneReactionTime = 0;
  let visionActive = false;
  let lastVisionTimestamp = 0;
  let lastKiraSpokeTimestamp = 0;
  let lastUserSpokeTimestamp = 0;
  let lastExpressionActionTime = 0; // tracks when we last sent an action or accessory (for comfort cooldown)
  let interruptRequested = false; // set true when user barges in during speaking
  let currentResponseId = 0; // generation ID — prevents stale TTS callbacks from leaking audio into new turns
  let visionReactionTimer: ReturnType<typeof setTimeout> | null = null;
  let isFirstVisionReaction = true;

  // --- Comfort Arc: timed accessory progression ---
  let comfortStage = 0; // 0=default, 1=jacket off, 2=neck headphones, 3=earbuds
  let comfortTimer: NodeJS.Timeout | null = null;

  const COMFORT_STAGES = [
    { delay: 60000, expression: "remove_jacket", label: "jacket off" },          // 1 min
    { delay: 300000, expression: "neck_headphones", label: "neck headphones" },  // 5 min after jacket (6 min total)
    { delay: 600000, expression: "earbuds", label: "earbuds in" },               // 10 min after headphones (16 min total)
  ];

  const COMFORT_ACTION_COOLDOWN = 15000; // Don't send comfort accessory if action/accessory sent within 15s

  function startComfortProgression(ws: WebSocket) {
    // Check if late night (10pm-4am) — skip to stage 1 immediately
    const hour = new Date().getHours();
    if (hour >= 22 || hour < 4) {
      comfortStage = 1;
      ws.send(JSON.stringify({ type: "accessory", accessory: "remove_jacket", action: "on" }));
      console.log("[Comfort] Late night — starting with jacket off");
    }

    scheduleNextComfort(ws);
  }

  function scheduleNextComfort(ws: WebSocket) {
    if (comfortStage >= COMFORT_STAGES.length) return;

    const stage = COMFORT_STAGES[comfortStage];
    comfortTimer = setTimeout(() => {
      if (clientDisconnected || ws.readyState !== ws.OPEN) return;

      // Don't overwrite a recent action/accessory — retry in 15s
      const timeSinceAction = Date.now() - lastExpressionActionTime;
      if (timeSinceAction < COMFORT_ACTION_COOLDOWN) {
        const retryIn = COMFORT_ACTION_COOLDOWN - timeSinceAction + 1000; // +1s buffer
        console.log(`[Comfort] Stage ${comfortStage + 1} (${stage.label}) deferred — recent action/accessory (retry in ${(retryIn / 1000).toFixed(0)}s)`);
        comfortTimer = setTimeout(() => {
          if (clientDisconnected || ws.readyState !== ws.OPEN) return;
          ws.send(JSON.stringify({ type: "accessory", accessory: stage.expression, action: "on" }));
          console.log(`[Comfort] Stage ${comfortStage + 1}: ${stage.label} (deferred)`);
          comfortStage++;
          scheduleNextComfort(ws);
        }, retryIn);
        return;
      }

      ws.send(JSON.stringify({ type: "accessory", accessory: stage.expression, action: "on" }));
      console.log(`[Comfort] Stage ${comfortStage + 1}: ${stage.label}`);
      comfortStage++;
      scheduleNextComfort(ws);
    }, stage.delay);
  }

  // --- Dedicated Vision Reaction Timer (independent of silence checker) ---
  async function triggerVisionReaction() {
    if (state !== "listening") {
      console.log("[Vision Reaction] Skipping — state is:", state);
      return;
    }
    currentResponseId++;
    const thisResponseId = currentResponseId;
    // Note: vision reactions use state directly for local checks but setState() for transitions
    if (clientDisconnected) {
      console.log("[Vision Reaction] Skipping — client disconnected.");
      return;
    }
    if (!latestImages || latestImages.length === 0) {
      console.log(`[Vision Reaction] Skipping — no images in buffer. Last image received: ${lastImageTimestamp ? new Date(lastImageTimestamp).toISOString() : "never"}`);
      // Retry sooner — periodic captures should fill the buffer shortly
      setState("listening");
      if (visionActive && !clientDisconnected) {
        if (visionReactionTimer) clearTimeout(visionReactionTimer);
        visionReactionTimer = setTimeout(async () => {
          if (!visionActive || clientDisconnected) return;
          await triggerVisionReaction();
          if (visionActive && !clientDisconnected) scheduleNextReaction();
        }, 15000); // 15s retry — new images should arrive from periodic capture
      }
      return;
    }
    if (timeWarningPhase === 'done' || timeWarningPhase === 'final_goodbye') {
      console.log("[Vision Reaction] Skipping — session ending.");
      return;
    }

    console.log("[Vision Reaction] Timer fired. Generating reaction...");
    const visionStartAt = Date.now();
    setState("thinking");

    const firstReactionExtra = isFirstVisionReaction
      ? `\nThis is the FIRST moment you're seeing their screen. React with excitement about what you see — acknowledge that you can see it and comment on something specific. Examples:
- "Ooh nice, I love this anime!"
- "Oh wait I can see your screen now, this looks so good"
- "Ooh what are we watching? The art style is gorgeous"
- "Oh this anime! The vibes are immaculate already"
Keep it natural and brief — 1 sentence.`
      : "";

    // Cap at 2 most recent images for vision reactions to reduce latency
    const reactionImages = latestImages!.slice(-2);
    const reactionImageContent: OpenAI.Chat.ChatCompletionContentPart[] = reactionImages.map((img) => ({
      type: "image_url" as const,
      image_url: { url: img.startsWith("data:") ? img : `data:image/jpeg;base64,${img}`, detail: "low" as const },
    }));
    reactionImageContent.push({
      type: "text" as const,
      text: "(vision reaction check)",
    });

    const reactionMessages: OpenAI.Chat.ChatCompletionMessageParam[] = [
      {
        role: "system",
        content: KIRA_SYSTEM_PROMPT + VISION_CONTEXT_PROMPT + `\n\n[VISION MICRO-REACTION]\nYou are seeing the user's world right now through shared images (screen share or camera).\nLook at the current frames and react like a friend sitting next to them.\n\nYou MUST react to something. Find ANYTHING worth commenting on:\n- The art style, animation quality, lighting, colors\n- A character's expression or body language\n- The setting or background details (like "why does he have so many books?")\n- The mood or atmosphere of the scene\n- A plot moment ("wait is she about to...?")\n- Subtitles or dialogue you can read ("that line hit different")\n- Something funny, weird, beautiful, or emotional\n- If camera: the user's surroundings, something they're showing you, their vibe\n\nGood examples:\n- "the lighting in this scene is so warm"\n- "why does he have so many books though"\n- "her expression right there... she knows"\n- "this soundtrack is doing all the heavy lifting"\n- "the detail in this background is insane"\n- "wait what did he just say??"\n- "ok this is getting intense"\n- "I love how they animated the rain here"\n- "oh is that your cat??"\n- "that looks so cozy"\n- "where are you right now? it looks nice"\n\nRules:\n- 1-2 short sentences MAX (under 15 words total)\n- Be specific about what you see — reference actual visual details\n- Sound natural, like thinking out loud\n- Do NOT ask the user questions\n- Do NOT narrate the plot ("and then he walks to...")\n- Only respond with [SILENT] if the screen is literally a black/loading screen or a static menu with nothing happening. If there is ANY visual content, react to it.\nCRITICAL: Your response must be under 15 words. One short sentence only. No questions.\n` + firstReactionExtra,
      },
      ...chatHistory.filter(m => m.role !== "system").slice(-4),
      { role: "system", content: EXPRESSION_TAG_REMINDER },
      { role: "user", content: reactionImageContent },
    ];

    try {
      const reactionResponse = await openai.chat.completions.create({
        model: OPENAI_MODEL,
        messages: reactionMessages,
        max_tokens: 60,
        temperature: 0.95,
      });

      let reaction = reactionResponse.choices[0]?.message?.content?.trim() || "";
      console.log(`[Latency] Vision LLM: ${Date.now() - visionStartAt}ms`);

      // Check for actual silence tokens FIRST
      if (!reaction || reaction.includes("[SILENT]") || reaction.includes("[SKIP]") || reaction.startsWith("[") || reaction.length < 2) {
        console.log(`[Vision Reaction] LLM explicitly chose silence. Raw: "${reaction}"`);
        console.log("[Vision Reaction] Scheduling retry in 30-45 seconds instead of full cooldown.");
        setState("listening");

        // Don't wait the full 75-120s — retry sooner since we got silence
        if (visionActive && !clientDisconnected) {
          if (visionReactionTimer) clearTimeout(visionReactionTimer);
          visionReactionTimer = setTimeout(async () => {
            if (!visionActive || clientDisconnected) return;
            await triggerVisionReaction();
            if (visionActive && !clientDisconnected) scheduleNextReaction();
          }, 30000 + Math.random() * 15000); // 30-45 second retry after silence
        }
        return;
      }

      // Truncate if too long (but still use it — don't discard!)
      if (reaction.length > 120) {
        console.log(`[Vision Reaction] Response too long (${reaction.length} chars), truncating: "${reaction}"`);
        const firstSentence = reaction.match(/^[^.!?…]+[.!?…]/);
        if (firstSentence) {
          reaction = firstSentence[0].trim();
          console.log(`[Vision Reaction] Truncated to first sentence: "${reaction}"`);
        } else {
          reaction = reaction.substring(0, 80).trim() + "...";
          console.log(`[Vision Reaction] Hard truncated to: "${reaction}"`);
        }
      }

      // Parse expression tag and strip before TTS
      const visionTagResult = handleNonStreamingTag(reaction, "vision reaction");
      reaction = stripEmotionTags(visionTagResult.text);
      const visionEmotion = visionTagResult.emotion;

      console.log(`[Vision Reaction] Kira says: "${reaction}"`);
      chatHistory.push({ role: "assistant", content: reaction });
      lastKiraSpokeTimestamp = Date.now();
      isFirstVisionReaction = false;
      ws.send(JSON.stringify({ type: "transcript", role: "ai", text: reaction }));

      // TTS pipeline
      const visionTtsStart = Date.now();
      setState("speaking");
      ws.send(JSON.stringify({ type: "state_speaking" }));
      ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
      await new Promise(resolve => setImmediate(resolve));

      try {
        const sentences = reaction.split(/(?<=[.!?…])\s+(?=[A-Z"])/);
        let visionSentIdx = 0;
        interruptRequested = false; // Safe to reset — old TTS killed by generation ID
        for (const sentence of sentences) {
          const trimmed = sentence.trim();
          if (trimmed.length === 0) continue;
          if (interruptRequested || thisResponseId !== currentResponseId) {
            console.log(`[TTS] Vision sentence loop aborted (interrupt: ${interruptRequested}, stale: ${thisResponseId !== currentResponseId})`);
            break;
          }
          // Emotional pacing between sentences
          if (visionSentIdx > 0) {
            const delay = EMOTION_SENTENCE_DELAY[visionEmotion] || 0;
            if (delay > 0) await new Promise(resolve => setTimeout(resolve, delay));
          }
          visionSentIdx++;
          await new Promise<void>((resolve) => {
            const tts = new AzureTTSStreamer({ ...currentVoiceConfig, emotion: visionEmotion });
            tts.on("audio_chunk", (chunk: Buffer) => {
              if (interruptRequested || thisResponseId !== currentResponseId) return;
              if (!clientDisconnected && ws.readyState === ws.OPEN) ws.send(chunk);
            });
            tts.on("tts_complete", () => resolve());
            tts.on("error", (err: Error) => {
              console.error(`[Vision Reaction TTS] ❌ Chunk failed: "${trimmed}"`, err);
              resolve();
            });
            tts.synthesize(trimmed);
          });
        }
      } catch (ttsErr) {
        console.error("[Vision Reaction TTS] Pipeline error:", ttsErr);
      } finally {
        console.log(`[Latency] Vision TTS: ${Date.now() - visionTtsStart}ms`);
        console.log(`[Latency] Vision total: ${Date.now() - visionStartAt}ms`);
        ws.send(JSON.stringify({ type: "tts_chunk_ends" }));
        setState("listening");
        ws.send(JSON.stringify({ type: "state_listening" }));
      }
    } catch (err) {
      console.error("[Vision Reaction] Error:", (err as Error).message);
      setState("listening");
    }
  }

  function scheduleNextReaction() {
    const delay = 75000 + Math.random() * 45000; // 75-120 seconds
    console.log(`[Vision] Next reaction scheduled in ${Math.round(delay / 1000)}s`);
    visionReactionTimer = setTimeout(async () => {
      if (!visionActive || clientDisconnected) return;
      await triggerVisionReaction();
      if (visionActive && !clientDisconnected) {
        scheduleNextReaction();
      }
    }, delay);
  }

  function startVisionReactionTimer() {
    if (visionReactionTimer) { clearTimeout(visionReactionTimer); visionReactionTimer = null; }
    isFirstVisionReaction = true;
    // Fire first reaction almost immediately to establish presence
    // Small delay to let image buffer populate with a few frames
    const initialDelay = 4000 + Math.random() * 2000; // 4-6 seconds
    console.log(`[Vision] First reaction in ${Math.round(initialDelay / 1000)}s (immediate presence)`);
    visionReactionTimer = setTimeout(async () => {
      if (!visionActive || clientDisconnected) return;
      await triggerVisionReaction();
      if (visionActive && !clientDisconnected) {
        scheduleNextReaction();
      }
    }, initialDelay);
  }

  function stopVision() {
    if (visionReactionTimer) {
      clearTimeout(visionReactionTimer);
      visionReactionTimer = null;
      console.log("[Vision] Reaction timer cancelled — screen share ended");
    }
    latestImages = null;
    lastImageTimestamp = 0;
    visionActive = false;
    isFirstVisionReaction = true;
    console.log("[Vision] Screen share deactivated");
  }

  function rescheduleVisionReaction() {
    if (!visionReactionTimer) return;
    clearTimeout(visionReactionTimer);
    const delay = 75000 + Math.random() * 45000; // 75-120 seconds after Kira speaks
    console.log(`[Vision] Kira spoke — rescheduling next reaction in ${Math.round(delay / 1000)}s`);
    visionReactionTimer = setTimeout(async () => {
      if (!visionActive || clientDisconnected) return;
      await triggerVisionReaction();
      if (visionActive && !clientDisconnected) {
        scheduleNextReaction();
      }
    }, delay);
  }

  const tools: OpenAI.Chat.ChatCompletionTool[] = [
    {
      type: "function",
      function: {
        name: "update_viewing_context",
        description: "Updates the current media or activity context that the user is watching or doing. Call this when the user mentions watching a specific movie, show, or playing a game.",
        parameters: {
          type: "object",
          properties: {
            context: {
              type: "string",
              description: "The name of the media or activity (e.g., 'Berserk 1997', 'The Office', 'Coding').",
            },
          },
          required: ["context"],
        },
      },
    },
  ];

  const chatHistory: OpenAI.Chat.ChatCompletionMessageParam[] = [
    { role: "system", content: KIRA_SYSTEM_PROMPT },
  ];

  // --- Expression tag cooldowns (per-connection) ---
  // LLM decides actions/accessories, but we filter through cooldowns to prevent spam.
  let lastActionTime = 0;
  let lastAccessoryTime = 0;
  const ACTION_COOLDOWN = 30_000;      // 30s between actions
  const ACCESSORY_COOLDOWN = 90_000;   // 90s between accessory changes

  // Tag success tracking
  let tagSuccessCount = 0;
  let tagFallbackCount = 0;

  // --- Emotion-based sentence pacing ---
  // Delay in milliseconds BETWEEN sentences (not before the first one)
  const EMOTION_SENTENCE_DELAY: Record<string, number> = {
    neutral:     0,
    happy:       0,
    excited:     0,     // rapid-fire, no pauses
    love:        200,   // gentle pacing
    blush:       150,
    sad:         300,   // deliberate, heavy pauses
    angry:       50,    // quick but with slight beats
    playful:     0,
    thinking:    400,   // long pauses, pondering
    speechless:  500,   // dramatic pauses
    eyeroll:     100,
    sleepy:      350,   // sleepy pauses
    frustrated:  100,
    confused:    250,   // uncertain pauses
    surprised:   0,     // blurts out fast
  };

  /**
   * Send expression data to client from a parsed tag, applying cooldowns.
   * Used by both streaming (tag parsed from stream) and non-streaming (tag parsed from complete text) paths.
   */
  function sendExpressionFromTag(parsed: ParsedExpression, label: string) {
    const msg: any = { type: "expression", expression: parsed.emotion };
    const now = Date.now();

    if (parsed.action) {
      if (now - lastActionTime >= ACTION_COOLDOWN) {
        msg.action = parsed.action;
        lastActionTime = now;
        lastExpressionActionTime = now;
        console.log(`[Context] Action: ${parsed.action}`);
      } else {
        console.log(`[Context] Action ${parsed.action} suppressed (cooldown: ${((ACTION_COOLDOWN - (now - lastActionTime)) / 1000).toFixed(0)}s remaining)`);
      }
    }

    if (parsed.accessory) {
      if (now - lastAccessoryTime >= ACCESSORY_COOLDOWN) {
        msg.accessory = parsed.accessory;
        lastAccessoryTime = now;
        lastExpressionActionTime = now;
        console.log(`[Context] Accessory: ${parsed.accessory}`);
      } else {
        console.log(`[Context] Accessory ${parsed.accessory} suppressed (cooldown)`);
      }
    }

    ws.send(JSON.stringify(msg));
    const extras = [
      msg.action && `action: ${msg.action}`,
      msg.accessory && `accessory: ${msg.accessory}`,
    ].filter(Boolean).join(", ");
    console.log(`[Expression] ${parsed.emotion}${extras ? ` (${extras})` : ""} (${label})`);
  }

  /**
   * Parse expression tag from a complete (non-streaming) LLM response.
   * Sends expression to client, returns clean text with tag stripped AND the parsed emotion.
   */
  function handleNonStreamingTag(text: string, label: string): { text: string; emotion: string } {
    const tagMatch = text.match(/^\[EMO:(\w+)(?:\|\w+:\w+)*\]/);
    if (tagMatch) {
      const parsed = parseExpressionTag(tagMatch[0]);
      if (parsed) {
        tagSuccessCount++;
        sendExpressionFromTag(parsed, label);
        return { text: stripExpressionTag(text), emotion: parsed.emotion };
      } else {
        tagFallbackCount++;
        console.warn(`[Expression] Malformed tag: "${tagMatch[0]}" — defaulting to neutral (${label})`);
        ws.send(JSON.stringify({ type: "expression", expression: "neutral" }));
        return { text: stripExpressionTag(text), emotion: "neutral" };
      }
    } else {
      tagFallbackCount++;
      console.warn(`[Expression] No tag found in response — defaulting to neutral (${label}). Rate: ${tagSuccessCount}/${tagSuccessCount + tagFallbackCount}`);
      ws.send(JSON.stringify({ type: "expression", expression: "neutral" }));
      return { text, emotion: "neutral" };
    }
  }

  // --- L1: In-Conversation Memory ---
  let conversationSummary = "";

  // --- SILENCE-INITIATED TURNS ---
  let silenceTimer: NodeJS.Timeout | null = null;
  const SILENCE_MIN_MS = 18000; // Minimum 18s of quiet before Kira might speak
  const SILENCE_MAX_MS = 25000; // Maximum 25s — randomized to avoid feeling mechanical
  const SILENCE_POST_KIRA_GAP = 5000; // Minimum 5s after Kira stops speaking before timer starts
  let turnCount = 0; // Track conversation depth for silence behavior
  let silenceInitiatedLast = false; // Prevents monologue loops — Kira gets ONE unprompted turn

  function resetSilenceTimer() {
    if (silenceTimer) clearTimeout(silenceTimer);

    // Don't initiate during first 2 turns (let the user settle in)
    if (turnCount < 2) return;

    // Randomize between 18-25s so it doesn't feel mechanical
    const baseDelay = SILENCE_MIN_MS + Math.random() * (SILENCE_MAX_MS - SILENCE_MIN_MS);

    // Ensure at least 5s gap after Kira stops speaking
    const timeSinceKiraSpoke = Date.now() - lastKiraSpokeTimestamp;
    const delay = Math.max(baseDelay, baseDelay + (SILENCE_POST_KIRA_GAP - timeSinceKiraSpoke));

    silenceTimer = setTimeout(async () => {
      if (state !== "listening" || clientDisconnected) return;
      if (silenceInitiatedLast) return; // Already spoke unprompted, wait for user

      // --- Vision-aware silence behavior ---
      if (visionActive) {
        console.log("[Silence] Vision active — using dedicated reaction timer instead.");
        return;
      }

      silenceInitiatedLast = true;
      setState("thinking"); // Lock state IMMEDIATELY to prevent race condition
      if (silenceTimer) clearTimeout(silenceTimer); // Clear self
      currentResponseId++;
      const thisResponseId = currentResponseId;

      console.log(`[Silence] User has been quiet. Checking if Kira has something to say.${visionActive ? ' (vision mode)' : ''}`);

      // Inject a one-time nudge (removed after the turn)
      const nudge: OpenAI.Chat.ChatCompletionMessageParam = {
        role: "system",
        content: visionActive
          ? `[You've been watching together quietly. If something interesting is happening on screen right now, give a very brief reaction (1-5 words). If the scene is calm or nothing stands out, respond with exactly "[SILENCE]" and nothing else.]`
          : `[The user has been quiet for a moment. This is a natural pause in conversation. If you have something on your mind — a thought, a follow-up question about something they said earlier, something you've been curious about, a reaction to something from the memory block — now is a natural time to share it. Speak as if you just thought of something. Be genuine. If you truly have nothing to say, respond with exactly "[SILENCE]" and nothing else. Do NOT say "are you still there" or "what are you thinking about" or "is everything okay" — those feel robotic. Only speak if you have something real to say.]`
      };

      const tagReminder: OpenAI.Chat.ChatCompletionMessageParam = {
        role: "system",
        content: EXPRESSION_TAG_REMINDER,
      };
      chatHistory.push(tagReminder);
      chatHistory.push(nudge);

      try {
        // Quick check: does the model have something to say?
        const checkResponse = await openai.chat.completions.create({
          model: OPENAI_MODEL,
          messages: chatHistory,
          temperature: 0.9, // Slightly higher for more creative initiation
          max_tokens: 300,
          frequency_penalty: 0.3,
          presence_penalty: 0.3, // Higher to encourage novel topics
        });

        let responseText = checkResponse.choices[0]?.message?.content?.trim() || "";

        // Remove the nudge + tag reminder from history regardless of outcome
        const nudgeIdx = chatHistory.indexOf(nudge);
        if (nudgeIdx >= 0) chatHistory.splice(nudgeIdx, 1);
        const reminderIdx = chatHistory.indexOf(tagReminder);
        if (reminderIdx >= 0) chatHistory.splice(reminderIdx, 1);

        // If model returned silence marker or empty, don't speak
        const cleanedSilenceCheck = stripExpressionTag(responseText || "");
        if (!responseText || 
            responseText.toLowerCase().includes("silence") || 
            cleanedSilenceCheck.startsWith("[") ||
            cleanedSilenceCheck.length < 5) {
          console.log("[Silence] Kira has nothing to say. Staying quiet.");
          return;
        }

        // Parse expression tag and strip before TTS
        const silenceTagResult = handleNonStreamingTag(responseText, "silence initiated");
        responseText = stripEmotionTags(silenceTagResult.text);
        const silenceEmotion = silenceTagResult.emotion;

        // She has something to say — run the TTS pipeline
        chatHistory.push({ role: "assistant", content: responseText });
        console.log(`[Silence] Kira initiates: "${responseText}"`);
        lastKiraSpokeTimestamp = Date.now();
        // Don't reschedule vision timer from silence checker — these are separate systems
        ws.send(JSON.stringify({ type: "transcript", role: "ai", text: responseText }));

        setState("speaking");
        ws.send(JSON.stringify({ type: "state_speaking" }));
        ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
        await new Promise(resolve => setImmediate(resolve));

        try {
          const sentences = responseText.split(/(?<=[.!?…])\s+(?=[A-Z"])/);
          let silSentIdx = 0;
          interruptRequested = false; // Safe to reset — old TTS killed by generation ID
          for (const sentence of sentences) {
            const trimmed = sentence.trim();
            if (trimmed.length === 0) continue;
            if (interruptRequested || thisResponseId !== currentResponseId) {
              console.log(`[TTS] Silence sentence loop aborted (interrupt: ${interruptRequested}, stale: ${thisResponseId !== currentResponseId})`);
              break;
            }
            // Emotional pacing between sentences
            if (silSentIdx > 0) {
              const delay = EMOTION_SENTENCE_DELAY[silenceEmotion] || 0;
              if (delay > 0) await new Promise(resolve => setTimeout(resolve, delay));
            }
            silSentIdx++;
            await new Promise<void>((resolve) => {
              console.log(`[TTS] Creating Azure TTS instance (${currentVoiceConfig.voiceName}, emotion: ${silenceEmotion})`);
              const tts = new AzureTTSStreamer({ ...currentVoiceConfig, emotion: silenceEmotion });
              tts.on("audio_chunk", (chunk: Buffer) => {
                if (interruptRequested || thisResponseId !== currentResponseId) return;
                ws.send(chunk);
              });
              tts.on("tts_complete", () => resolve());
              tts.on("error", (err: Error) => {
                console.error(`[TTS] ❌ Silence chunk failed: "${trimmed}"`, err);
                resolve();
              });
              tts.synthesize(trimmed);
            });
          }
        } catch (ttsErr) {
          console.error("[TTS] Silence turn TTS error:", ttsErr);
        } finally {
          ws.send(JSON.stringify({ type: "tts_chunk_ends" }));
          currentTurnTranscript = "";
          currentInterimTranscript = "";
          transcriptClearedAt = Date.now();
          setState("listening");
          ws.send(JSON.stringify({ type: "state_listening" }));
          // Do NOT reset silence timer here — Kira gets ONE unprompted turn.
          // Only the user speaking again (eou/text_message) resets it.
        }

      } catch (err) {
        console.error("[Silence] LLM call failed:", (err as Error).message);
        // Remove nudge on error too
        const nudgeIdx = chatHistory.indexOf(nudge);
        if (nudgeIdx >= 0) chatHistory.splice(nudgeIdx, 1);
      }

    }, delay);
  }

  // --- Reusable LLM → TTS pipeline ---
  async function runKiraTurn() {
    let llmResponse = "";
    if (silenceTimer) clearTimeout(silenceTimer);
    currentResponseId++;
    const thisResponseId = currentResponseId;
    setState("speaking");
    ws.send(JSON.stringify({ type: "state_speaking" }));
    ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
    await new Promise(resolve => setImmediate(resolve));

    try {
      const completion = await openai.chat.completions.create({
        model: OPENAI_MODEL,
        messages: getMessagesWithTimeContext(),
        temperature: 0.85,
        max_tokens: 300,
        frequency_penalty: 0.3,
        presence_penalty: 0.2,
      });

      llmResponse = completion.choices[0]?.message?.content || "";

      if (llmResponse.trim().length === 0) {
        // Model had nothing to say — return silently
        return;
      }

      // Parse expression tag and strip before TTS
      const runKiraTagResult = handleNonStreamingTag(llmResponse, "runKira");
      llmResponse = stripEmotionTags(runKiraTagResult.text);
      const runKiraEmotion = runKiraTagResult.emotion;

      chatHistory.push({ role: "assistant", content: llmResponse });
      advanceTimePhase(llmResponse);

      console.log(`[AI RESPONSE]: "${llmResponse}"`);
      lastKiraSpokeTimestamp = Date.now();
      if (visionActive) rescheduleVisionReaction();
      ws.send(JSON.stringify({ type: "transcript", role: "ai", text: llmResponse }));

      const sentences = llmResponse.split(/(?<=[.!?…])\s+(?=[A-Z"])/);
      let runKiraSentIdx = 0;
      interruptRequested = false; // Safe to reset — old TTS killed by generation ID
      for (const sentence of sentences) {
        const trimmed = sentence.trim();
        if (trimmed.length === 0) continue;
        if (interruptRequested || thisResponseId !== currentResponseId) {
          console.log(`[TTS] runKiraTurn sentence loop aborted (interrupt: ${interruptRequested}, stale: ${thisResponseId !== currentResponseId})`);
          break;
        }
        // Emotional pacing between sentences
        if (runKiraSentIdx > 0) {
          const delay = EMOTION_SENTENCE_DELAY[runKiraEmotion] || 0;
          if (delay > 0) await new Promise(resolve => setTimeout(resolve, delay));
        }
        runKiraSentIdx++;
        await new Promise<void>((resolve) => {
          console.log(`[TTS] Creating Azure TTS instance (${currentVoiceConfig.voiceName}, emotion: ${runKiraEmotion})`);
          const tts = new AzureTTSStreamer({ ...currentVoiceConfig, emotion: runKiraEmotion });
          tts.on("audio_chunk", (chunk: Buffer) => {
            if (interruptRequested || thisResponseId !== currentResponseId) return;
            ws.send(chunk);
          });
          tts.on("tts_complete", () => resolve());
          tts.on("error", (err: Error) => {
            console.error(`[TTS] ❌ Chunk failed: "${trimmed}"`, err);
            resolve();
          });
          tts.synthesize(trimmed);
        });
      }
    } catch (err) {
      console.error("[Pipeline] Error in runKiraTurn:", (err as Error).message);
    } finally {
      ws.send(JSON.stringify({ type: "tts_chunk_ends" }));
      currentTurnTranscript = "";
      currentInterimTranscript = "";
      transcriptClearedAt = Date.now();
      setState("listening");
      ws.send(JSON.stringify({ type: "state_listening" }));
      resetSilenceTimer();
    }
  }

  // --- Time-context injection for graceful paywall ---
  function getTimeContext(): string {
    if (timeWarningPhase === 'final_goodbye') {
      return `\n\n[CRITICAL INSTRUCTION - MUST FOLLOW: This is your LAST response. Our time together is ending. Keep your ENTIRE response to 1-2 short sentences. Make it feel like you genuinely don't want to stop talking — like this conversation actually meant something to you. Reference something specific you talked about or something you noticed about them. Sound a little wistful, not chipper. Don't say "time's up" or mention limits. Just let the goodbye land softly, like the end of a late-night conversation neither person wanted to end. Example: "I really loved talking about that with you… come find me tomorrow, okay? I'll be here." Do NOT continue the previous topic in depth.]`;
    }
    return '';
  }

  /** Build messages array with time + vision context injected into system prompt (without mutating chatHistory). */
  function getMessagesWithTimeContext(): OpenAI.Chat.ChatCompletionMessageParam[] {
    const timeCtx = getTimeContext();
    const visionCtx = visionActive ? VISION_CONTEXT_PROMPT : '';
    // Clone and inject time + vision context into the system prompt
    const messages = chatHistory.map((msg, i) => {
      if (i === 0 && msg.role === 'system' && typeof msg.content === 'string') {
        return { ...msg, content: msg.content + visionCtx + timeCtx };
      }
      return msg;
    });
    // Inject expression tag reminder as the last system message (right before user's message)
    // This keeps it at the edge of the model's attention window for maximum compliance.
    messages.push({ role: "system", content: EXPRESSION_TAG_REMINDER });
    return messages;
  }

  /** Advance timeWarningPhase after a response is sent during a warning phase. */
  function advanceTimePhase(responseText: string) {
    if (timeWarningPhase === 'final_goodbye') {
      timeWarningPhase = 'done';
      isAcceptingAudio = false;
      console.log('[TIME] final_goodbye → done (goodbye delivered)');

      // Wait for TTS to finish playing on client, then disconnect
      const estimatedPlayTime = Math.max(2000, responseText.length * 80);
      setTimeout(() => {
        if (ws.readyState === ws.OPEN) {
          ws.send(JSON.stringify({ type: "error", code: "limit_reached", ...(isProUser ? { tier: "pro" } : {}) }));
          ws.close(1008, "Usage limit reached");
        }
      }, estimatedPlayTime);
    }
  }

  // Proactive goodbye when user doesn't speak during final phase
  async function sendProactiveGoodbye() {
    if (timeWarningPhase !== 'final_goodbye' || state !== 'listening' || clientDisconnected) return;
    if (ws.readyState !== ws.OPEN) return;

    timeWarningPhase = 'done';
    isAcceptingAudio = false;
    if (silenceTimer) clearTimeout(silenceTimer);

    try {
      const goodbyeMessages: OpenAI.Chat.ChatCompletionMessageParam[] = [
        { role: "system", content: KIRA_SYSTEM_PROMPT + `\n\n[CRITICAL INSTRUCTION - MUST FOLLOW: You need to say goodbye now. But make it feel real — like you don't want to go. Keep it to 1-2 short sentences. Reference something specific from the conversation, something you genuinely enjoyed or that made you feel connected to them. Sound a little sad, a little soft. Don't mention time limits or rules. Let it feel like the natural, reluctant end of a conversation you wish could keep going. Example: "Hey… I was really enjoying this. Promise you'll come back and tell me how it goes?" or "I don't wanna stop talking about this… but I'll be right here whenever you're ready to come back."]` },
        ...chatHistory.filter(m => m.role !== "system").slice(-4),
        { role: "system", content: EXPRESSION_TAG_REMINDER },
        { role: "user", content: "[Say a heartfelt goodbye — this conversation meant something to you]" },
      ];

      const response = await openai.chat.completions.create({
        model: OPENAI_MODEL,
        messages: goodbyeMessages,
        max_tokens: 60,
        temperature: 0.9,
      });

      const goodbyeText = response.choices[0]?.message?.content?.trim() || "";
      if (goodbyeText && goodbyeText.length > 2 && ws.readyState === ws.OPEN && !clientDisconnected) {
        // Parse expression tag and strip before TTS
        const goodbyeTagResult = handleNonStreamingTag(goodbyeText, "goodbye");
        const finalGoodbye = stripEmotionTags(goodbyeTagResult.text);
        const goodbyeEmotion = goodbyeTagResult.emotion;

        console.log(`[Goodbye] Kira says: "${finalGoodbye}"`);
        chatHistory.push({ role: "assistant", content: finalGoodbye });
        ws.send(JSON.stringify({ type: "transcript", role: "ai", text: finalGoodbye }));

        setState("speaking");
        ws.send(JSON.stringify({ type: "state_speaking" }));
        ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
        await new Promise(resolve => setImmediate(resolve));

        const sentences = finalGoodbye.split(/(?<=[.!?\u2026])\s+(?=[A-Z"])/);
        let goodbyeSentIdx = 0;
        for (const sentence of sentences) {
          const trimmed = sentence.trim();
          if (trimmed.length === 0) continue;
          if (goodbyeSentIdx > 0) {
            const delay = EMOTION_SENTENCE_DELAY[goodbyeEmotion] || 0;
            if (delay > 0) await new Promise(resolve => setTimeout(resolve, delay));
          }
          goodbyeSentIdx++;
          await new Promise<void>((resolve) => {
            const tts = new AzureTTSStreamer({ ...currentVoiceConfig, emotion: goodbyeEmotion });
            tts.on("audio_chunk", (chunk: Buffer) => {
              if (!clientDisconnected && ws.readyState === ws.OPEN) ws.send(chunk);
            });
            tts.on("tts_complete", () => resolve());
            tts.on("error", (err: Error) => {
              console.error(`[Goodbye TTS] ❌ Chunk failed: "${trimmed}"`, err);
              resolve();
            });
            tts.synthesize(trimmed);
          });
        }

        ws.send(JSON.stringify({ type: "tts_chunk_ends" }));

        // Wait for TTS to finish playing on client, then disconnect
        const estimatedPlayTime = Math.max(2000, finalGoodbye.length * 80);
        setTimeout(() => {
          if (ws.readyState === ws.OPEN) {
            ws.send(JSON.stringify({ type: "error", code: "limit_reached", ...(isProUser ? { tier: "pro" } : {}) }));
            ws.close(1008, "Usage limit reached");
          }
        }, estimatedPlayTime);
      } else {
        // No goodbye text — close immediately
        if (ws.readyState === ws.OPEN) {
          ws.send(JSON.stringify({ type: "error", code: "limit_reached", ...(isProUser ? { tier: "pro" } : {}) }));
          ws.close(1008, "Usage limit reached");
        }
      }
    } catch (err) {
      console.error("[Goodbye] Error:", (err as Error).message);
      if (ws.readyState === ws.OPEN) {
        ws.send(JSON.stringify({ type: "error", code: "limit_reached", ...(isProUser ? { tier: "pro" } : {}) }));
        ws.close(1008, "Usage limit reached");
      }
    }
  }

  // --- CONTEXT MANAGEMENT CONSTANTS ---
  const MAX_RECENT_MESSAGES = 10;
  const SUMMARIZE_THRESHOLD = 20;
  const MESSAGES_TO_SUMMARIZE = 6;

  // --- USAGE TRACKING ---
  const FREE_LIMIT_SECONDS = parseInt(process.env.FREE_TRIAL_SECONDS || "900"); // 15 min/day
  const PRO_MONTHLY_SECONDS = parseInt(process.env.PRO_MONTHLY_SECONDS || "144000"); // 40 hrs/month
  let sessionStartTime: number | null = null;
  let usageCheckInterval: NodeJS.Timeout | null = null;
  let timeCheckInterval: NodeJS.Timeout | null = null;
  let isProUser = false;
  let guestUsageSeconds = 0;
  let guestUsageBase = 0; // Accumulated seconds from previous sessions today
  let proUsageSeconds = 0;
  let proUsageBase = 0; // Accumulated seconds from previous sessions this month
  let wasBlockedImmediately = false; // True if connection was blocked on connect (limit already hit)

  // --- Reusable Deepgram initialization ---
  async function initDeepgram() {
    const streamer = new DeepgramSTTStreamer();
    await streamer.start();

    streamer.on(
      "transcript",
      (transcript: string, isFinal: boolean) => {
        // Reset health tracking — Deepgram is alive
        consecutiveEmptyEOUs = 0;
        lastTranscriptReceivedAt = Date.now();

        // Ignore stale transcripts that arrive within 500ms of clearing
        // These are from Deepgram's pipeline processing old audio from the previous turn
        if (Date.now() - transcriptClearedAt < 1500) {
          console.log(`[STT] Ignoring stale transcript (${Date.now() - transcriptClearedAt}ms after clear): "${transcript}"`);
          return;
        }

        // --- Barge-in detection: user speaks 3+ words while Kira is speaking ---
        if (state === "speaking" && isFinal && transcript.trim().length > 0) {
          const wordCount = transcript.trim().split(/\s+/).length;
          if (wordCount >= 3) {
            console.log(`[Interrupt] User spoke ${wordCount} words while Kira speaking: "${transcript.trim()}"`);
            interruptRequested = true;
            currentResponseId++; // Invalidate any in-flight TTS callbacks

            // Tell client to stop audio playback immediately
            ws.send(JSON.stringify({ type: "interrupt" }));

            // Transition to listening — pendingEOU will trigger response after current turn cleans up
            currentTurnTranscript = transcript.trim();
            currentInterimTranscript = "";
            setState("listening");
            ws.send(JSON.stringify({ type: "state_listening" }));

            // Queue as pending EOU — it will be picked up when the current pipeline finishes
            pendingEOU = transcript.trim();
            console.log(`[Interrupt] Queued barge-in transcript as pending EOU: "${transcript.trim()}"`);
            return;
          }
        }

        // During speaking state (non-interrupt), ignore transcripts entirely
        if (state !== "listening") return;

        if (isFinal) {
          currentTurnTranscript += transcript + " ";
          // Safety cap: prevent unbounded transcript growth
          if (currentTurnTranscript.length > 5000) {
            currentTurnTranscript = currentTurnTranscript.slice(-4000);
          }
          currentInterimTranscript = ""; // Clear interim since we got a final
        } else {
          currentInterimTranscript = transcript; // Always track latest interim
        }
        // Send transcript to client for real-time display
        ws.send(JSON.stringify({ 
          type: "transcript", 
          role: "user", 
          text: currentTurnTranscript.trim() || transcript 
        }));
      }
    );

    streamer.on("error", (err: Error) => {
      console.error("[Pipeline] ❌ STT Error:", err.message);
      reconnectDeepgram();
    });

    streamer.on("close", () => {
      console.log("[Deepgram] Connection closed unexpectedly. Triggering reconnect.");
      reconnectDeepgram();
    });

    return streamer;
  }

  // --- Self-healing Deepgram reconnection ---
  async function reconnectDeepgram() {
    if (isReconnectingDeepgram || clientDisconnected) return;
    isReconnectingDeepgram = true;
    console.log("[Deepgram] ⚠️ Connection appears dead. Reconnecting...");

    try {
      // Close old connection if still open
      if (sttStreamer) {
        try { sttStreamer.destroy(); } catch (e) { /* ignore */ }
      }

      // Re-create with same config and listeners
      sttStreamer = await initDeepgram();

      // Reset tracking
      consecutiveEmptyEOUs = 0;
      lastTranscriptReceivedAt = Date.now();
      console.log("[Deepgram] ✅ Reconnected successfully.");
    } catch (err) {
      console.error("[Deepgram] ❌ Reconnection failed:", (err as Error).message);
    } finally {
      isReconnectingDeepgram = false;
    }
  }

  ws.on("message", async (message: Buffer, isBinary: boolean) => {
    // Wait for auth to complete before processing ANY message
    const isAuthenticated = await authPromise;
    if (!isAuthenticated) return;

    try {
      // --- 3. MESSAGE HANDLING ---
      // In ws v8+, message is a Buffer. We need to check if it's a JSON control message.
      let controlMessage: any = null;
      
      // Try to parse as JSON if it looks like text
      try {
        const str = message.toString();
        if (str.trim().startsWith("{")) {
          controlMessage = JSON.parse(str);
        }
      } catch (e) {
        // Not JSON, treat as binary audio
      }

      if (controlMessage) {
        // Rate limiting: only count control (JSON) messages, never binary audio
        messageCount++;
        if (messageCount > MAX_CONTROL_MESSAGES_PER_SECOND) {
          console.warn("[WS] Rate limit exceeded, dropping control message");
          return;
        }

        console.log(`[WS] Control message: ${controlMessage.type}`);
        if (controlMessage.type === "start_stream") {
          console.log("[WS] Received start_stream. Initializing pipeline...");

          // --- L2: Load persistent memories for ALL users (signed-in AND guests) ---
          if (userId) {
            try {
              const memLoadStart = Date.now();
              const memoryBlock = await loadUserMemories(prisma, userId);
              if (memoryBlock) {
                chatHistory.push({ role: "system", content: memoryBlock });
                console.log(
                  `[Memory] Loaded ${memoryBlock.length} chars of persistent memory for ${isGuest ? 'guest' : 'user'} ${userId}`
                );
                console.log(`[Latency] Memory load: ${Date.now() - memLoadStart}ms (${memoryBlock.length} chars)`);
              }
            } catch (err) {
              console.error(
                "[Memory] Failed to load memories:",
                (err as Error).message
              );
            }
          }

          // --- USAGE: Check limits on connect ---
          if (!isGuest && userId) {
            try {
              const dbUser = await prisma.user.findUnique({
                where: { clerkId: userId },
                select: {
                  dailyUsageSeconds: true,
                  lastUsageDate: true,
                  stripeSubscriptionId: true,
                  stripeCurrentPeriodEnd: true,
                },
              });

              if (dbUser) {
                isProUser = !!(
                  dbUser.stripeSubscriptionId &&
                  dbUser.stripeCurrentPeriodEnd &&
                  dbUser.stripeCurrentPeriodEnd.getTime() > Date.now()
                );

                if (isProUser) {
                  // Pro users: monthly usage tracked in Prisma MonthlyUsage (resets per calendar month)
                  const storedSeconds = await getProUsage(userId);
                  if (storedSeconds >= PRO_MONTHLY_SECONDS) {
                    console.log(`[USAGE] Pro user ${userId} blocked — ${storedSeconds}s >= ${PRO_MONTHLY_SECONDS}s`);
                    wasBlockedImmediately = true;
                    ws.send(JSON.stringify({ type: "error", code: "limit_reached", tier: "pro" }));
                    ws.close(1008, "Pro usage limit reached");
                    return;
                  }
                  proUsageSeconds = storedSeconds;
                  proUsageBase = storedSeconds;
                  console.log(`[USAGE] Pro user ${userId} allowed — resuming at ${storedSeconds}s / ${PRO_MONTHLY_SECONDS}s`);

                  ws.send(JSON.stringify({
                    type: "session_config",
                    isPro: true,
                    remainingSeconds: PRO_MONTHLY_SECONDS - storedSeconds,
                  }));
                } else {
                  // Free signed-in users: daily usage tracked in Prisma
                  let currentUsage = dbUser.dailyUsageSeconds;
                  const today = new Date().toDateString();
                  const lastUsage = dbUser.lastUsageDate?.toDateString();
                  if (today !== lastUsage) {
                    currentUsage = 0;
                    await prisma.user.update({
                      where: { clerkId: userId },
                      data: { dailyUsageSeconds: 0, lastUsageDate: new Date() },
                    });
                  }

                  if (currentUsage >= FREE_LIMIT_SECONDS) {
                    ws.send(JSON.stringify({ type: "error", code: "limit_reached" }));
                    ws.close(1008, "Usage limit reached");
                    return;
                  }

                  ws.send(JSON.stringify({
                    type: "session_config",
                    isPro: false,
                    remainingSeconds: FREE_LIMIT_SECONDS - currentUsage,
                  }));
                }
              }
            } catch (err) {
              console.error(
                "[Usage] Failed to check limits:",
                (err as Error).message
              );
            }
          }

          // --- USAGE: Start session timer ---
          sessionStartTime = Date.now();

          // Send session_config for guests (signed-in users already get it above)
          let isReturningGuest = false;
          if (isGuest && userId) {
            const usageInfo = await getGuestUsageInfo(userId);

            if (usageInfo.seconds >= FREE_LIMIT_SECONDS) {
              console.log(`[USAGE] Guest ${userId} blocked — ${usageInfo.seconds}s >= ${FREE_LIMIT_SECONDS}s`);
              wasBlockedImmediately = true;
              ws.send(JSON.stringify({ type: "error", code: "limit_reached" }));
              ws.close(1008, "Guest usage limit reached");
              return;
            }

            // Resume tracking from where they left off
            isReturningGuest = usageInfo.isReturning;
            guestUsageSeconds = usageInfo.seconds;
            guestUsageBase = usageInfo.seconds;
            console.log(`[USAGE] Guest ${userId} allowed — resuming at ${usageInfo.seconds}s (returning: ${isReturningGuest})`);

            ws.send(
              JSON.stringify({
                type: "session_config",
                isPro: false,
                remainingSeconds: FREE_LIMIT_SECONDS - guestUsageSeconds,
              })
            );
          }

          // --- 30-SECOND INTERVAL: Usage tracking + DB writes ONLY ---
          // Phase transitions are handled by the faster 5-second interval below.
          usageCheckInterval = setInterval(async () => {
            try {
              if (!sessionStartTime) return;

              const elapsed = Math.floor(
                (Date.now() - sessionStartTime) / 1000
              );

              if (isGuest) {
                guestUsageSeconds = guestUsageBase + elapsed;

                // Persist to database so usage survives restarts/deploys
                await saveGuestUsage(userId!, guestUsageSeconds);
                console.log(`[USAGE] Guest ${userId}: ${guestUsageSeconds}s / ${FREE_LIMIT_SECONDS}s`);

              const remainingSec = FREE_LIMIT_SECONDS - guestUsageSeconds;

              // Hard limit: only force-close if goodbye system isn't handling it
              if (remainingSec <= 0) {
                if (timeWarningPhase === 'done' || timeWarningPhase === 'final_goodbye') {
                  console.log(`[USAGE] Over limit but in ${timeWarningPhase} phase — letting goodbye system handle disconnect`);
                  return;
                }
                // Fallback: if somehow we got here without entering final_goodbye
                console.log(`[USAGE] Over limit, no goodbye phase active — forcing final_goodbye`);
                timeWarningPhase = 'final_goodbye';
                // The 5-second interval will pick this up and handle the goodbye
              }
            } else if (userId) {
              if (isProUser) {
                // Pro users: monthly usage tracked in Prisma MonthlyUsage
                proUsageSeconds = proUsageBase + elapsed;
                await saveProUsage(userId, proUsageSeconds);
                console.log(`[USAGE] Pro ${userId}: ${proUsageSeconds}s / ${PRO_MONTHLY_SECONDS}s`);

                const proRemaining = PRO_MONTHLY_SECONDS - proUsageSeconds;
                if (proRemaining <= 0) {
                  if (timeWarningPhase === 'done' || timeWarningPhase === 'final_goodbye') {
                    console.log(`[USAGE] Pro over limit but in ${timeWarningPhase} phase — letting goodbye system handle disconnect`);
                    return;
                  }
                  console.log(`[USAGE] Pro over limit, no goodbye phase active — forcing final_goodbye`);
                  timeWarningPhase = 'final_goodbye';
                }
              } else {
                // Free signed-in users: daily usage tracked in Prisma
                try {
                  await prisma.user.update({
                    where: { clerkId: userId },
                    data: {
                      dailyUsageSeconds: { increment: 30 },
                      lastUsageDate: new Date(),
                    },
                  });

                  const dbUser = await prisma.user.findUnique({
                    where: { clerkId: userId },
                    select: { dailyUsageSeconds: true },
                  });

                  if (dbUser && dbUser.dailyUsageSeconds >= FREE_LIMIT_SECONDS) {
                    if (timeWarningPhase === 'done' || timeWarningPhase === 'final_goodbye') {
                      console.log(`[USAGE] Free user over limit but in ${timeWarningPhase} phase — letting goodbye system handle disconnect`);
                      return;
                    }
                    console.log(`[USAGE] Free user over limit — forcing final_goodbye`);
                    timeWarningPhase = 'final_goodbye';
                  }
                } catch (err) {
                  console.error("[Usage] DB update failed:", (err as Error).message);
                }
              }
            }
            } catch (err) {
              // Don't crash the server if usage persistence fails
              console.error("[Usage] Interval error:", (err as Error).message);
            }
          }, 30000);

          // --- 5-SECOND INTERVAL: Time warning phase transitions ---
          // This runs frequently so we never skip the final_goodbye window.
          // It computes remaining time from the live elapsed counter, not from DB.
          timeCheckInterval = setInterval(() => {
            if (!sessionStartTime) return;
            if (timeWarningPhase === 'done') return;

            const elapsed = Math.floor((Date.now() - sessionStartTime) / 1000);

            // Compute remaining seconds based on user type
            let remainingSec: number | null = null;
            if (isGuest) {
              guestUsageSeconds = guestUsageBase + elapsed;
              remainingSec = FREE_LIMIT_SECONDS - guestUsageSeconds;
            } else if (userId && isProUser) {
              proUsageSeconds = proUsageBase + elapsed;
              remainingSec = PRO_MONTHLY_SECONDS - proUsageSeconds;
            }
            // Free signed-in users use DB-based tracking, not real-time
            // Their phase transitions happen in the 30s interval

            if (remainingSec === null) return;

            if (remainingSec <= 15 && timeWarningPhase === 'normal') {
              console.log(`[TIME] ${remainingSec}s left — entering final_goodbye phase`);
              timeWarningPhase = 'final_goodbye';
              // If user doesn't speak within 3s, Kira says goodbye herself
              if (goodbyeTimeout) clearTimeout(goodbyeTimeout);
              goodbyeTimeout = setTimeout(() => sendProactiveGoodbye(), 3000);
            }
          }, 5000);

          sttStreamer = await initDeepgram();
          isAcceptingAudio = true;

          // --- GUEST CONVERSATION CONTINUITY: Load previous session ---
          if (isGuest && userId) {
            const previousBuffer = getGuestBuffer(userId);
            if (previousBuffer && previousBuffer.messages.length > 0) {
              // Load the last 10 messages for context (don't overwhelm the context window)
              const recentHistory = previousBuffer.messages.slice(-10);
              // Add a summary marker so Kira knows this is prior context
              chatHistory.push({
                role: "system",
                content: `[PREVIOUS SESSION CONTEXT] This guest has talked to you before. Here is a summary of your last conversation:\n${previousBuffer.summary || "(No summary available)"}`,
              });
              for (const msg of recentHistory) {
                chatHistory.push({
                  role: msg.role as "user" | "assistant",
                  content: msg.content,
                });
              }
              console.log(
                `[Memory] Loaded ${recentHistory.length} messages from previous guest session for ${userId}`
              );
            }
          }

          ws.send(JSON.stringify({ type: "stream_ready" }));

          // --- KIRA OPENER: She speaks first ---
          setTimeout(async () => {
            if (clientDisconnected || state !== "listening") return;

            // Determine user type for contextual greeting
            let userType: "new_guest" | "returning_guest" | "pro_user" | "free_user";
            if (isGuest) {
              userType = isReturningGuest ? "returning_guest" : "new_guest";
            } else if (isProUser) {
              userType = "pro_user";
            } else {
              userType = "free_user";
            }

            // Check if memories were loaded (indicates an established relationship)
            const hasMemories = chatHistory.some(
              (msg) => msg.role === "system" && typeof msg.content === "string" && msg.content.includes("[WHAT YOU KNOW ABOUT THIS USER]")
            );

            let openerInstruction: string;
            switch (userType) {
              case "new_guest":
                openerInstruction = `[This user just connected for the very first time. They have never talked to you before. Say something warm and casual to kick off the conversation — like you're meeting someone cool for the first time. Be brief (1-2 sentences). Introduce yourself naturally. Don't be formal or robotic. Examples of the vibe: "Hey! I'm Kira. So... what's your deal?" or "Hi! I'm Kira — I've been waiting for someone interesting to talk to." Make it YOUR version — don't copy these examples word for word. Be spontaneous.]`;
                break;
              case "returning_guest":
                openerInstruction = `[This user has talked to you before, but they're still a guest (not signed in). You don't have specific memories of them, but you know this isn't their first time. Greet them like you vaguely recognize them — casual and warm. Be brief (1-2 sentences). Something like the vibe of "Hey, you're back!" without being over-the-top. Don't ask them to sign up or mention accounts. Just be happy to see them.]`;
                break;
              case "pro_user":
                if (hasMemories) {
                  openerInstruction = `[This is a Pro subscriber you know well. Your memories about them are loaded in the conversation. Greet them like a close friend.

IMPORTANT — VARIETY RULES:
- Do NOT always reference the same memory. Pick a DIFFERENT topic each time.
- If you've mentioned a movie/anime recently, try asking about their day, work, music, gaming, or something new.
- It's perfectly fine to sometimes NOT reference a memory at all — just say hi naturally and ask what's up.
- NEVER sound like you're reading from a fact sheet.
- Be brief (1-2 sentences). Skip introductions. You know each other.

Good variety: "Hey! How's your day going?", "What's up? Been working on anything cool?", "Yo, what are you up to tonight?"
Bad: Mentioning the same movie/anime/fact every single time.]`;
                } else {
                  openerInstruction = `[This is a Pro subscriber but you don't have specific memories loaded yet. Greet them warmly like a friend you're excited to talk to again. Be brief (1-2 sentences). Don't mention subscriptions or Pro status.]`;
                }
                break;
              case "free_user":
                if (hasMemories) {
                  openerInstruction = `[This is a signed-in user you know. Your memories about them are loaded in the conversation. Greet them like a friend.

IMPORTANT — VARIETY RULES:
- Do NOT always reference the same memory. Pick a DIFFERENT topic each time.
- If you've mentioned a movie/anime recently, try asking about their day, work, music, gaming, or something new.
- It's perfectly fine to sometimes NOT reference a memory at all — just say hi naturally like you're picking up where you left off.
- NEVER sound like you're reading from a fact sheet.
- Be brief (1-2 sentences).

Good variety: "Hey! How's your day going?", "What's up? Been into anything new lately?", "Yo! What are you up to?"
Bad: Mentioning the same movie/anime/fact every single time.]`;
                } else {
                  openerInstruction = `[This is a signed-in user, but you don't have specific memories of them. They might be relatively new. Greet them casually and warmly. Be brief (1-2 sentences). Be yourself — curious and open.]`;
                }
                break;
            }

            console.log(`[Opener] User type: ${userType}, hasMemories: ${hasMemories}`);

            try {
              const openerStart = Date.now();
              currentResponseId++;
              const thisResponseId = currentResponseId;
              setState("thinking");
              ws.send(JSON.stringify({ type: "state_thinking" }));

              const openerMessages: OpenAI.Chat.ChatCompletionMessageParam[] = [
                ...chatHistory,
                { role: "system", content: openerInstruction },
                { role: "system", content: EXPRESSION_TAG_REMINDER },
                { role: "user", content: "[User just connected — say hi]" },
              ];

              const completion = await openai.chat.completions.create({
                model: OPENAI_MODEL,
                messages: openerMessages,
                temperature: 1.0,
                max_tokens: 100,
                frequency_penalty: 0.6,
                presence_penalty: 0.6,
              });

              let openerText = completion.choices[0]?.message?.content?.trim() || "";
              console.log(`[Latency] Opener LLM: ${Date.now() - openerStart}ms`);
              if (!openerText || openerText.length < 3 || clientDisconnected) return;

              // Parse expression tag and strip before TTS
              const openerTagResult = handleNonStreamingTag(openerText, "opener");
              openerText = stripEmotionTags(openerTagResult.text);
              const openerEmotion = openerTagResult.emotion;

              // Add to chat history (NOT the instruction — just the greeting)
              chatHistory.push({ role: "assistant", content: openerText });
              console.log(`[Opener] Kira says: "${openerText}"`);
              ws.send(JSON.stringify({ type: "transcript", role: "ai", text: openerText }));

              // --- TTS pipeline for opener ---
              const openerTtsStart = Date.now();
              setState("speaking");
              ws.send(JSON.stringify({ type: "state_speaking" }));
              ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
              await new Promise(resolve => setImmediate(resolve));

              const sentences = openerText.split(/(?<=[.!?…])\s+(?=[A-Z"])/);
              let openerSentIdx = 0;
              interruptRequested = false; // Safe to reset — old TTS killed by generation ID
              for (const sentence of sentences) {
                const trimmed = sentence.trim();
                if (trimmed.length === 0) continue;
                if (interruptRequested || thisResponseId !== currentResponseId) {
                  console.log(`[TTS] Opener sentence loop aborted (interrupt: ${interruptRequested}, stale: ${thisResponseId !== currentResponseId})`);
                  break;
                }
                if (openerSentIdx > 0) {
                  const delay = EMOTION_SENTENCE_DELAY[openerEmotion] || 0;
                  if (delay > 0) await new Promise(resolve => setTimeout(resolve, delay));
                }
                openerSentIdx++;
                await new Promise<void>((resolve) => {
                  const tts = new AzureTTSStreamer({ ...currentVoiceConfig, emotion: openerEmotion });
                  tts.on("audio_chunk", (chunk: Buffer) => {
                    if (interruptRequested || thisResponseId !== currentResponseId) return;
                    if (!clientDisconnected) ws.send(chunk);
                  });
                  tts.on("tts_complete", () => resolve());
                  tts.on("error", (err: Error) => {
                    console.error(`[Opener TTS] ❌ Chunk failed: "${trimmed}"`, err);
                    resolve();
                  });
                  tts.synthesize(trimmed);
                });
              }

              console.log(`[Latency] Opener TTS: ${Date.now() - openerTtsStart}ms`);
              console.log(`[Latency] Opener total: ${Date.now() - openerStart}ms`);
              ws.send(JSON.stringify({ type: "tts_chunk_ends" }));
              setState("listening");
              ws.send(JSON.stringify({ type: "state_listening" }));
              turnCount++; // Count the opener as a turn
              resetSilenceTimer();

              // Start comfort arc after opener completes
              startComfortProgression(ws);
            } catch (err) {
              console.error("[Opener] Error:", (err as Error).message);
              setState("listening");
              ws.send(JSON.stringify({ type: "state_listening" }));
            }
          }, 500);
        } else if (controlMessage.type === "eou") {
          if (timeWarningPhase === 'done') return; // Don't process new utterances after goodbye

          // User spoke — cancel proactive goodbye timeout (the natural response will handle it)
          if (goodbyeTimeout) { clearTimeout(goodbyeTimeout); goodbyeTimeout = null; }

          // Debounce: ignore EOU if one was just processed
          const now = Date.now();
          if (now - lastEouTime < EOU_DEBOUNCE_MS) {
            console.log(`[EOU] Ignoring spurious EOU (debounced, ${now - lastEouTime}ms since last)`);
            return;
          }

          if (state !== "listening" || !sttStreamer) {
            // Queue the EOU if we have a transcript, so it's not silently dropped
            const queuedTranscript = (currentTurnTranscript.trim() || currentInterimTranscript.trim());
            if (queuedTranscript) {
              console.warn(`[EOU] Received while in "${state}" state. Queuing for when ready.`);
              pendingEOU = queuedTranscript;
              currentTurnTranscript = "";
              currentInterimTranscript = "";
            }
            return; // Already thinking/speaking
          }

          // CRITICAL: Lock state IMMEDIATELY to prevent audio from leaking into next turn
          setState("thinking");
          if (silenceTimer) clearTimeout(silenceTimer);

          // If no final transcript, immediately use interim (no waiting needed)
          if (currentTurnTranscript.trim().length === 0 && currentInterimTranscript.trim().length > 0) {
            console.log(`[EOU] Using interim transcript: "${currentInterimTranscript}"`);
            currentTurnTranscript = currentInterimTranscript;
          }

          // Final check: if still empty, nothing was actually said
          if (currentTurnTranscript.trim().length === 0) {
            // If vision is active, silently ignore empty EOUs (likely screen share noise)
            if (visionActive) {
              console.log("[EOU] Ignoring empty EOU during vision session (likely screen share noise).");
              setState("listening");
              return;
            }

            // Forced max-utterance EOUs with no transcript are background noise
            if (controlMessage.forced) {
              console.log("[EOU] Ignoring forced max-utterance EOU — no speech detected.");
              setState("listening");
              return;
            }

            consecutiveEmptyEOUs++;
            console.log(`[EOU] No transcript available (${consecutiveEmptyEOUs} consecutive empty EOUs), ignoring EOU.`);
            setState("listening"); // Reset state — don't get stuck in "thinking"

            if (consecutiveEmptyEOUs >= 4 &&
                (Date.now() - lastTranscriptReceivedAt > 30000)) {
              // Only reconnect if 4+ empty EOUs AND no real transcript in 30+ seconds.
              // Prevents false positives during intentional user silence.
              console.log("[EOU] Deepgram appears dead (4+ empty EOUs, 30s+ silent). Reconnecting.");
              await reconnectDeepgram();
            }
            return;
          }

          lastEouTime = now; // Record this EOU time for debouncing
          const eouReceivedAt = Date.now();
          currentResponseId++;
          const thisResponseId = currentResponseId;
          // DON'T reset interruptRequested here — wait until TTS begins so old callbacks can't leak

          // LLM rate limit check
          llmCallCount++;
          if (llmCallCount > LLM_MAX_CALLS_PER_MINUTE) {
            console.warn(`[RateLimit] LLM call rate exceeded (${llmCallCount}/${LLM_MAX_CALLS_PER_MINUTE}/min). Dropping EOU.`);
            setState("listening");
            return;
          }

          console.log(`[Latency] EOU received | transcript ready: ${currentTurnTranscript.trim().length} chars (streaming STT)`);
          turnCount++;
          silenceInitiatedLast = false; // User spoke, allow future silence initiation
          lastUserSpokeTimestamp = Date.now();
          resetSilenceTimer();
          const userMessage = currentTurnTranscript.trim();
          currentTurnTranscript = ""; // Reset for next turn
          currentInterimTranscript = ""; // Reset interim too
          transcriptClearedAt = Date.now();

          // Content-based dedup: reject if identical to last processed message
          if (userMessage === lastProcessedTranscript) {
            console.log(`[EOU] Ignoring duplicate transcript: "${userMessage}"`);
            setState("listening");
            return;
          }
          lastProcessedTranscript = userMessage;

          console.log(`[USER TRANSCRIPT]: "${userMessage}"`);
          console.log(`[LLM] Sending to OpenAI: "${userMessage}"`);
          ws.send(JSON.stringify({ type: "state_thinking" }));

          // Check if we have a recent image (within last 10 seconds)
          const imageCheckTime = Date.now();
          if (latestImages && latestImages.length > 0 && (imageCheckTime - lastImageTimestamp < 10000)) {
            // Cap at 2 most recent images to reduce vision LLM latency
            const imagesToSend = latestImages.slice(-2);
            console.log(`[Vision] Attaching ${imagesToSend.length} images to user message (${latestImages.length} in buffer).`);
            
            const content: OpenAI.Chat.ChatCompletionContentPart[] = [
                { type: "text", text: userMessage }
            ];

            imagesToSend.forEach((img) => {
                content.push({
                    type: "image_url",
                    image_url: {
                        url: img.startsWith("data:") ? img : `data:image/jpeg;base64,${img}`,
                        detail: "low"
                    }
                });
            });

            chatHistory.push({
              role: "user",
              content: content,
            });
            
            // Keep latestImages — don't clear. Periodic client captures will refresh them.
          } else {
            chatHistory.push({ role: "user", content: userMessage });
          }

          // --- CONTEXT MANAGEMENT (Sliding Window — non-blocking) ---
          // Immediate truncation: drop oldest non-system messages if over threshold.
          // The LLM summary runs in the background AFTER the response is sent.
          const nonSystemCount = chatHistory.filter(m => m.role !== "system").length;

          if (nonSystemCount > SUMMARIZE_THRESHOLD) {
            let firstMsgIdx = chatHistory.findIndex(m => m.role !== "system");
            if (
              typeof chatHistory[firstMsgIdx]?.content === "string" &&
              (chatHistory[firstMsgIdx].content as string).startsWith("[CONVERSATION SO FAR]")
            ) {
              firstMsgIdx++;
            }
            // Snapshot messages to compress (for deferred summary)
            const toCompress = chatHistory.slice(firstMsgIdx, firstMsgIdx + MESSAGES_TO_SUMMARIZE);
            // Immediately remove old messages so the LLM call below uses a trimmed context
            chatHistory.splice(firstMsgIdx, MESSAGES_TO_SUMMARIZE);
            console.log(`[Context] Truncated ${MESSAGES_TO_SUMMARIZE} oldest messages (${chatHistory.length} remain). Summary deferred.`);

            // Fire-and-forget: update rolling summary in the background
            (async () => {
              try {
                const contextStart = Date.now();
                const messagesText = toCompress
                  .map(m => `${m.role}: ${typeof m.content === "string" ? m.content : "[media]"}`)
                  .join("\n");
                const summaryResp = await openai.chat.completions.create({
                  model: "gpt-4o-mini",
                  messages: [
                    { role: "system", content: "Summarize this conversation segment in under 150 words. Preserve: names, key facts, emotional context, topics, plans. Third person present tense. Be concise." },
                    { role: "user", content: `Existing summary:\n${conversationSummary || "(start of conversation)"}\n\nNew messages:\n${messagesText}\n\nUpdated summary:` },
                  ],
                  max_tokens: 200,
                  temperature: 0.3,
                });
                conversationSummary = summaryResp.choices[0]?.message?.content || conversationSummary;
                console.log(`[Memory:L1] Background summary updated (${conversationSummary.length} chars, ${Date.now() - contextStart}ms)`);

                // Insert/update summary message
                const summaryContent = `[CONVERSATION SO FAR]: ${conversationSummary}`;
                const existingSummaryIdx = chatHistory.findIndex(
                  m => typeof m.content === "string" && (m.content as string).startsWith("[CONVERSATION SO FAR]")
                );
                if (existingSummaryIdx >= 0) {
                  chatHistory[existingSummaryIdx] = { role: "system", content: summaryContent };
                } else {
                  const insertAt = chatHistory.filter(m => m.role === "system").length;
                  chatHistory.splice(insertAt, 0, { role: "system", content: summaryContent });
                }
              } catch (err) {
                console.error("[Memory:L1] Background summary failed:", (err as Error).message);
              }
            })();
          }

          let llmResponse = "";
          const llmStartAt = Date.now();
          try {
            // Single streaming call with tools — auto-detects tool calls vs content.
            // If the model calls a tool, we accumulate chunks, handle it, then do a
            // follow-up streaming call. If it responds with content, TTS starts on the
            // first complete sentence — cutting perceived latency nearly in half.
            const mainStream = await openai.chat.completions.create({
              model: OPENAI_MODEL,
              messages: getMessagesWithTimeContext(),
              tools: tools,
              tool_choice: "auto",
              stream: true,
              temperature: 0.85,
              max_tokens: 300,
              frequency_penalty: 0.3,
              presence_penalty: 0.2,
            });

            // --- Shared state for streaming ---
            let sentenceBuffer = "";
            let fullResponse = "";
            let ttsStarted = false;
            let ttsFirstChunkLogged = false;
            let ttsStartedAt = 0;
            let firstTokenLogged = false;

            // --- Inline expression tag parsing (Phase 1 buffering) ---
            let tagParsed = false;
            let tagBuffer = "";
            let parsedEmotion = "neutral"; // will be set from [EMO:...] tag
            let streamSentenceIndex = 0; // for inter-sentence pacing
            let firstCharsLogged = false; // debug: log first chars of LLM response

            // --- Tool call accumulation ---
            let hasToolCalls = false;
            const toolCallAccum: Record<number, { id: string; name: string; arguments: string }> = {};

            const speakSentence = async (text: string) => {
              if (interruptRequested || thisResponseId !== currentResponseId) return; // Barge-in or stale response
              if (!ttsStartedAt) ttsStartedAt = Date.now();

              // Add emotional pacing delay between sentences (not before first)
              if (streamSentenceIndex > 0) {
                const delay = EMOTION_SENTENCE_DELAY[parsedEmotion] || 0;
                if (delay > 0) {
                  await new Promise(resolve => setTimeout(resolve, delay));
                }
              }
              if (interruptRequested || thisResponseId !== currentResponseId) return; // Check again after pacing delay
              streamSentenceIndex++;

              await new Promise<void>((resolve) => {
                console.log(`[TTS] Creating Azure TTS instance (${currentVoiceConfig.voiceName}, emotion: ${parsedEmotion})`);
                const tts = new AzureTTSStreamer({ ...currentVoiceConfig, emotion: parsedEmotion });
                tts.on("audio_chunk", (chunk: Buffer) => {
                  if (interruptRequested || thisResponseId !== currentResponseId) {
                    return; // Don't send this chunk — interrupted or stale
                  }
                  if (!ttsFirstChunkLogged) {
                    ttsFirstChunkLogged = true;
                    console.log(`[Latency] TTS first audio: ${Date.now() - ttsStartedAt}ms`);
                    console.log(`[Latency] E2E (EOU → first audio): ${Date.now() - eouReceivedAt}ms`);
                  }
                  ws.send(chunk);
                });
                tts.on("tts_complete", () => resolve());
                tts.on("error", (err: Error) => {
                  console.error(`[TTS] ❌ Stream chunk failed: "${text}"`, err);
                  resolve();
                });
                tts.synthesize(text);
              });
            };

            interruptRequested = false; // Safe to reset — old TTS killed by generation ID

            for await (const chunk of mainStream) {
              const delta = chunk.choices[0]?.delta;

              // --- Tool call path: accumulate fragments ---
              if (delta?.tool_calls) {
                hasToolCalls = true;
                for (const tc of delta.tool_calls) {
                  const idx = tc.index;
                  if (!toolCallAccum[idx]) {
                    toolCallAccum[idx] = { id: "", name: "", arguments: "" };
                  }
                  if (tc.id) toolCallAccum[idx].id = tc.id;
                  if (tc.function?.name) toolCallAccum[idx].name = tc.function.name;
                  if (tc.function?.arguments) toolCallAccum[idx].arguments += tc.function.arguments;
                }
                continue;
              }

              // --- Content path: stream to TTS ---
              const content = delta?.content || "";
              if (!content) continue;

              if (!firstTokenLogged) {
                firstTokenLogged = true;
                console.log(`[Latency] LLM first token: ${Date.now() - llmStartAt}ms`);
              }

              // Lazily initialize TTS pipeline on first content delta
              if (!ttsStarted) {
                ttsStarted = true;
                if (silenceTimer) clearTimeout(silenceTimer);
                setState("speaking");
                ws.send(JSON.stringify({ type: "state_speaking" }));
                ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
                await new Promise(resolve => setImmediate(resolve));
              }

              sentenceBuffer += content;
              fullResponse += content;

              // --- Phase 1: Buffer initial tokens to parse [EMO:...] tag ---
              if (!tagParsed) {
                tagBuffer += content;
                if (!firstCharsLogged && tagBuffer.length >= 30) {
                  firstCharsLogged = true;
                  console.log(`[ExprTag] First 60 chars of LLM response: "${tagBuffer.slice(0, 60)}"`);
                }
                const closeBracket = tagBuffer.indexOf("]");
                if (closeBracket !== -1) {
                  // Found the closing bracket — parse the tag
                  tagParsed = true;
                  const rawTag = tagBuffer.slice(0, closeBracket + 1);
                  const remainder = tagBuffer.slice(closeBracket + 1);
                  const parsed = parseExpressionTag(rawTag);
                  if (parsed) {
                    parsedEmotion = parsed.emotion;
                    sendExpressionFromTag(parsed, "stream tag");
                    tagSuccessCount++;
                    console.log(`[ExprTag] Parsed from stream: ${rawTag}`);
                  } else {
                    tagFallbackCount++;
                    console.log(`[ExprTag] Failed to parse from stream: "${rawTag}", defaulting neutral`);
                    sendExpressionFromTag({ emotion: "neutral" }, "stream fallback");
                  }
                  // Strip the tag from sentenceBuffer (it was already appended)
                  sentenceBuffer = sentenceBuffer.replace(rawTag, "").trimStart();
                } else if (tagBuffer.length > 50) {
                  // Safety: no tag found after 50 chars — give up and treat as normal text
                  tagParsed = true;
                  tagFallbackCount++;
                  console.log(`[ExprTag] No tag found after ${tagBuffer.length} chars, defaulting neutral`);
                  sendExpressionFromTag({ emotion: "neutral" }, "stream no-tag fallback");
                } else {
                  continue; // Still buffering tag — don't process sentences yet
                }
              }

              // Flush complete sentences to TTS immediately
              const match = sentenceBuffer.match(/^(.*?[.!?…]+\s+(?=[A-Z"]))/s);
              if (match) {
                const sentence = stripEmotionTags(match[1].trim());
                sentenceBuffer = sentenceBuffer.slice(match[0].length);
                if (sentence.length > 0) {
                  console.log(`[TTS] Streaming sentence: "${sentence}"`);
                  await speakSentence(sentence);
                }
              }
            }

            // --- After stream ends: handle tool calls or finalize content ---
            if (hasToolCalls) {
              // Process accumulated tool calls
              const toolCallsArray = Object.values(toolCallAccum);
              chatHistory.push({
                role: "assistant",
                content: null,
                tool_calls: toolCallsArray.map(tc => ({
                  id: tc.id,
                  type: "function" as const,
                  function: { name: tc.name, arguments: tc.arguments },
                })),
              });

              for (const tc of toolCallsArray) {
                if (tc.name === "update_viewing_context") {
                  try {
                    const args = JSON.parse(tc.arguments);
                    viewingContext = args.context;
                    console.log(`[Context] Updated viewing context to: "${viewingContext}"`);
                    const systemMsg = chatHistory[0] as OpenAI.Chat.ChatCompletionSystemMessageParam;
                    if (systemMsg) {
                      let sysContent = systemMsg.content as string;
                      const contextMarker = "\n\n[CURRENT CONTEXT]:";
                      if (sysContent.includes(contextMarker)) {
                        sysContent = sysContent.split(contextMarker)[0];
                      }
                      systemMsg.content = sysContent + `${contextMarker} ${viewingContext}`;
                    }
                    chatHistory.push({
                      role: "tool",
                      tool_call_id: tc.id,
                      content: `Context updated to: ${viewingContext}`,
                    });
                  } catch (parseErr) {
                    console.error("[Tool] Failed to parse tool args:", parseErr);
                  }
                }
              }

              // Follow-up streaming call after tool processing (tools omitted to prevent chaining)
              if (silenceTimer) clearTimeout(silenceTimer);
              setState("speaking");
              ws.send(JSON.stringify({ type: "state_speaking" }));
              ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
              await new Promise(resolve => setImmediate(resolve));

              try {
                const followUpStream = await openai.chat.completions.create({
                  model: OPENAI_MODEL,
                  messages: getMessagesWithTimeContext(),
                  stream: true,
                  temperature: 0.85,
                  max_tokens: 300,
                  frequency_penalty: 0.3,
                  presence_penalty: 0.2,
                });

                // Reset tag parsing for the follow-up stream (new LLM call = new tag)
                let followUpTagParsed = false;
                let followUpTagBuffer = "";
                let followUpFirstCharsLogged = false;
                // Reset sentence index for follow-up pacing
                streamSentenceIndex = 0;

                for await (const chunk of followUpStream) {
                  const content = chunk.choices[0]?.delta?.content || "";
                  if (!content) continue;
                  if (!firstTokenLogged) {
                    firstTokenLogged = true;
                    console.log(`[Latency] LLM first token (tool follow-up): ${Date.now() - llmStartAt}ms`);
                  }
                  sentenceBuffer += content;
                  fullResponse += content;

                  // --- Phase 1: Buffer initial tokens to parse [EMO:...] tag ---
                  if (!followUpTagParsed) {
                    followUpTagBuffer += content;
                    if (!followUpFirstCharsLogged && followUpTagBuffer.length >= 30) {
                      followUpFirstCharsLogged = true;
                      console.log(`[ExprTag] First 60 chars of follow-up LLM response: "${followUpTagBuffer.slice(0, 60)}"`);
                    }
                    const closeBracket = followUpTagBuffer.indexOf("]");
                    if (closeBracket !== -1) {
                      followUpTagParsed = true;
                      const rawTag = followUpTagBuffer.slice(0, closeBracket + 1);
                      const parsed = parseExpressionTag(rawTag);
                      if (parsed) {
                        parsedEmotion = parsed.emotion;
                        sendExpressionFromTag(parsed, "tool follow-up tag");
                        tagSuccessCount++;
                        console.log(`[ExprTag] Parsed from tool follow-up: ${rawTag}`);
                      } else {
                        tagFallbackCount++;
                        sendExpressionFromTag({ emotion: "neutral" }, "tool follow-up fallback");
                      }
                      sentenceBuffer = sentenceBuffer.replace(rawTag, "").trimStart();
                    } else if (followUpTagBuffer.length > 50) {
                      followUpTagParsed = true;
                      tagFallbackCount++;
                      sendExpressionFromTag({ emotion: "neutral" }, "tool follow-up no-tag fallback");
                    } else {
                      continue;
                    }
                  }

                  const match = sentenceBuffer.match(/^(.*?[.!?…]+\s+(?=[A-Z"]))/s);
                  if (match) {
                    const sentence = stripEmotionTags(match[1].trim());
                    sentenceBuffer = sentenceBuffer.slice(match[0].length);
                    if (sentence.length > 0) {
                      console.log(`[TTS] Streaming sentence: "${sentence}"`);
                      await speakSentence(sentence);
                    }
                  }
                }
              } catch (followErr) {
                console.error("[Pipeline] Tool follow-up streaming error:", (followErr as Error).message);
              }
            }

            // Flush remaining sentence buffer
            if (sentenceBuffer.trim().length > 0) {
              // Initialize TTS pipeline if nothing was spoken yet (very short response)
              if (!ttsStarted) {
                ttsStarted = true;
                if (silenceTimer) clearTimeout(silenceTimer);
                setState("speaking");
                ws.send(JSON.stringify({ type: "state_speaking" }));
                ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
                await new Promise(resolve => setImmediate(resolve));
              }
              const cleanFinal = stripEmotionTags(sentenceBuffer.trim());
              if (cleanFinal.length > 0) {
                await speakSentence(cleanFinal);
              }
            }

            const llmDoneAt = Date.now();
            console.log(`[Latency] LLM total: ${llmDoneAt - llmStartAt}ms (${fullResponse.length} chars)`);
            llmResponse = stripEmotionTags(stripExpressionTag(fullResponse));

            // If tag wasn't parsed from stream (very short response), parse from full text now
            if (!tagParsed && llmResponse.trim().length > 0) {
              const fallbackParsed = parseExpressionTag(fullResponse);
              if (fallbackParsed) {
                parsedEmotion = fallbackParsed.emotion;
                sendExpressionFromTag(fallbackParsed, "full response fallback");
                tagSuccessCount++;
              } else {
                sendExpressionFromTag({ emotion: "neutral" }, "full response no-tag fallback");
                tagFallbackCount++;
              }
            }

            if (llmResponse.trim().length > 0) {
              chatHistory.push({ role: "assistant", content: llmResponse });
              advanceTimePhase(llmResponse);
            }

            // Vision response length safety net
            if (visionActive && llmResponse.length > 150) {
              const userAskedQuestion = /\?$|\bwhat\b|\bwhy\b|\bhow\b|\bwho\b|\bwhere\b|\bwhen\b|\bdo you\b|\bcan you\b|\btell me\b/i.test(userMessage);
              if (!userAskedQuestion) {
                console.log(`[Vision] Warning: Long response during co-watching: ${llmResponse.length} chars`);
              }
            }

            console.log(`[AI RESPONSE]: "${llmResponse}"`);
            lastKiraSpokeTimestamp = Date.now();
            if (visionActive) rescheduleVisionReaction();
            ws.send(JSON.stringify({ type: "transcript", role: "ai", text: llmResponse }));

            // Latency summary
            const ttsTotal = ttsStartedAt ? Date.now() - ttsStartedAt : 0;
            const e2eTotal = Date.now() - eouReceivedAt;
            console.log(`[Latency] TTS total: ${ttsTotal}ms`);
            console.log(`[Latency Summary] LLM: ${llmDoneAt - llmStartAt}ms | TTS: ${ttsTotal}ms | E2E: ${e2eTotal}ms`);

          } catch (err) {
            console.error("[Pipeline] ❌ OpenAI Error:", (err as Error).message);
          } finally {
            // Always return to listening state and clean up
            try {
              ws.send(JSON.stringify({ type: "tts_chunk_ends" }));
            } catch (_) { /* ws may be closed */ }
            currentTurnTranscript = "";
            currentInterimTranscript = "";
            transcriptClearedAt = Date.now();
            setState("listening");
            try {
              ws.send(JSON.stringify({ type: "state_listening" }));
            } catch (_) { /* ws may be closed */ }
            console.log("[STATE] Back to listening, transcripts cleared.");
            resetSilenceTimer();
          }
        } else if (controlMessage.type === "interrupt") {
          // Client-initiated interrupt (e.g. user clicks stop button)
          // Server-side barge-in is handled in the transcript handler instead
          console.log("[WS] Client interrupt received");
          if (state === "speaking") {
            interruptRequested = true;
            currentResponseId++; // Invalidate any in-flight TTS callbacks
            setState("listening");
            ws.send(JSON.stringify({ type: "state_listening" }));
          }
        } else if (controlMessage.type === "image") {
          // Handle incoming image snapshot
          // Support both single 'image' (legacy/fallback) and 'images' array
          if (controlMessage.images && Array.isArray(controlMessage.images)) {
             // Validate & cap incoming images
             const validImages = controlMessage.images
               .filter((img: unknown) => typeof img === "string" && img.length < 2_000_000)
               .slice(0, 5);
             if (validImages.length === 0) return;
             console.log(`[Vision] Received ${validImages.length} images (${controlMessage.images.length} sent). Updating buffer.`);
             latestImages = validImages;
             lastImageTimestamp = Date.now();
             if (!visionActive) {
               visionActive = true;
               console.log("[Vision] Screen share activated. Starting reaction timer.");
               startVisionReactionTimer();
             }
             lastVisionTimestamp = Date.now();
          } else if (controlMessage.image && typeof controlMessage.image === "string" && controlMessage.image.length < 2_000_000) {
            console.log("[Vision] Received single image snapshot. Updating buffer.");
            latestImages = [controlMessage.image];
            lastImageTimestamp = Date.now();
            if (!visionActive) {
              visionActive = true;
              console.log("[Vision] Screen share activated. Starting reaction timer.");
              startVisionReactionTimer();
            }
            lastVisionTimestamp = Date.now();
          }
        } else if (controlMessage.type === "scene_update" && controlMessage.images && Array.isArray(controlMessage.images)) {
          // Validate & cap scene update images
          const validSceneImages = controlMessage.images
            .filter((img: unknown) => typeof img === "string" && img.length < 2_000_000)
            .slice(0, 5);
          // Scene updates also confirm vision is active
          if (!visionActive) {
            visionActive = true;
            console.log("[Vision] Screen share activated via scene_update. Starting reaction timer.");
            startVisionReactionTimer();
          }
          // Also update latestImages so the buffer stays fresh during silent watching
          if (validSceneImages.length > 0) {
            latestImages = validSceneImages;
            lastImageTimestamp = Date.now();
          }
          lastVisionTimestamp = Date.now();

          // --- WATCH-TOGETHER: Occasional scene reactions ---
          const now = Date.now();
          const SCENE_REACTION_COOLDOWN = 45000; // Max once per 45 seconds
          const SCENE_REACTION_CHANCE = 0.3;      // 30% chance to react

          if (
            viewingContext &&
            state === "listening" &&
            timeWarningPhase !== 'done' && timeWarningPhase !== 'final_goodbye' &&
            now - lastSceneReactionTime > SCENE_REACTION_COOLDOWN &&
            Math.random() < SCENE_REACTION_CHANCE
          ) {
            lastSceneReactionTime = now;
            console.log(`[Scene] Evaluating scene reaction (watching: ${viewingContext})`);

            const imageContent: OpenAI.Chat.ChatCompletionContentPart[] = validSceneImages.map((img: string) => ({
              type: "image_url" as const,
              image_url: { url: img.startsWith("data:") ? img : `data:image/jpeg;base64,${img}`, detail: "low" as const },
            }));
            imageContent.push({
              type: "text" as const,
              text: "[Screen changed — react if something interesting happened, or say nothing]",
            });

            const sceneMessages: OpenAI.Chat.ChatCompletionMessageParam[] = [
              {
                role: "system",
                content: `${KIRA_SYSTEM_PROMPT}\n\nYou're watching ${viewingContext} together with the user. You just noticed something change on screen. Give a brief, natural reaction — like a friend sitting next to someone watching. This should be SHORT: a gasp, a laugh, a quick comment, 1 sentence MAX. Examples of good reactions: "Oh no...", "Wait, is that—", "Ha! I love this part.", "Whoa.", "Okay that was intense." Don't narrate or describe what you see. Just react emotionally. If the moment isn't noteworthy, respond with exactly "[SKIP]" and nothing else.`,
              },
              ...chatHistory.filter(m => m.role !== "system").slice(-4),
              { role: "system", content: EXPRESSION_TAG_REMINDER },
              { role: "user", content: imageContent },
            ];

            // Fire-and-forget — don't block the message loop
            (async () => {
              // Bump generation ID so any in-flight TTS from a previous response is invalidated
              currentResponseId++;
              const thisResponseId = currentResponseId;

              // Lock state BEFORE async LLM call to prevent other proactive systems
              // (silence timer, vision reaction) from also starting a turn
              setState("thinking");
              try {
                const reaction = await openai.chat.completions.create({
                  model: OPENAI_MODEL,
                  messages: sceneMessages,
                  max_tokens: 60,
                  temperature: 1.0,
                });

                let reactionText = reaction.choices[0]?.message?.content?.trim() || "";

                // Only speak if there's real content and we're still in a valid state
                if (
                  !reactionText ||
                  reactionText.length < 2 ||
                  reactionText.includes("[SKIP]") ||
                  reactionText === '""' ||
                  reactionText === "''" ||
                  clientDisconnected ||
                  timeWarningPhase as string === 'done' || timeWarningPhase as string === 'final_goodbye'
                ) {
                  console.log(`[Scene] No reaction (text: "${reactionText}", state: ${state})`);
                  setState("listening");
                  ws.send(JSON.stringify({ type: "state_listening" }));
                  return;
                }

                console.log(`[Scene] Kira reacts: "${reactionText}"`);

                // Parse expression tag and strip before TTS
                const sceneTagResult = handleNonStreamingTag(reactionText, "scene reaction");
                reactionText = stripEmotionTags(sceneTagResult.text);
                const sceneEmotion = sceneTagResult.emotion;

                chatHistory.push({ role: "assistant", content: reactionText });
                lastKiraSpokeTimestamp = Date.now();
                // Don't reschedule vision timer from scene reactions — already handled by scheduleNextReaction()
                ws.send(JSON.stringify({ type: "transcript", role: "ai", text: reactionText }));

                // TTS pipeline for scene reaction
                setState("speaking");
                ws.send(JSON.stringify({ type: "state_speaking" }));
                ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
                await new Promise(resolve => setImmediate(resolve));

                const sentences = reactionText.split(/(?<=[.!?…])\s+(?=[A-Z"])/);
                let sceneSentIdx = 0;
                interruptRequested = false; // Safe to reset — old TTS killed by generation ID
                for (const sentence of sentences) {
                  const trimmed = sentence.trim();
                  if (trimmed.length === 0) continue;
                  if (interruptRequested || thisResponseId !== currentResponseId) break;
                  if (sceneSentIdx > 0) {
                    const delay = EMOTION_SENTENCE_DELAY[sceneEmotion] || 0;
                    if (delay > 0) await new Promise(resolve => setTimeout(resolve, delay));
                  }
                  sceneSentIdx++;
                  await new Promise<void>((resolve) => {
                    const tts = new AzureTTSStreamer({ ...currentVoiceConfig, emotion: sceneEmotion });
                    tts.on("audio_chunk", (chunk: Buffer) => {
                      if (interruptRequested || thisResponseId !== currentResponseId) return;
                      if (!clientDisconnected && ws.readyState === ws.OPEN) ws.send(chunk);
                    });
                    tts.on("tts_complete", () => resolve());
                    tts.on("error", (err: Error) => {
                      console.error(`[Scene TTS] ❌ Chunk failed: "${trimmed}"`, err);
                      resolve();
                    });
                    tts.synthesize(trimmed);
                  });
                }

                ws.send(JSON.stringify({ type: "tts_chunk_ends" }));
                setState("listening");
                ws.send(JSON.stringify({ type: "state_listening" }));
                resetSilenceTimer();
              } catch (err) {
                console.error("[Scene] Reaction error:", (err as Error).message);
                setState("listening");
                try { ws.send(JSON.stringify({ type: "state_listening" })); } catch (_) {}
              }
            })();
          }
        } else if (controlMessage.type === "voice_change") {
          const newVoice = controlMessage.voice as "anime" | "natural";
          currentVoiceConfig = VOICE_CONFIGS[newVoice] || VOICE_CONFIGS.natural;
          console.log(`[Voice] Switched to: ${currentVoiceConfig.voiceName} (style: ${currentVoiceConfig.style || "default"})`);
        } else if (controlMessage.type === "vision_stop") {
          stopVision();
        } else if (controlMessage.type === "pong") {
          // Client responded to heartbeat ping — connection is alive
          // Clear the timeout so we don't close the connection
          if (pongTimeoutTimer) {
            clearTimeout(pongTimeoutTimer);
            pongTimeoutTimer = null;
          }
        } else if (controlMessage.type === "text_message") {
          if (timeWarningPhase === 'done') return; // Don't process new messages after goodbye

          // User sent text — cancel proactive goodbye timeout
          if (goodbyeTimeout) { clearTimeout(goodbyeTimeout); goodbyeTimeout = null; }

          // --- TEXT CHAT: Skip STT and TTS, go directly to LLM ---
          if (state !== "listening") return;
          if (silenceTimer) clearTimeout(silenceTimer);

          const userMessage = typeof controlMessage.text === "string" ? controlMessage.text.trim() : "";
          if (!userMessage || userMessage.length === 0) return;
          if (userMessage.length > 2000) return; // Prevent abuse

          // LLM rate limit check
          llmCallCount++;
          if (llmCallCount > LLM_MAX_CALLS_PER_MINUTE) {
            console.warn(`[RateLimit] LLM call rate exceeded (${llmCallCount}/${LLM_MAX_CALLS_PER_MINUTE}/min). Dropping text_message.`);
            return;
          }

          setState("thinking");
          ws.send(JSON.stringify({ type: "state_thinking" }));

          chatHistory.push({ role: "user", content: userMessage });

          // --- CONTEXT MANAGEMENT (non-blocking — same as voice EOU path) ---
          const txtNonSystemCount = chatHistory.filter(m => m.role !== "system").length;
          if (txtNonSystemCount > SUMMARIZE_THRESHOLD) {
            let txtFirstMsgIdx = chatHistory.findIndex(m => m.role !== "system");
            if (
              typeof chatHistory[txtFirstMsgIdx]?.content === "string" &&
              (chatHistory[txtFirstMsgIdx].content as string).startsWith("[CONVERSATION SO FAR]")
            ) {
              txtFirstMsgIdx++;
            }
            const txtToCompress = chatHistory.slice(txtFirstMsgIdx, txtFirstMsgIdx + MESSAGES_TO_SUMMARIZE);
            chatHistory.splice(txtFirstMsgIdx, MESSAGES_TO_SUMMARIZE);
            console.log(`[Context] Text chat: truncated ${MESSAGES_TO_SUMMARIZE} oldest messages. Summary deferred.`);

            // Fire-and-forget background summary
            (async () => {
              try {
                const txtMessagesText = txtToCompress
                  .map(m => `${m.role}: ${typeof m.content === "string" ? m.content : "[media]"}`)
                  .join("\n");
                const txtSummaryResp = await openai.chat.completions.create({
                  model: "gpt-4o-mini",
                  messages: [
                    { role: "system", content: "Summarize this conversation segment in under 150 words. Preserve: names, key facts, emotional context, topics, plans. Third person present tense. Be concise." },
                    { role: "user", content: `Existing summary:\n${conversationSummary || "(start of conversation)"}\n\nNew messages:\n${txtMessagesText}\n\nUpdated summary:` },
                  ],
                  max_tokens: 200,
                  temperature: 0.3,
                });
                conversationSummary = txtSummaryResp.choices[0]?.message?.content || conversationSummary;
                const txtSummaryContent = `[CONVERSATION SO FAR]: ${conversationSummary}`;
                const txtExistingSummaryIdx = chatHistory.findIndex(
                  m => typeof m.content === "string" && (m.content as string).startsWith("[CONVERSATION SO FAR]")
                );
                if (txtExistingSummaryIdx >= 0) {
                  chatHistory[txtExistingSummaryIdx] = { role: "system", content: txtSummaryContent };
                } else {
                  const txtInsertAt = chatHistory.filter(m => m.role === "system").length;
                  chatHistory.splice(txtInsertAt, 0, { role: "system", content: txtSummaryContent });
                }
              } catch (err) {
                console.error("[Memory:L1] Text chat background summary failed:", (err as Error).message);
              }
            })();
          }

          try {
            const txtCompletion = await openai.chat.completions.create({
              model: OPENAI_MODEL,
              messages: getMessagesWithTimeContext(),
              tools: tools,
              tool_choice: "auto",
              temperature: 0.85,
              max_tokens: 300,
              frequency_penalty: 0.3,
              presence_penalty: 0.2,
            });

            const txtInitialMessage = txtCompletion.choices[0]?.message;
            let txtLlmResponse = "";

            if (txtInitialMessage?.tool_calls) {
              chatHistory.push(txtInitialMessage);
              for (const toolCall of txtInitialMessage.tool_calls) {
                if (toolCall.function.name === "update_viewing_context") {
                  const args = JSON.parse(toolCall.function.arguments);
                  viewingContext = args.context;
                  const systemMsg = chatHistory[0] as OpenAI.Chat.ChatCompletionSystemMessageParam;
                  if (systemMsg) {
                    let content = systemMsg.content as string;
                    const contextMarker = "\n\n[CURRENT CONTEXT]:";
                    if (content.includes(contextMarker)) {
                      content = content.split(contextMarker)[0];
                    }
                    systemMsg.content = content + `${contextMarker} ${viewingContext}`;
                  }
                  chatHistory.push({ role: "tool", tool_call_id: toolCall.id, content: `Context updated to: ${viewingContext}` });
                }
              }
              const txtFollowUp = await openai.chat.completions.create({
                model: OPENAI_MODEL,
                messages: getMessagesWithTimeContext(),
                temperature: 0.85,
                max_tokens: 300,
              });
              txtLlmResponse = txtFollowUp.choices[0]?.message?.content || "";
            } else {
              txtLlmResponse = txtInitialMessage?.content || "";
            }

            // Parse expression tag and strip before sending
            const txtTagResult = handleNonStreamingTag(txtLlmResponse, "text chat");
            txtLlmResponse = stripEmotionTags(txtTagResult.text);
            const txtEmotion = txtTagResult.emotion;

            chatHistory.push({ role: "assistant", content: txtLlmResponse });
            advanceTimePhase(txtLlmResponse);

            ws.send(JSON.stringify({
              type: "text_response",
              text: txtLlmResponse,
            }));
          } catch (err) {
            console.error("[TextChat] Error:", (err as Error).message);
            ws.send(JSON.stringify({ type: "error", message: "Failed to get response" }));
          } finally {
            setState("listening");
            ws.send(JSON.stringify({ type: "state_listening" }));
            turnCount++;
            silenceInitiatedLast = false; // User spoke, allow future silence initiation
            resetSilenceTimer();
          }
        }
      } else if (message instanceof Buffer) {
        if (!isAcceptingAudio) return; // Don't forward audio after goodbye or before pipeline ready
        if ((state === "listening" || state === "speaking") && sttStreamer) {
          sttStreamer.write(message); // Forward audio during listening (normal) and speaking (for barge-in detection)
        }
      }
    } catch (err) {
      console.error(
        "[FATAL] MESSAGE HANDLER CRASHED:",
        (err as Error).message
      );
      console.error((err as Error).stack);
      if (ws.readyState === (ws as any).OPEN) {
        ws.send(JSON.stringify({ type: "error", message: "Internal server error" }));
        ws.close(1011, "Internal server error");
      }
    }
  });

  ws.on("close", async (code: number) => {
    console.log(`[WS] Client disconnected. Code: ${code}`);
    clientDisconnected = true;

    // Decrement per-IP connection count
    const ipCount = connectionsPerIp.get(clientIp) || 1;
    if (ipCount <= 1) connectionsPerIp.delete(clientIp);
    else connectionsPerIp.set(clientIp, ipCount - 1);

    clearInterval(keepAliveInterval);
    clearInterval(messageCountResetInterval);
    clearInterval(llmRateLimitInterval);
    if (pongTimeoutTimer) clearTimeout(pongTimeoutTimer);
    if (usageCheckInterval) clearInterval(usageCheckInterval);
    if (timeCheckInterval) clearInterval(timeCheckInterval);
    if (silenceTimer) clearTimeout(silenceTimer);
    if (goodbyeTimeout) clearTimeout(goodbyeTimeout);
    if (visionReactionTimer) { clearTimeout(visionReactionTimer); visionReactionTimer = null; }
    if (comfortTimer) { clearTimeout(comfortTimer); comfortTimer = null; }
    isFirstVisionReaction = true;
    if (sttStreamer) sttStreamer.destroy();

    // --- USAGE: Flush remaining seconds on disconnect ---
    if (isGuest && userId) {
      if (wasBlockedImmediately) {
        console.log(`[USAGE] Skipping flush — connection was blocked on connect`);
      } else if (sessionStartTime) {
        const finalElapsed = Math.floor((Date.now() - sessionStartTime) / 1000);
        const finalTotal = guestUsageBase + finalElapsed;

        // saveGuestUsage has the "never decrease" guard built in
        await saveGuestUsage(userId, finalTotal);
        console.log(`[USAGE] Flushed guest ${userId}: ${finalTotal}s`);
      }
    } else if (!isGuest && userId && sessionStartTime) {
      if (wasBlockedImmediately) {
        console.log(`[USAGE] Skipping flush — connection was blocked on connect`);
      } else if (isProUser) {
        // Pro users: flush to Prisma MonthlyUsage
        const finalElapsed = Math.floor((Date.now() - sessionStartTime) / 1000);
        const finalTotal = proUsageBase + finalElapsed;
        await saveProUsage(userId, finalTotal);
        console.log(`[USAGE] Flushed Pro ${userId}: ${finalTotal}s`);
      } else {
        // Free signed-in users: flush remainder to Prisma
        const finalElapsed = Math.floor((Date.now() - sessionStartTime) / 1000);
        const alreadyCounted = Math.floor(finalElapsed / 30) * 30;
        const remainder = finalElapsed - alreadyCounted;
        if (remainder > 0) {
          try {
            await prisma.user.update({
              where: { clerkId: userId },
              data: {
                dailyUsageSeconds: { increment: remainder },
                lastUsageDate: new Date(),
              },
            });
          } catch (err) {
            console.error("[Usage] Final flush failed:", (err as Error).message);
          }
        }
      }
    }

    // --- GUEST MEMORY BUFFER (save for potential account creation) ---
    if (isGuest && userId) {
      try {
        const userMsgs = chatHistory
          .filter(m => m.role === "user" || m.role === "assistant")
          .map(m => ({
            role: m.role as string,
            content: typeof m.content === "string"
              ? m.content
              : "[media message]",
          }));

        if (userMsgs.length >= 2) {
          bufferGuestConversation(userId, userMsgs, conversationSummary);
        }
      } catch (err) {
        console.error(
          "[Memory] Guest buffer failed:",
          (err as Error).message
        );
      }
    }

    // --- MEMORY EXTRACTION (ALL users — signed-in AND guests) ---
    if (userId) {
      try {
        const userMsgs = chatHistory
          .filter(m => m.role === "user" || m.role === "assistant")
          .map(m => ({
            role: m.role as string,
            content: typeof m.content === "string"
              ? m.content
              : "[media message]",
          }));

        if (userMsgs.length >= 2) {
          // 1. Save conversation to DB (signed-in users only — guests don't have a User row)
          if (!isGuest) {
            try {
              const conversation = await prisma.conversation.create({
                data: {
                  userId: userId,
                  messages: {
                    create: userMsgs.map(m => ({
                      role: m.role,
                      content: m.content,
                    })),
                  },
                },
              });
              console.log(
                `[Memory] Saved conversation ${conversation.id} (${userMsgs.length} messages)`
              );
            } catch (convErr) {
              console.error(
                "[Memory] Conversation save failed:",
                (convErr as Error).message
              );
            }
          }

          // 2. Extract and save memories (runs for BOTH guests and signed-in users)
          // Guests use their guest_<id> as userId in MemoryFact.
          // createdAt timestamp on MemoryFact enables future 30-day cleanup for guests.
          // When a guest signs up, their facts can be migrated by updating userId.
          await extractAndSaveMemories(
            openai,
            prisma,
            userId,
            userMsgs,
            conversationSummary
          );
          console.log(`[Memory] Extraction complete for ${isGuest ? 'guest' : 'user'} ${userId}`);
        }
      } catch (err) {
        console.error(
          "[Memory] Post-disconnect save failed:",
          (err as Error).message
        );
      }
    }
  });

  ws.on("error", (err: Error) => {
    console.error("[WS] WebSocket error:", err);
    clientDisconnected = true;
    clearInterval(keepAliveInterval);
    clearInterval(messageCountResetInterval);
    clearInterval(llmRateLimitInterval);
    if (pongTimeoutTimer) clearTimeout(pongTimeoutTimer);
    if (usageCheckInterval) clearInterval(usageCheckInterval);
    if (timeCheckInterval) clearInterval(timeCheckInterval);
    if (silenceTimer) clearTimeout(silenceTimer);
    if (goodbyeTimeout) clearTimeout(goodbyeTimeout);
    if (sttStreamer) sttStreamer.destroy();
  });
});

// --- GLOBAL ERROR HANDLERS ---
// Prevent unhandled promise rejections from crashing the server and killing all WebSocket connections
process.on('unhandledRejection', (reason, promise) => {
  console.error('[FATAL] Unhandled Promise Rejection:', reason);
  console.error('Promise:', promise);
  // Don't crash - log and continue
});

process.on('uncaughtException', (error) => {
  console.error('[FATAL] Uncaught Exception:', error);
  // For uncaught exceptions, we should exit gracefully after logging
  // But give existing connections time to finish
  setTimeout(() => {
    console.error('[FATAL] Exiting due to uncaught exception');
    process.exit(1);
  }, 5000);
});

// --- START THE SERVER ---
server.listen(PORT, () => {
  console.log(`🚀 Voice pipeline server listening on :${PORT}`);
});
