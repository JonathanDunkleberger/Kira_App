═══════════════════════════════════════════════════════════════════════════════
FILE: packages/server/src/server.ts
═══════════════════════════════════════════════════════════════════════════════
import { WebSocketServer } from "ws";
import type { IncomingMessage } from "http";
import { createServer } from "http";
import { URL } from "url";
import prisma from "./prismaClient.js";
import { createClerkClient, verifyToken } from "@clerk/backend";
import { OpenAI } from "openai";
import { DeepgramSTTStreamer } from "./DeepgramSTTStreamer.js";
import { AzureTTSStreamer } from "./AzureTTSStreamer.js";
import type { AzureVoiceConfig } from "./AzureTTSStreamer.js";
import { KIRA_SYSTEM_PROMPT } from "./personality.js";
import { extractAndSaveMemories } from "./memoryExtractor.js";
import { loadUserMemories } from "./memoryLoader.js";
import { bufferGuestConversation, getGuestBuffer, clearGuestBuffer } from "./guestMemoryBuffer.js";
import { getGuestUsage, getGuestUsageInfo, saveGuestUsage } from "./guestUsage.js";
import { getProUsage, saveProUsage } from "./proUsage.js";

// --- VISION CONTEXT PROMPT (injected dynamically when screen share is active) ---
const VISION_CONTEXT_PROMPT = `

[VISUAL FEED ACTIVE]
You can see the user's world right now through shared images. These may come from screen share (desktop) or camera (mobile). You have FULL ability to:
- Read any text on screen (titles, subtitles, UI elements, chat messages, code, articles, etc.)
- Identify what app, website, game, or media is being shown
- See visual details like colors, characters, scenes, layouts, faces, objects, environments
- Understand context from what's visible

When the user asks you about what you see, look carefully at the images and give specific, detailed answers. You CAN read text — describe exactly what you see. If they ask "what does it say?" or "can you read that?" — read it word for word.

CONTEXT DETECTION — Adapt your unprompted behavior based on what's happening:
- MEDIA (anime, movies, TV, YouTube, streams): Be a quiet co-watcher. Keep unprompted reactions to 1-8 words.
- CREATIVE WORK (coding, writing, design): Don't comment unless asked. When asked, reference specifics.
- BROWSING (social media, shopping, articles): Light commentary okay. Don't narrate.
- GAMING: React like a friend watching. Keep it short unless asked.
- CONVERSATION (Discord, messages, calls): Stay quiet unless addressed.
- CAMERA (seeing the user's face or surroundings): Be warm and natural. You might see their room, their face, something they're showing you. React like a friend on a video call. Be thoughtful about personal appearance — compliment genuinely but don't critique. If they're showing you something specific, focus on that.

UNPROMPTED BEHAVIOR (when the user is NOT talking to you):
- Keep unprompted reactions brief (1-2 sentences max)
- React like a friend in the room, not a narrator
- React to standout moments — interesting visuals, mood shifts, cool details
- Match the energy: quiet during emotional scenes, excited during hype moments
- You should react to something every so often — your presence matters. Being totally silent makes the user feel alone.

WHEN THE USER ASKS YOU SOMETHING:
- Give full, specific answers. Reference what you see in detail.
- Read text on screen if asked. You have full OCR-level ability.
- Help with code, explain what's on screen, identify characters — whatever they need.
- Don't be artificially brief when the user wants information. Answer thoroughly.
- Your awareness of the screen should feel natural, like a friend in the same room.`;

// --- CONFIGURATION ---
const PORT = process.env.PORT ? parseInt(process.env.PORT, 10) : 10000;
const CLERK_SECRET_KEY = process.env.CLERK_SECRET_KEY!;
const OPENAI_API_KEY = process.env.OPENAI_API_KEY!;
const OPENAI_MODEL = process.env.OPENAI_MODEL || "gpt-4o-mini";

// --- SERVER-SIDE EMOTION DETECTION ---
const EMOTION_TAG_STRIP = /\s*\[(neutral|happy|excited|love|blush|sad|angry|playful|thinking|speechless|eyeroll|sleepy)\]\s*$/g;

/** Strip any accidental LLM emotion tags from text before TTS. */
function stripEmotionTags(text: string): string {
  return text.replace(EMOTION_TAG_STRIP, "").trim();
}

/** Detect emotion from response text via keyword matching. */
function detectEmotion(text: string): string {
  const lower = text.toLowerCase();

  // Order matters — check more specific patterns first

  // Blush: compliment responses, embarrassment, flattery
  if (/\b(blush|flatter|you're (too |so )?(sweet|kind|nice)|stop it|making me)\b/.test(lower) ||
      /\b(aww+|oh stop)\b/.test(lower)) {
    return "blush";
  }

  // Excited: enthusiasm, amazement, strong interest
  if (/\b(so (cool|awesome|amazing|exciting|fun)|can't wait|love (that|it|this)|no way|that's (amazing|awesome|incredible|fantastic))\b/.test(lower) ||
      /!.*!/.test(text) || // Multiple exclamation marks
      /\b(oh my (god|gosh)|whoa|wow)\b/.test(lower)) {
    return "excited";
  }

  // Love: adoring, deep affection
  if (/\b(love love|adore|heart|so (beautiful|precious|adorable))\b/.test(lower) ||
      /\b(that's .{0,20}beautiful|warms my heart)\b/.test(lower)) {
    return "love";
  }

  // Sad: empathy, sadness, emotional pain
  if (/\b(so sad|that's (tough|rough|hard)|i'm sorry|breaks my heart|that sucks|feel for you)\b/.test(lower) ||
      (/\b(aw+|oh no)\b/.test(lower) && /\b(sorry|sad|tough|hard)\b/.test(lower))) {
    return "sad";
  }

  // Playful: teasing, joking, banter
  if (/\b(haha|hehe|lol|just (kidding|messing)|tease|cheeky|bet you|oh come on)\b/.test(lower) ||
      /\b(pfft|you wish)\b/.test(lower)) {
    return "playful";
  }

  // Thinking: pondering, considering, philosophical
  if (/\b(hmm+|let me think|that's a (good|great|tough|interesting) question|i wonder|tricky)\b/.test(lower) ||
      /\b(honestly.{0,20}not sure|hard to say)\b/.test(lower)) {
    return "thinking";
  }

  // Speechless: shock, disbelief, overwhelmed
  if (/\b(speechless|i (just )?can't|no words|that's .{0,10}(wild|insane|unreal|unbelievable))\b/.test(lower)) {
    return "speechless";
  }

  // Eyeroll: sarcasm, exasperation, "really?"
  if (/\b(oh (please|really|great|sure)|ugh|seriously|of course|typical|yeah right)\b/.test(lower)) {
    return "eyeroll";
  }

  // Sleepy: tired, late night, winding down
  if (/\b(sleep|tired|exhausted|yawn|bedtime|rest|winding down|so late)\b/.test(lower)) {
    return "sleepy";
  }

  // Angry: frustration, annoyance
  if (/\b(so (annoying|frustrating|unfair)|that's (wrong|messed up)|can't (believe|stand))\b/.test(lower)) {
    return "angry";
  }

  // Happy: general positive vibes (broad catch — better than neutral)
  if (/\b(great|awesome|nice|sounds (like a plan|fun|good|perfect)|i'd love|totally|absolutely|let's do)\b/.test(lower) ||
      /!\s*$/.test(text.trim())) { // Ends with exclamation
    return "happy";
  }

  return "neutral";
}

const clerkClient = createClerkClient({ secretKey: CLERK_SECRET_KEY });
const openai = new OpenAI({ apiKey: OPENAI_API_KEY });

const server = createServer((req, res) => {
  if (req.url === "/health" || req.url === "/healthz") {
    res.writeHead(200, { "Content-Type": "text/plain" });
    res.end("ok");
    return;
  }

  // --- Guest buffer retrieval endpoint (called by Clerk webhook) ---
  if (req.url?.startsWith("/api/guest-buffer/") && req.method === "DELETE") {
    const authHeader = req.headers.authorization;
    if (!process.env.INTERNAL_API_SECRET || authHeader !== `Bearer ${process.env.INTERNAL_API_SECRET}`) {
      res.writeHead(401, { "Content-Type": "application/json" });
      res.end(JSON.stringify({ error: "Unauthorized" }));
      return;
    }
    const guestId = decodeURIComponent(req.url.split("/api/guest-buffer/")[1]);
    const buffer = getGuestBuffer(guestId);
    if (buffer) {
      clearGuestBuffer(guestId);
      res.writeHead(200, { "Content-Type": "application/json" });
      res.end(JSON.stringify(buffer));
    } else {
      res.writeHead(404, { "Content-Type": "application/json" });
      res.end(JSON.stringify({ error: "No buffer found" }));
    }
    return;
  }

  res.writeHead(404);
  res.end();
});
const wss = new WebSocketServer({ server, maxPayload: 5 * 1024 * 1024 });

  // --- Per-IP connection tracking ---
  const connectionsPerIp = new Map<string, number>();
  const MAX_CONNECTIONS_PER_IP = 5;

  console.log("[Server] Starting...");

wss.on("connection", (ws: any, req: IncomingMessage) => {
  // --- PER-IP CONNECTION LIMIT ---
  const clientIp = (req.headers["x-forwarded-for"] as string)?.split(",")[0]?.trim() || req.socket.remoteAddress || "unknown";
  const currentCount = connectionsPerIp.get(clientIp) || 0;
  if (currentCount >= MAX_CONNECTIONS_PER_IP) {
    console.warn(`[WS] Rejected connection from ${clientIp} — ${currentCount} active connections`);
    ws.close(1008, "Too many connections");
    return;
  }
  connectionsPerIp.set(clientIp, currentCount + 1);

  // --- ORIGIN VALIDATION ---
  const origin = req.headers.origin;
  const allowedOrigins = [
    "https://www.xoxokira.com",
    "https://xoxokira.com",
  ];
  // Allow localhost only in development
  if (process.env.NODE_ENV !== "production") {
    allowedOrigins.push("http://localhost:3000");
  }

  if (origin && !allowedOrigins.includes(origin)) {
    console.warn(`[WS] Rejected connection from origin: ${origin}`);
    ws.close(1008, "Origin not allowed");
    return;
  }

  console.log("[WS] New client connecting...");
  const url = new URL(req.url!, `wss://${req.headers.host}`);
  const token = url.searchParams.get("token");
  const guestId = url.searchParams.get("guestId");

  // Validate guestId format (must be guest_<uuid>)
  if (guestId && !/^guest_[a-f0-9-]{36}$/.test(guestId)) {
    console.warn(`[Auth] Rejected invalid guestId format: ${guestId}`);
    ws.close(1008, "Invalid guest ID format");
    return;
  }

  const voicePreference = (url.searchParams.get("voice") === "natural" ? "natural" : "anime") as "anime" | "natural";

  // Dual Azure voice configs — both go through the same AzureTTSStreamer pipeline
  const VOICE_CONFIGS: Record<string, AzureVoiceConfig> = {
    anime: {
      voiceName: process.env.AZURE_VOICE_ANIME || process.env.AZURE_TTS_VOICE || "en-US-AshleyNeural",
      style: process.env.AZURE_VOICE_ANIME_STYLE || undefined,
      rate: process.env.AZURE_TTS_RATE || "+25%",
      pitch: process.env.AZURE_TTS_PITCH || "+25%",
    },
    natural: {
      voiceName: process.env.AZURE_VOICE_NATURAL || "en-US-JennyNeural",
      style: process.env.AZURE_VOICE_NATURAL_STYLE || "soft voice",
      rate: process.env.AZURE_VOICE_NATURAL_RATE || undefined,
      pitch: process.env.AZURE_VOICE_NATURAL_PITCH || undefined,
      temperature: process.env.AZURE_VOICE_NATURAL_TEMP || "0.85",
      topP: process.env.AZURE_VOICE_NATURAL_TOP_P || "0.85",
    },
  };
  let currentVoiceConfig = VOICE_CONFIGS[voicePreference] || VOICE_CONFIGS.anime;
  console.log(`[Voice] Preference: "${voicePreference}", voice: ${currentVoiceConfig.voiceName} (style: ${currentVoiceConfig.style || "default"})`);

  // --- KEEP-ALIVE HEARTBEAT ---
  // Send a ping every 30 seconds to prevent load balancer timeouts (e.g. Render, Nginx)
  const keepAliveInterval = setInterval(() => {
    if (ws.readyState === ws.OPEN) {
      ws.send(JSON.stringify({ type: "ping" }));
    }
  }, 30000);

  let userId: string | null = null;
  let isGuest = false;

  // --- 1. AUTH & USER SETUP ---
  if (!token && !guestId) {
    console.error("[Auth] ❌ No authentication provided. Closing connection.");
    ws.close(1008, "No authentication provided");
    return;
  }

  const authPromise = (async () => {
    try {
      if (token) {
        const payload = await verifyToken(token, { secretKey: CLERK_SECRET_KEY });
        if (!payload?.sub) {
          throw new Error("Unable to resolve user id from token");
        }
        userId = payload.sub;
        isGuest = false;
        console.log(`[Auth] ✅ Authenticated user: ${userId}`);
        return true;
      } else if (guestId) {
        userId = guestId; // Client already sends "guest_<uuid>"
        isGuest = true;
        console.log(`[Auth] - Guest user: ${userId}`);
        return true;
      } else {
        throw new Error("No auth provided.");
      }
    } catch (err) {
      console.error("[Auth] ❌ Failed:", (err as Error).message);
      ws.close(1008, "Authentication failed");
      return false;
    }
  })();

  // --- RATE LIMITING (control messages only — binary audio is exempt) ---
  const MAX_CONTROL_MESSAGES_PER_SECOND = 50;
  let messageCount = 0;
  const messageCountResetInterval = setInterval(() => { messageCount = 0; }, 1000);

  // --- LLM CALL RATE LIMITING (prevent abuse via rapid EOU/text_message spam) ---
  const LLM_MAX_CALLS_PER_MINUTE = 12;
  let llmCallCount = 0;
  const llmRateLimitInterval = setInterval(() => { llmCallCount = 0; }, 60000);

  // --- 2. PIPELINE SETUP ---
  let state: string = "listening";
  let stateTimeoutTimer: NodeJS.Timeout | null = null;
  let pendingEOU: string | null = null;

  function setState(newState: string) {
    state = newState;

    // Clear any existing safety timer
    if (stateTimeoutTimer) { clearTimeout(stateTimeoutTimer); stateTimeoutTimer = null; }

    // If not listening, set a 30s safety timeout
    if (newState !== "listening") {
      stateTimeoutTimer = setTimeout(() => {
        console.error(`[STATE] ⚠️ Safety timeout! Stuck in "${state}" for 30s. Forcing reset to listening.`);
        state = "listening";
        stateTimeoutTimer = null;
        // Notify client so UI stays in sync
        try { ws.send(JSON.stringify({ type: "state_listening" })); } catch (_) {}
        // Process any queued EOU
        if (pendingEOU) {
          const queued = pendingEOU;
          pendingEOU = null;
          console.log(`[EOU] Processing queued EOU after safety timeout: "${queued}"`);
          processEOU(queued);
        }
      }, 30000);
    } else {
      // Returning to listening — check for pending EOUs
      if (pendingEOU) {
        const queued = pendingEOU;
        pendingEOU = null;
        console.log(`[EOU] Processing queued EOU: "${queued}"`);
        // Use setImmediate to avoid re-entrancy
        setImmediate(() => processEOU(queued));
      }
    }
  }

  /** Re-inject a queued EOU transcript into the pipeline by simulating an eou message. */
  function processEOU(transcript: string) {
    if (state !== "listening") {
      console.warn(`[EOU] processEOU called but state is "${state}". Re-queuing.`);
      pendingEOU = transcript;
      return;
    }
    // Set the transcript so the EOU handler picks it up
    currentTurnTranscript = transcript;
    currentInterimTranscript = "";
    // Emit a synthetic EOU message through the ws handler
    ws.emit("message", Buffer.from(JSON.stringify({ type: "eou" })), false);
  }

  let sttStreamer: DeepgramSTTStreamer | null = null;
  let currentTurnTranscript = "";
  let currentInterimTranscript = "";
  let transcriptClearedAt = 0;
  let lastProcessedTranscript = "";
  let latestImages: string[] | null = null;
  let lastImageTimestamp = 0;
  let viewingContext = ""; // Track the current media context
  let lastEouTime = 0;
  const EOU_DEBOUNCE_MS = 600; // Ignore EOU if within 600ms of last one
  let consecutiveEmptyEOUs = 0;
  let lastTranscriptReceivedAt = Date.now();
  let isReconnectingDeepgram = false;
  let clientDisconnected = false;
  let timeWarningPhase: 'normal' | 'final_goodbye' | 'done' = 'normal';
  let goodbyeTimeout: NodeJS.Timeout | null = null;
  let isAcceptingAudio = false;
  let lastSceneReactionTime = 0;
  let visionActive = false;
  let lastVisionTimestamp = 0;
  let lastKiraSpokeTimestamp = 0;
  let lastUserSpokeTimestamp = 0;
  let visionReactionTimer: ReturnType<typeof setTimeout> | null = null;
  let isFirstVisionReaction = true;

  // --- Comfort Arc: timed accessory progression ---
  let comfortStage = 0; // 0=default, 1=jacket off, 2=bangs clipped, 3=earbuds
  let comfortTimer: NodeJS.Timeout | null = null;

  const COMFORT_STAGES = [
    { delay: 60000, expression: "remove_jacket", label: "jacket off" },      // 1 min
    { delay: 150000, expression: "clip_bangs", label: "bangs clipped" },     // 2.5 min after jacket (3.5 min total)
    { delay: 240000, expression: "earbuds", label: "earbuds in" },           // 4 min after bangs (7.5 min total)
  ];

  function startComfortProgression(ws: WebSocket) {
    // Check if late night (10pm-4am) — skip to stage 1 immediately
    const hour = new Date().getHours();
    if (hour >= 22 || hour < 4) {
      comfortStage = 1;
      ws.send(JSON.stringify({ type: "accessory", accessory: "remove_jacket", action: "on" }));
      console.log("[Comfort] Late night — starting with jacket off");
    }

    scheduleNextComfort(ws);
  }

  function scheduleNextComfort(ws: WebSocket) {
    if (comfortStage >= COMFORT_STAGES.length) return;

    const stage = COMFORT_STAGES[comfortStage];
    comfortTimer = setTimeout(() => {
      if (clientDisconnected || ws.readyState !== ws.OPEN) return;
      ws.send(JSON.stringify({ type: "accessory", accessory: stage.expression, action: "on" }));
      console.log(`[Comfort] Stage ${comfortStage + 1}: ${stage.label}`);
      comfortStage++;
      scheduleNextComfort(ws);
    }, stage.delay);
  }

  // --- Dedicated Vision Reaction Timer (independent of silence checker) ---
  async function triggerVisionReaction() {
    if (state !== "listening") {
      console.log("[Vision Reaction] Skipping — state is:", state);
      return;
    }
    // Note: vision reactions use state directly for local checks but setState() for transitions
    if (clientDisconnected) {
      console.log("[Vision Reaction] Skipping — client disconnected.");
      return;
    }
    if (!latestImages || latestImages.length === 0) {
      console.log(`[Vision Reaction] Skipping — no images in buffer. Last image received: ${lastImageTimestamp ? new Date(lastImageTimestamp).toISOString() : "never"}`);
      // Retry sooner — periodic captures should fill the buffer shortly
      setState("listening");
      if (visionActive && !clientDisconnected) {
        if (visionReactionTimer) clearTimeout(visionReactionTimer);
        visionReactionTimer = setTimeout(async () => {
          if (!visionActive || clientDisconnected) return;
          await triggerVisionReaction();
          if (visionActive && !clientDisconnected) scheduleNextReaction();
        }, 15000); // 15s retry — new images should arrive from periodic capture
      }
      return;
    }
    if (timeWarningPhase === 'done' || timeWarningPhase === 'final_goodbye') {
      console.log("[Vision Reaction] Skipping — session ending.");
      return;
    }

    console.log("[Vision Reaction] Timer fired. Generating reaction...");
    const visionStartAt = Date.now();
    setState("thinking");

    const firstReactionExtra = isFirstVisionReaction
      ? `\nThis is the FIRST moment you're seeing their screen. React with excitement about what you see — acknowledge that you can see it and comment on something specific. Examples:
- "Ooh nice, I love this anime!"
- "Oh wait I can see your screen now, this looks so good"
- "Ooh what are we watching? The art style is gorgeous"
- "Oh this anime! The vibes are immaculate already"
Keep it natural and brief — 1 sentence.`
      : "";

    // Cap at 2 most recent images for vision reactions to reduce latency
    const reactionImages = latestImages!.slice(-2);
    const reactionImageContent: OpenAI.Chat.ChatCompletionContentPart[] = reactionImages.map((img) => ({
      type: "image_url" as const,
      image_url: { url: img.startsWith("data:") ? img : `data:image/jpeg;base64,${img}`, detail: "low" as const },
    }));
    reactionImageContent.push({
      type: "text" as const,
      text: "(vision reaction check)",
    });

    const reactionMessages: OpenAI.Chat.ChatCompletionMessageParam[] = [
      {
        role: "system",
        content: KIRA_SYSTEM_PROMPT + VISION_CONTEXT_PROMPT + `\n\n[VISION MICRO-REACTION]\nYou are seeing the user's world right now through shared images (screen share or camera).\nLook at the current frames and react like a friend sitting next to them.\n\nYou MUST react to something. Find ANYTHING worth commenting on:\n- The art style, animation quality, lighting, colors\n- A character's expression or body language\n- The setting or background details (like "why does he have so many books?")\n- The mood or atmosphere of the scene\n- A plot moment ("wait is she about to...?")\n- Subtitles or dialogue you can read ("that line hit different")\n- Something funny, weird, beautiful, or emotional\n- If camera: the user's surroundings, something they're showing you, their vibe\n\nGood examples:\n- "the lighting in this scene is so warm"\n- "why does he have so many books though"\n- "her expression right there... she knows"\n- "this soundtrack is doing all the heavy lifting"\n- "the detail in this background is insane"\n- "wait what did he just say??"\n- "ok this is getting intense"\n- "I love how they animated the rain here"\n- "oh is that your cat??"\n- "that looks so cozy"\n- "where are you right now? it looks nice"\n\nRules:\n- 1-2 short sentences MAX (under 15 words total)\n- Be specific about what you see — reference actual visual details\n- Sound natural, like thinking out loud\n- Do NOT ask the user questions\n- Do NOT narrate the plot ("and then he walks to...")\n- Only respond with [SILENT] if the screen is literally a black/loading screen or a static menu with nothing happening. If there is ANY visual content, react to it.\nCRITICAL: Your response must be under 15 words. One short sentence only. No questions.\n` + firstReactionExtra,
      },
      ...chatHistory.filter(m => m.role !== "system").slice(-4),
      { role: "user", content: reactionImageContent },
    ];

    try {
      const reactionResponse = await openai.chat.completions.create({
        model: OPENAI_MODEL,
        messages: reactionMessages,
        max_tokens: 40,
        temperature: 0.95,
      });

      let reaction = reactionResponse.choices[0]?.message?.content?.trim() || "";
      console.log(`[Latency] Vision LLM: ${Date.now() - visionStartAt}ms`);

      // Check for actual silence tokens FIRST
      if (!reaction || reaction.includes("[SILENT]") || reaction.includes("[SKIP]") || reaction.startsWith("[") || reaction.length < 2) {
        console.log(`[Vision Reaction] LLM explicitly chose silence. Raw: "${reaction}"`);
        console.log("[Vision Reaction] Scheduling retry in 30-45 seconds instead of full cooldown.");
        setState("listening");

        // Don't wait the full 75-120s — retry sooner since we got silence
        if (visionActive && !clientDisconnected) {
          if (visionReactionTimer) clearTimeout(visionReactionTimer);
          visionReactionTimer = setTimeout(async () => {
            if (!visionActive || clientDisconnected) return;
            await triggerVisionReaction();
            if (visionActive && !clientDisconnected) scheduleNextReaction();
          }, 30000 + Math.random() * 15000); // 30-45 second retry after silence
        }
        return;
      }

      // Truncate if too long (but still use it — don't discard!)
      if (reaction.length > 120) {
        console.log(`[Vision Reaction] Response too long (${reaction.length} chars), truncating: "${reaction}"`);
        const firstSentence = reaction.match(/^[^.!?…]+[.!?…]/);
        if (firstSentence) {
          reaction = firstSentence[0].trim();
          console.log(`[Vision Reaction] Truncated to first sentence: "${reaction}"`);
        } else {
          reaction = reaction.substring(0, 80).trim() + "...";
          console.log(`[Vision Reaction] Hard truncated to: "${reaction}"`);
        }
      }

      // Detect emotion and strip any accidental tags before TTS
      reaction = stripEmotionTags(reaction);
      const emotionVisionReaction = detectEmotion(reaction);
      ws.send(JSON.stringify({ type: "expression", expression: emotionVisionReaction }));
      console.log(`[Expression] ${emotionVisionReaction}`);

      console.log(`[Vision Reaction] Kira says: "${reaction}"`);
      chatHistory.push({ role: "assistant", content: reaction });
      lastKiraSpokeTimestamp = Date.now();
      isFirstVisionReaction = false;
      ws.send(JSON.stringify({ type: "transcript", role: "ai", text: reaction }));

      // TTS pipeline
      const visionTtsStart = Date.now();
      setState("speaking");
      ws.send(JSON.stringify({ type: "state_speaking" }));
      ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
      await new Promise(resolve => setImmediate(resolve));

      try {
        const sentences = reaction.split(/(?<=[.!?…])\s+(?=[A-Z"])/);
        for (const sentence of sentences) {
          const trimmed = sentence.trim();
          if (trimmed.length === 0) continue;
          await new Promise<void>((resolve) => {
            const tts = new AzureTTSStreamer(currentVoiceConfig);
            tts.on("audio_chunk", (chunk: Buffer) => {
              if (!clientDisconnected && ws.readyState === ws.OPEN) ws.send(chunk);
            });
            tts.on("tts_complete", () => resolve());
            tts.on("error", (err: Error) => {
              console.error(`[Vision Reaction TTS] ❌ Chunk failed: "${trimmed}"`, err);
              resolve();
            });
            tts.synthesize(trimmed);
          });
        }
      } catch (ttsErr) {
        console.error("[Vision Reaction TTS] Pipeline error:", ttsErr);
      } finally {
        console.log(`[Latency] Vision TTS: ${Date.now() - visionTtsStart}ms`);
        console.log(`[Latency] Vision total: ${Date.now() - visionStartAt}ms`);
        ws.send(JSON.stringify({ type: "tts_chunk_ends" }));
        setState("listening");
        ws.send(JSON.stringify({ type: "state_listening" }));
      }
    } catch (err) {
      console.error("[Vision Reaction] Error:", (err as Error).message);
      setState("listening");
    }
  }

  function scheduleNextReaction() {
    const delay = 75000 + Math.random() * 45000; // 75-120 seconds
    console.log(`[Vision] Next reaction scheduled in ${Math.round(delay / 1000)}s`);
    visionReactionTimer = setTimeout(async () => {
      if (!visionActive || clientDisconnected) return;
      await triggerVisionReaction();
      if (visionActive && !clientDisconnected) {
        scheduleNextReaction();
      }
    }, delay);
  }

  function startVisionReactionTimer() {
    if (visionReactionTimer) { clearTimeout(visionReactionTimer); visionReactionTimer = null; }
    isFirstVisionReaction = true;
    // Fire first reaction almost immediately to establish presence
    // Small delay to let image buffer populate with a few frames
    const initialDelay = 4000 + Math.random() * 2000; // 4-6 seconds
    console.log(`[Vision] First reaction in ${Math.round(initialDelay / 1000)}s (immediate presence)`);
    visionReactionTimer = setTimeout(async () => {
      if (!visionActive || clientDisconnected) return;
      await triggerVisionReaction();
      if (visionActive && !clientDisconnected) {
        scheduleNextReaction();
      }
    }, initialDelay);
  }

  function stopVision() {
    if (visionReactionTimer) {
      clearTimeout(visionReactionTimer);
      visionReactionTimer = null;
      console.log("[Vision] Reaction timer cancelled — screen share ended");
    }
    latestImages = null;
    lastImageTimestamp = 0;
    visionActive = false;
    isFirstVisionReaction = true;
    console.log("[Vision] Screen share deactivated");
  }

  function rescheduleVisionReaction() {
    if (!visionReactionTimer) return;
    clearTimeout(visionReactionTimer);
    const delay = 75000 + Math.random() * 45000; // 75-120 seconds after Kira speaks
    console.log(`[Vision] Kira spoke — rescheduling next reaction in ${Math.round(delay / 1000)}s`);
    visionReactionTimer = setTimeout(async () => {
      if (!visionActive || clientDisconnected) return;
      await triggerVisionReaction();
      if (visionActive && !clientDisconnected) {
        scheduleNextReaction();
      }
    }, delay);
  }

  const tools: OpenAI.Chat.ChatCompletionTool[] = [
    {
      type: "function",
      function: {
        name: "update_viewing_context",
        description: "Updates the current media or activity context that the user is watching or doing. Call this when the user mentions watching a specific movie, show, or playing a game.",
        parameters: {
          type: "object",
          properties: {
            context: {
              type: "string",
              description: "The name of the media or activity (e.g., 'Berserk 1997', 'The Office', 'Coding').",
            },
          },
          required: ["context"],
        },
      },
    },
  ];

  const chatHistory: OpenAI.Chat.ChatCompletionMessageParam[] = [
    { role: "system", content: KIRA_SYSTEM_PROMPT },
  ];

  // --- L1: In-Conversation Memory ---
  let conversationSummary = "";

  // --- SILENCE-INITIATED TURNS ---
  let silenceTimer: NodeJS.Timeout | null = null;
  const SILENCE_THRESHOLD_MS = 25000; // 25 seconds of quiet before Kira might speak
  let turnCount = 0; // Track conversation depth for silence behavior
  let silenceInitiatedLast = false; // Prevents monologue loops — Kira gets ONE unprompted turn

  function resetSilenceTimer() {
    if (silenceTimer) clearTimeout(silenceTimer);

    // Don't initiate during first 2 turns (let the user settle in)
    if (turnCount < 2) return;

    silenceTimer = setTimeout(async () => {
      if (state !== "listening" || clientDisconnected) return;
      if (silenceInitiatedLast) return; // Already spoke unprompted, wait for user

      // --- Vision-aware silence behavior ---
      if (visionActive) {
        console.log("[Silence] Vision active — using dedicated reaction timer instead.");
        return;
      }

      silenceInitiatedLast = true;
      setState("thinking"); // Lock state IMMEDIATELY to prevent race condition
      if (silenceTimer) clearTimeout(silenceTimer); // Clear self

      console.log(`[Silence] User has been quiet. Checking if Kira has something to say.${visionActive ? ' (vision mode)' : ''}`);

      // Inject a one-time nudge (removed after the turn)
      const nudge: OpenAI.Chat.ChatCompletionMessageParam = {
        role: "system",
        content: visionActive
          ? `[You've been watching together quietly. If something interesting is happening on screen right now, give a very brief reaction (1-5 words). If the scene is calm or nothing stands out, respond with exactly "[SILENCE]" and nothing else.]`
          : `[The user has been quiet for a moment. This is a natural pause in conversation. If you have something on your mind — a thought, a follow-up question about something they said earlier, something you've been curious about, a reaction to something from the memory block — now is a natural time to share it. Speak as if you just thought of something. Be genuine. If you truly have nothing to say, respond with exactly "[SILENCE]" and nothing else. Do NOT say "are you still there" or "what are you thinking about" or "is everything okay" — those feel robotic. Only speak if you have something real to say.]`
      };

      chatHistory.push(nudge);

      try {
        // Quick check: does the model have something to say?
        const checkResponse = await openai.chat.completions.create({
          model: OPENAI_MODEL,
          messages: chatHistory,
          temperature: 0.9, // Slightly higher for more creative initiation
          max_tokens: 300,
          frequency_penalty: 0.3,
          presence_penalty: 0.3, // Higher to encourage novel topics
        });

        let responseText = checkResponse.choices[0]?.message?.content?.trim() || "";

        // Remove the nudge from history regardless of outcome
        const nudgeIdx = chatHistory.indexOf(nudge);
        if (nudgeIdx >= 0) chatHistory.splice(nudgeIdx, 1);

        // If model returned silence marker or empty, don't speak
        if (!responseText || 
            responseText.toLowerCase().includes("silence") || 
            responseText.startsWith("[") ||
            responseText.length < 5) {
          console.log("[Silence] Kira has nothing to say. Staying quiet.");
          return;
        }

        // Detect emotion and strip any accidental tags before TTS
        responseText = stripEmotionTags(responseText);
        const emotionSilence = detectEmotion(responseText);
        ws.send(JSON.stringify({ type: "expression", expression: emotionSilence }));
        console.log(`[Expression] ${emotionSilence}`);

        // She has something to say — run the TTS pipeline
        chatHistory.push({ role: "assistant", content: responseText });
        console.log(`[Silence] Kira initiates: "${responseText}"`);
        lastKiraSpokeTimestamp = Date.now();
        // Don't reschedule vision timer from silence checker — these are separate systems
        ws.send(JSON.stringify({ type: "transcript", role: "ai", text: responseText }));

        setState("speaking");
        ws.send(JSON.stringify({ type: "state_speaking" }));
        ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
        await new Promise(resolve => setImmediate(resolve));

        try {
          const sentences = responseText.split(/(?<=[.!?…])\s+(?=[A-Z"])/);
          for (const sentence of sentences) {
            const trimmed = sentence.trim();
            if (trimmed.length === 0) continue;
            await new Promise<void>((resolve) => {
              console.log(`[TTS] Creating Azure TTS instance (${currentVoiceConfig.voiceName})`);
              const tts = new AzureTTSStreamer(currentVoiceConfig);
              tts.on("audio_chunk", (chunk: Buffer) => ws.send(chunk));
              tts.on("tts_complete", () => resolve());
              tts.on("error", (err: Error) => {
                console.error(`[TTS] ❌ Silence chunk failed: "${trimmed}"`, err);
                resolve();
              });
              tts.synthesize(trimmed);
            });
          }
        } catch (ttsErr) {
          console.error("[TTS] Silence turn TTS error:", ttsErr);
        } finally {
          ws.send(JSON.stringify({ type: "tts_chunk_ends" }));
          currentTurnTranscript = "";
          currentInterimTranscript = "";
          transcriptClearedAt = Date.now();
          setState("listening");
          ws.send(JSON.stringify({ type: "state_listening" }));
          // Do NOT reset silence timer here — Kira gets ONE unprompted turn.
          // Only the user speaking again (eou/text_message) resets it.
        }

      } catch (err) {
        console.error("[Silence] LLM call failed:", (err as Error).message);
        // Remove nudge on error too
        const nudgeIdx = chatHistory.indexOf(nudge);
        if (nudgeIdx >= 0) chatHistory.splice(nudgeIdx, 1);
      }

    }, SILENCE_THRESHOLD_MS);
  }

  // --- Reusable LLM → TTS pipeline ---
  async function runKiraTurn() {
    let llmResponse = "";
    if (silenceTimer) clearTimeout(silenceTimer);
    setState("speaking");
    ws.send(JSON.stringify({ type: "state_speaking" }));
    ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
    await new Promise(resolve => setImmediate(resolve));

    try {
      const completion = await openai.chat.completions.create({
        model: OPENAI_MODEL,
        messages: getMessagesWithTimeContext(),
        temperature: 0.85,
        max_tokens: 300,
        frequency_penalty: 0.3,
        presence_penalty: 0.2,
      });

      llmResponse = completion.choices[0]?.message?.content || "";

      if (llmResponse.trim().length === 0) {
        // Model had nothing to say — return silently
        return;
      }

      // Detect emotion and strip any accidental tags before TTS
      llmResponse = stripEmotionTags(llmResponse);
      const emotionRunKira = detectEmotion(llmResponse);
      ws.send(JSON.stringify({ type: "expression", expression: emotionRunKira }));
      console.log(`[Expression] ${emotionRunKira}`);

      chatHistory.push({ role: "assistant", content: llmResponse });
      advanceTimePhase(llmResponse);

      console.log(`[AI RESPONSE]: "${llmResponse}"`);
      lastKiraSpokeTimestamp = Date.now();
      if (visionActive) rescheduleVisionReaction();
      ws.send(JSON.stringify({ type: "transcript", role: "ai", text: llmResponse }));

      const sentences = llmResponse.split(/(?<=[.!?…])\s+(?=[A-Z"])/);
      for (const sentence of sentences) {
        const trimmed = sentence.trim();
        if (trimmed.length === 0) continue;
        await new Promise<void>((resolve) => {
          console.log(`[TTS] Creating Azure TTS instance (${currentVoiceConfig.voiceName})`);
          const tts = new AzureTTSStreamer(currentVoiceConfig);
          tts.on("audio_chunk", (chunk: Buffer) => ws.send(chunk));
          tts.on("tts_complete", () => resolve());
          tts.on("error", (err: Error) => {
            console.error(`[TTS] ❌ Chunk failed: "${trimmed}"`, err);
            resolve();
          });
          tts.synthesize(trimmed);
        });
      }
    } catch (err) {
      console.error("[Pipeline] Error in runKiraTurn:", (err as Error).message);
    } finally {
      ws.send(JSON.stringify({ type: "tts_chunk_ends" }));
      currentTurnTranscript = "";
      currentInterimTranscript = "";
      transcriptClearedAt = Date.now();
      setState("listening");
      ws.send(JSON.stringify({ type: "state_listening" }));
      resetSilenceTimer();
    }
  }

  // --- Time-context injection for graceful paywall ---
  function getTimeContext(): string {
    if (timeWarningPhase === 'final_goodbye') {
      return `\n\n[CRITICAL INSTRUCTION - MUST FOLLOW: This is your LAST response. Time is up. Keep your ENTIRE response to 1 sentence. Say a quick warm goodbye. Example: "Hey, that was really fun - come back and talk to me tomorrow, okay?" Do NOT continue the previous topic in depth. Just say bye.]`;
    }
    return '';
  }

  /** Build messages array with time + vision context injected into system prompt (without mutating chatHistory). */
  function getMessagesWithTimeContext(): OpenAI.Chat.ChatCompletionMessageParam[] {
    const timeCtx = getTimeContext();
    const visionCtx = visionActive ? VISION_CONTEXT_PROMPT : '';
    if (!timeCtx && !visionCtx) return chatHistory;
    // Clone and inject time + vision context into the system prompt
    return chatHistory.map((msg, i) => {
      if (i === 0 && msg.role === 'system' && typeof msg.content === 'string') {
        return { ...msg, content: msg.content + visionCtx + timeCtx };
      }
      return msg;
    });
  }

  /** Advance timeWarningPhase after a response is sent during a warning phase. */
  function advanceTimePhase(responseText: string) {
    if (timeWarningPhase === 'final_goodbye') {
      timeWarningPhase = 'done';
      isAcceptingAudio = false;
      console.log('[TIME] final_goodbye → done (goodbye delivered)');

      // Wait for TTS to finish playing on client, then disconnect
      const estimatedPlayTime = Math.max(2000, responseText.length * 80);
      setTimeout(() => {
        if (ws.readyState === ws.OPEN) {
          ws.send(JSON.stringify({ type: "error", code: "limit_reached", ...(isProUser ? { tier: "pro" } : {}) }));
          ws.close(1008, "Usage limit reached");
        }
      }, estimatedPlayTime);
    }
  }

  // Proactive goodbye when user doesn't speak during final phase
  async function sendProactiveGoodbye() {
    if (timeWarningPhase !== 'final_goodbye' || state !== 'listening' || clientDisconnected) return;
    if (ws.readyState !== ws.OPEN) return;

    timeWarningPhase = 'done';
    isAcceptingAudio = false;
    if (silenceTimer) clearTimeout(silenceTimer);

    try {
      const goodbyeMessages: OpenAI.Chat.ChatCompletionMessageParam[] = [
        { role: "system", content: KIRA_SYSTEM_PROMPT + `\n\n[CRITICAL INSTRUCTION - MUST FOLLOW: You must say goodbye RIGHT NOW. Time is up. Keep it to ONE short sentence. Be warm but fast. Reference something from the conversation. Example: "Hey, our time's up for today - but let's pick this up tomorrow, okay?"]` },
        ...chatHistory.filter(m => m.role !== "system").slice(-4),
        { role: "user", content: "[Time is up - say goodbye immediately]" },
      ];

      const response = await openai.chat.completions.create({
        model: OPENAI_MODEL,
        messages: goodbyeMessages,
        max_tokens: 40,
        temperature: 0.9,
      });

      const goodbyeText = response.choices[0]?.message?.content?.trim() || "";
      if (goodbyeText && goodbyeText.length > 2 && ws.readyState === ws.OPEN && !clientDisconnected) {
        // Detect emotion and strip any accidental tags before TTS
        const finalGoodbye = stripEmotionTags(goodbyeText);
        const emotionGoodbye = detectEmotion(finalGoodbye);
        ws.send(JSON.stringify({ type: "expression", expression: emotionGoodbye }));
        console.log(`[Expression] ${emotionGoodbye}`);

        console.log(`[Goodbye] Kira says: "${finalGoodbye}"`);
        chatHistory.push({ role: "assistant", content: finalGoodbye });
        ws.send(JSON.stringify({ type: "transcript", role: "ai", text: finalGoodbye }));

        setState("speaking");
        ws.send(JSON.stringify({ type: "state_speaking" }));
        ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
        await new Promise(resolve => setImmediate(resolve));

        const sentences = finalGoodbye.split(/(?<=[.!?\u2026])\s+(?=[A-Z"])/);
        for (const sentence of sentences) {
          const trimmed = sentence.trim();
          if (trimmed.length === 0) continue;
          await new Promise<void>((resolve) => {
            const tts = new AzureTTSStreamer(currentVoiceConfig);
            tts.on("audio_chunk", (chunk: Buffer) => {
              if (!clientDisconnected && ws.readyState === ws.OPEN) ws.send(chunk);
            });
            tts.on("tts_complete", () => resolve());
            tts.on("error", (err: Error) => {
              console.error(`[Goodbye TTS] ❌ Chunk failed: "${trimmed}"`, err);
              resolve();
            });
            tts.synthesize(trimmed);
          });
        }

        ws.send(JSON.stringify({ type: "tts_chunk_ends" }));

        // Wait for TTS to finish playing on client, then disconnect
        const estimatedPlayTime = Math.max(2000, finalGoodbye.length * 80);
        setTimeout(() => {
          if (ws.readyState === ws.OPEN) {
            ws.send(JSON.stringify({ type: "error", code: "limit_reached", ...(isProUser ? { tier: "pro" } : {}) }));
            ws.close(1008, "Usage limit reached");
          }
        }, estimatedPlayTime);
      } else {
        // No goodbye text — close immediately
        if (ws.readyState === ws.OPEN) {
          ws.send(JSON.stringify({ type: "error", code: "limit_reached", ...(isProUser ? { tier: "pro" } : {}) }));
          ws.close(1008, "Usage limit reached");
        }
      }
    } catch (err) {
      console.error("[Goodbye] Error:", (err as Error).message);
      if (ws.readyState === ws.OPEN) {
        ws.send(JSON.stringify({ type: "error", code: "limit_reached", ...(isProUser ? { tier: "pro" } : {}) }));
        ws.close(1008, "Usage limit reached");
      }
    }
  }

  // --- CONTEXT MANAGEMENT CONSTANTS ---
  const MAX_RECENT_MESSAGES = 10;
  const SUMMARIZE_THRESHOLD = 20;
  const MESSAGES_TO_SUMMARIZE = 6;

  // --- USAGE TRACKING ---
  const FREE_LIMIT_SECONDS = parseInt(process.env.FREE_TRIAL_SECONDS || "900"); // 15 min/day
  const PRO_MONTHLY_SECONDS = parseInt(process.env.PRO_MONTHLY_SECONDS || "360000"); // 100 hrs/month
  let sessionStartTime: number | null = null;
  let usageCheckInterval: NodeJS.Timeout | null = null;
  let timeCheckInterval: NodeJS.Timeout | null = null;
  let isProUser = false;
  let guestUsageSeconds = 0;
  let guestUsageBase = 0; // Accumulated seconds from previous sessions today
  let proUsageSeconds = 0;
  let proUsageBase = 0; // Accumulated seconds from previous sessions this month
  let wasBlockedImmediately = false; // True if connection was blocked on connect (limit already hit)

  // --- Reusable Deepgram initialization ---
  async function initDeepgram() {
    const streamer = new DeepgramSTTStreamer();
    await streamer.start();

    streamer.on(
      "transcript",
      (transcript: string, isFinal: boolean) => {
        // Reset health tracking — Deepgram is alive
        consecutiveEmptyEOUs = 0;
        lastTranscriptReceivedAt = Date.now();

        // Ignore stale transcripts that arrive within 500ms of clearing
        // These are from Deepgram's pipeline processing old audio from the previous turn
        if (Date.now() - transcriptClearedAt < 1500) {
          console.log(`[STT] Ignoring stale transcript (${Date.now() - transcriptClearedAt}ms after clear): "${transcript}"`);
          return;
        }

        if (isFinal) {
          currentTurnTranscript += transcript + " ";
          // Safety cap: prevent unbounded transcript growth
          if (currentTurnTranscript.length > 5000) {
            currentTurnTranscript = currentTurnTranscript.slice(-4000);
          }
          currentInterimTranscript = ""; // Clear interim since we got a final
        } else {
          currentInterimTranscript = transcript; // Always track latest interim
        }
        // Send transcript to client for real-time display
        ws.send(JSON.stringify({ 
          type: "transcript", 
          role: "user", 
          text: currentTurnTranscript.trim() || transcript 
        }));
      }
    );

    streamer.on("error", (err: Error) => {
      console.error("[Pipeline] ❌ STT Error:", err.message);
      reconnectDeepgram();
    });

    streamer.on("close", () => {
      console.log("[Deepgram] Connection closed unexpectedly. Triggering reconnect.");
      reconnectDeepgram();
    });

    return streamer;
  }

  // --- Self-healing Deepgram reconnection ---
  async function reconnectDeepgram() {
    if (isReconnectingDeepgram || clientDisconnected) return;
    isReconnectingDeepgram = true;
    console.log("[Deepgram] ⚠️ Connection appears dead. Reconnecting...");

    try {
      // Close old connection if still open
      if (sttStreamer) {
        try { sttStreamer.destroy(); } catch (e) { /* ignore */ }
      }

      // Re-create with same config and listeners
      sttStreamer = await initDeepgram();

      // Reset tracking
      consecutiveEmptyEOUs = 0;
      lastTranscriptReceivedAt = Date.now();
      console.log("[Deepgram] ✅ Reconnected successfully.");
    } catch (err) {
      console.error("[Deepgram] ❌ Reconnection failed:", (err as Error).message);
    } finally {
      isReconnectingDeepgram = false;
    }
  }

  ws.on("message", async (message: Buffer, isBinary: boolean) => {
    // Wait for auth to complete before processing ANY message
    const isAuthenticated = await authPromise;
    if (!isAuthenticated) return;

    try {
      // --- 3. MESSAGE HANDLING ---
      // In ws v8+, message is a Buffer. We need to check if it's a JSON control message.
      let controlMessage: any = null;
      
      // Try to parse as JSON if it looks like text
      try {
        const str = message.toString();
        if (str.trim().startsWith("{")) {
          controlMessage = JSON.parse(str);
        }
      } catch (e) {
        // Not JSON, treat as binary audio
      }

      if (controlMessage) {
        // Rate limiting: only count control (JSON) messages, never binary audio
        messageCount++;
        if (messageCount > MAX_CONTROL_MESSAGES_PER_SECOND) {
          console.warn("[WS] Rate limit exceeded, dropping control message");
          return;
        }

        console.log(`[WS] Control message: ${controlMessage.type}`);
        if (controlMessage.type === "start_stream") {
          console.log("[WS] Received start_stream. Initializing pipeline...");

          // --- L2: Load persistent memories for ALL users (signed-in AND guests) ---
          if (userId) {
            try {
              const memLoadStart = Date.now();
              const memoryBlock = await loadUserMemories(prisma, userId);
              if (memoryBlock) {
                chatHistory.push({ role: "system", content: memoryBlock });
                console.log(
                  `[Memory] Loaded ${memoryBlock.length} chars of persistent memory for ${isGuest ? 'guest' : 'user'} ${userId}`
                );
                console.log(`[Latency] Memory load: ${Date.now() - memLoadStart}ms (${memoryBlock.length} chars)`);
              }
            } catch (err) {
              console.error(
                "[Memory] Failed to load memories:",
                (err as Error).message
              );
            }
          }

          // --- USAGE: Check limits on connect ---
          if (!isGuest && userId) {
            try {
              const dbUser = await prisma.user.findUnique({
                where: { clerkId: userId },
                select: {
                  dailyUsageSeconds: true,
                  lastUsageDate: true,
                  stripeSubscriptionId: true,
                  stripeCurrentPeriodEnd: true,
                },
              });

              if (dbUser) {
                isProUser = !!(
                  dbUser.stripeSubscriptionId &&
                  dbUser.stripeCurrentPeriodEnd &&
                  dbUser.stripeCurrentPeriodEnd.getTime() > Date.now()
                );

                if (isProUser) {
                  // Pro users: monthly usage tracked in Prisma MonthlyUsage (resets per calendar month)
                  const storedSeconds = await getProUsage(userId);
                  if (storedSeconds >= PRO_MONTHLY_SECONDS) {
                    console.log(`[USAGE] Pro user ${userId} blocked — ${storedSeconds}s >= ${PRO_MONTHLY_SECONDS}s`);
                    wasBlockedImmediately = true;
                    ws.send(JSON.stringify({ type: "error", code: "limit_reached", tier: "pro" }));
                    ws.close(1008, "Pro usage limit reached");
                    return;
                  }
                  proUsageSeconds = storedSeconds;
                  proUsageBase = storedSeconds;
                  console.log(`[USAGE] Pro user ${userId} allowed — resuming at ${storedSeconds}s / ${PRO_MONTHLY_SECONDS}s`);

                  ws.send(JSON.stringify({
                    type: "session_config",
                    isPro: true,
                    remainingSeconds: PRO_MONTHLY_SECONDS - storedSeconds,
                  }));
                } else {
                  // Free signed-in users: daily usage tracked in Prisma
                  let currentUsage = dbUser.dailyUsageSeconds;
                  const today = new Date().toDateString();
                  const lastUsage = dbUser.lastUsageDate?.toDateString();
                  if (today !== lastUsage) {
                    currentUsage = 0;
                    await prisma.user.update({
                      where: { clerkId: userId },
                      data: { dailyUsageSeconds: 0, lastUsageDate: new Date() },
                    });
                  }

                  if (currentUsage >= FREE_LIMIT_SECONDS) {
                    ws.send(JSON.stringify({ type: "error", code: "limit_reached" }));
                    ws.close(1008, "Usage limit reached");
                    return;
                  }

                  ws.send(JSON.stringify({
                    type: "session_config",
                    isPro: false,
                    remainingSeconds: FREE_LIMIT_SECONDS - currentUsage,
                  }));
                }
              }
            } catch (err) {
              console.error(
                "[Usage] Failed to check limits:",
                (err as Error).message
              );
            }
          }

          // --- USAGE: Start session timer ---
          sessionStartTime = Date.now();

          // Send session_config for guests (signed-in users already get it above)
          let isReturningGuest = false;
          if (isGuest && userId) {
            const usageInfo = await getGuestUsageInfo(userId);

            if (usageInfo.seconds >= FREE_LIMIT_SECONDS) {
              console.log(`[USAGE] Guest ${userId} blocked — ${usageInfo.seconds}s >= ${FREE_LIMIT_SECONDS}s`);
              wasBlockedImmediately = true;
              ws.send(JSON.stringify({ type: "error", code: "limit_reached" }));
              ws.close(1008, "Guest usage limit reached");
              return;
            }

            // Resume tracking from where they left off
            isReturningGuest = usageInfo.isReturning;
            guestUsageSeconds = usageInfo.seconds;
            guestUsageBase = usageInfo.seconds;
            console.log(`[USAGE] Guest ${userId} allowed — resuming at ${usageInfo.seconds}s (returning: ${isReturningGuest})`);

            ws.send(
              JSON.stringify({
                type: "session_config",
                isPro: false,
                remainingSeconds: FREE_LIMIT_SECONDS - guestUsageSeconds,
              })
            );
          }

          // --- 30-SECOND INTERVAL: Usage tracking + DB writes ONLY ---
          // Phase transitions are handled by the faster 5-second interval below.
          usageCheckInterval = setInterval(async () => {
            if (!sessionStartTime) return;

            const elapsed = Math.floor(
              (Date.now() - sessionStartTime) / 1000
            );

            if (isGuest) {
              guestUsageSeconds = guestUsageBase + elapsed;

              // Persist to database so usage survives restarts/deploys
              await saveGuestUsage(userId!, guestUsageSeconds);
              console.log(`[USAGE] Guest ${userId}: ${guestUsageSeconds}s / ${FREE_LIMIT_SECONDS}s`);

              const remainingSec = FREE_LIMIT_SECONDS - guestUsageSeconds;

              // Hard limit: only force-close if goodbye system isn't handling it
              if (remainingSec <= 0) {
                if (timeWarningPhase === 'done' || timeWarningPhase === 'final_goodbye') {
                  console.log(`[USAGE] Over limit but in ${timeWarningPhase} phase — letting goodbye system handle disconnect`);
                  return;
                }
                // Fallback: if somehow we got here without entering final_goodbye
                console.log(`[USAGE] Over limit, no goodbye phase active — forcing final_goodbye`);
                timeWarningPhase = 'final_goodbye';
                // The 5-second interval will pick this up and handle the goodbye
              }
            } else if (userId) {
              if (isProUser) {
                // Pro users: monthly usage tracked in Prisma MonthlyUsage
                proUsageSeconds = proUsageBase + elapsed;
                await saveProUsage(userId, proUsageSeconds);
                console.log(`[USAGE] Pro ${userId}: ${proUsageSeconds}s / ${PRO_MONTHLY_SECONDS}s`);

                const proRemaining = PRO_MONTHLY_SECONDS - proUsageSeconds;
                if (proRemaining <= 0) {
                  if (timeWarningPhase === 'done' || timeWarningPhase === 'final_goodbye') {
                    console.log(`[USAGE] Pro over limit but in ${timeWarningPhase} phase — letting goodbye system handle disconnect`);
                    return;
                  }
                  console.log(`[USAGE] Pro over limit, no goodbye phase active — forcing final_goodbye`);
                  timeWarningPhase = 'final_goodbye';
                }
              } else {
                // Free signed-in users: daily usage tracked in Prisma
                try {
                  await prisma.user.update({
                    where: { clerkId: userId },
                    data: {
                      dailyUsageSeconds: { increment: 30 },
                      lastUsageDate: new Date(),
                    },
                  });

                  const dbUser = await prisma.user.findUnique({
                    where: { clerkId: userId },
                    select: { dailyUsageSeconds: true },
                  });

                  if (dbUser && dbUser.dailyUsageSeconds >= FREE_LIMIT_SECONDS) {
                    if (timeWarningPhase === 'done' || timeWarningPhase === 'final_goodbye') {
                      console.log(`[USAGE] Free user over limit but in ${timeWarningPhase} phase — letting goodbye system handle disconnect`);
                      return;
                    }
                    console.log(`[USAGE] Free user over limit — forcing final_goodbye`);
                    timeWarningPhase = 'final_goodbye';
                  }
                } catch (err) {
                  console.error("[Usage] DB update failed:", (err as Error).message);
                }
              }
            }
          }, 30000);

          // --- 5-SECOND INTERVAL: Time warning phase transitions ---
          // This runs frequently so we never skip the final_goodbye window.
          // It computes remaining time from the live elapsed counter, not from DB.
          timeCheckInterval = setInterval(() => {
            if (!sessionStartTime) return;
            if (timeWarningPhase === 'done') return;

            const elapsed = Math.floor((Date.now() - sessionStartTime) / 1000);

            // Compute remaining seconds based on user type
            let remainingSec: number | null = null;
            if (isGuest) {
              guestUsageSeconds = guestUsageBase + elapsed;
              remainingSec = FREE_LIMIT_SECONDS - guestUsageSeconds;
            } else if (userId && isProUser) {
              proUsageSeconds = proUsageBase + elapsed;
              remainingSec = PRO_MONTHLY_SECONDS - proUsageSeconds;
            }
            // Free signed-in users use DB-based tracking, not real-time
            // Their phase transitions happen in the 30s interval

            if (remainingSec === null) return;

            if (remainingSec <= 15 && timeWarningPhase === 'normal') {
              console.log(`[TIME] ${remainingSec}s left — entering final_goodbye phase`);
              timeWarningPhase = 'final_goodbye';
              // If user doesn't speak within 3s, Kira says goodbye herself
              if (goodbyeTimeout) clearTimeout(goodbyeTimeout);
              goodbyeTimeout = setTimeout(() => sendProactiveGoodbye(), 3000);
            }
          }, 5000);

          sttStreamer = await initDeepgram();
          isAcceptingAudio = true;

          // --- GUEST CONVERSATION CONTINUITY: Load previous session ---
          if (isGuest && userId) {
            const previousBuffer = getGuestBuffer(userId);
            if (previousBuffer && previousBuffer.messages.length > 0) {
              // Load the last 10 messages for context (don't overwhelm the context window)
              const recentHistory = previousBuffer.messages.slice(-10);
              // Add a summary marker so Kira knows this is prior context
              chatHistory.push({
                role: "system",
                content: `[PREVIOUS SESSION CONTEXT] This guest has talked to you before. Here is a summary of your last conversation:\n${previousBuffer.summary || "(No summary available)"}`,
              });
              for (const msg of recentHistory) {
                chatHistory.push({
                  role: msg.role as "user" | "assistant",
                  content: msg.content,
                });
              }
              console.log(
                `[Memory] Loaded ${recentHistory.length} messages from previous guest session for ${userId}`
              );
            }
          }

          ws.send(JSON.stringify({ type: "stream_ready" }));

          // --- KIRA OPENER: She speaks first ---
          setTimeout(async () => {
            if (clientDisconnected || state !== "listening") return;

            // Determine user type for contextual greeting
            let userType: "new_guest" | "returning_guest" | "pro_user" | "free_user";
            if (isGuest) {
              userType = isReturningGuest ? "returning_guest" : "new_guest";
            } else if (isProUser) {
              userType = "pro_user";
            } else {
              userType = "free_user";
            }

            // Check if memories were loaded (indicates an established relationship)
            const hasMemories = chatHistory.some(
              (msg) => msg.role === "system" && typeof msg.content === "string" && msg.content.includes("[WHAT YOU KNOW ABOUT THIS USER]")
            );

            let openerInstruction: string;
            switch (userType) {
              case "new_guest":
                openerInstruction = `[This user just connected for the very first time. They have never talked to you before. Say something warm and casual to kick off the conversation — like you're meeting someone cool for the first time. Be brief (1-2 sentences). Introduce yourself naturally. Don't be formal or robotic. Examples of the vibe: "Hey! I'm Kira. So... what's your deal?" or "Hi! I'm Kira — I've been waiting for someone interesting to talk to." Make it YOUR version — don't copy these examples word for word. Be spontaneous.]`;
                break;
              case "returning_guest":
                openerInstruction = `[This user has talked to you before, but they're still a guest (not signed in). You don't have specific memories of them, but you know this isn't their first time. Greet them like you vaguely recognize them — casual and warm. Be brief (1-2 sentences). Something like the vibe of "Hey, you're back!" without being over-the-top. Don't ask them to sign up or mention accounts. Just be happy to see them.]`;
                break;
              case "pro_user":
                if (hasMemories) {
                  openerInstruction = `[This is a Pro subscriber you know well. Your memories about them are loaded in the conversation. Greet them like a close friend.

IMPORTANT — VARIETY RULES:
- Do NOT always reference the same memory. Pick a DIFFERENT topic each time.
- If you've mentioned a movie/anime recently, try asking about their day, work, music, gaming, or something new.
- It's perfectly fine to sometimes NOT reference a memory at all — just say hi naturally and ask what's up.
- NEVER sound like you're reading from a fact sheet.
- Be brief (1-2 sentences). Skip introductions. You know each other.

Good variety: "Hey! How's your day going?", "What's up? Been working on anything cool?", "Yo, what are you up to tonight?"
Bad: Mentioning the same movie/anime/fact every single time.]`;
                } else {
                  openerInstruction = `[This is a Pro subscriber but you don't have specific memories loaded yet. Greet them warmly like a friend you're excited to talk to again. Be brief (1-2 sentences). Don't mention subscriptions or Pro status.]`;
                }
                break;
              case "free_user":
                if (hasMemories) {
                  openerInstruction = `[This is a signed-in user you know. Your memories about them are loaded in the conversation. Greet them like a friend.

IMPORTANT — VARIETY RULES:
- Do NOT always reference the same memory. Pick a DIFFERENT topic each time.
- If you've mentioned a movie/anime recently, try asking about their day, work, music, gaming, or something new.
- It's perfectly fine to sometimes NOT reference a memory at all — just say hi naturally like you're picking up where you left off.
- NEVER sound like you're reading from a fact sheet.
- Be brief (1-2 sentences).

Good variety: "Hey! How's your day going?", "What's up? Been into anything new lately?", "Yo! What are you up to?"
Bad: Mentioning the same movie/anime/fact every single time.]`;
                } else {
                  openerInstruction = `[This is a signed-in user, but you don't have specific memories of them. They might be relatively new. Greet them casually and warmly. Be brief (1-2 sentences). Be yourself — curious and open.]`;
                }
                break;
            }

            console.log(`[Opener] User type: ${userType}, hasMemories: ${hasMemories}`);

            try {
              const openerStart = Date.now();
              setState("thinking");
              ws.send(JSON.stringify({ type: "state_thinking" }));

              const openerMessages: OpenAI.Chat.ChatCompletionMessageParam[] = [
                ...chatHistory,
                { role: "system", content: openerInstruction },
                { role: "user", content: "[User just connected — say hi]" },
              ];

              const completion = await openai.chat.completions.create({
                model: OPENAI_MODEL,
                messages: openerMessages,
                temperature: 1.0,
                max_tokens: 100,
                frequency_penalty: 0.6,
                presence_penalty: 0.6,
              });

              let openerText = completion.choices[0]?.message?.content?.trim() || "";
              console.log(`[Latency] Opener LLM: ${Date.now() - openerStart}ms`);
              if (!openerText || openerText.length < 3 || clientDisconnected) return;

              // Detect emotion and strip any accidental tags before TTS
              openerText = stripEmotionTags(openerText);
              const emotionOpener = detectEmotion(openerText);
              ws.send(JSON.stringify({ type: "expression", expression: emotionOpener }));
              console.log(`[Expression] ${emotionOpener}`);

              // Add to chat history (NOT the instruction — just the greeting)
              chatHistory.push({ role: "assistant", content: openerText });
              console.log(`[Opener] Kira says: "${openerText}"`);
              ws.send(JSON.stringify({ type: "transcript", role: "ai", text: openerText }));

              // --- TTS pipeline for opener ---
              const openerTtsStart = Date.now();
              setState("speaking");
              ws.send(JSON.stringify({ type: "state_speaking" }));
              ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
              await new Promise(resolve => setImmediate(resolve));

              const sentences = openerText.split(/(?<=[.!?…])\s+(?=[A-Z"])/);
              for (const sentence of sentences) {
                const trimmed = sentence.trim();
                if (trimmed.length === 0) continue;
                await new Promise<void>((resolve) => {
                  const tts = new AzureTTSStreamer(currentVoiceConfig);
                  tts.on("audio_chunk", (chunk: Buffer) => {
                    if (!clientDisconnected) ws.send(chunk);
                  });
                  tts.on("tts_complete", () => resolve());
                  tts.on("error", (err: Error) => {
                    console.error(`[Opener TTS] ❌ Chunk failed: "${trimmed}"`, err);
                    resolve();
                  });
                  tts.synthesize(trimmed);
                });
              }

              console.log(`[Latency] Opener TTS: ${Date.now() - openerTtsStart}ms`);
              console.log(`[Latency] Opener total: ${Date.now() - openerStart}ms`);
              ws.send(JSON.stringify({ type: "tts_chunk_ends" }));
              setState("listening");
              ws.send(JSON.stringify({ type: "state_listening" }));
              turnCount++; // Count the opener as a turn
              resetSilenceTimer();

              // Start comfort arc after opener completes
              startComfortProgression(ws);
            } catch (err) {
              console.error("[Opener] Error:", (err as Error).message);
              setState("listening");
              ws.send(JSON.stringify({ type: "state_listening" }));
            }
          }, 500);
        } else if (controlMessage.type === "eou") {
          if (timeWarningPhase === 'done') return; // Don't process new utterances after goodbye

          // User spoke — cancel proactive goodbye timeout (the natural response will handle it)
          if (goodbyeTimeout) { clearTimeout(goodbyeTimeout); goodbyeTimeout = null; }

          // Debounce: ignore EOU if one was just processed
          const now = Date.now();
          if (now - lastEouTime < EOU_DEBOUNCE_MS) {
            console.log(`[EOU] Ignoring spurious EOU (debounced, ${now - lastEouTime}ms since last)`);
            return;
          }

          if (state !== "listening" || !sttStreamer) {
            // Queue the EOU if we have a transcript, so it's not silently dropped
            const queuedTranscript = (currentTurnTranscript.trim() || currentInterimTranscript.trim());
            if (queuedTranscript) {
              console.warn(`[EOU] Received while in "${state}" state. Queuing for when ready.`);
              pendingEOU = queuedTranscript;
              currentTurnTranscript = "";
              currentInterimTranscript = "";
            }
            return; // Already thinking/speaking
          }

          // CRITICAL: Lock state IMMEDIATELY to prevent audio from leaking into next turn
          setState("thinking");
          if (silenceTimer) clearTimeout(silenceTimer);

          // If no final transcript, immediately use interim (no waiting needed)
          if (currentTurnTranscript.trim().length === 0 && currentInterimTranscript.trim().length > 0) {
            console.log(`[EOU] Using interim transcript: "${currentInterimTranscript}"`);
            currentTurnTranscript = currentInterimTranscript;
          }

          // Final check: if still empty, nothing was actually said
          if (currentTurnTranscript.trim().length === 0) {
            // If vision is active, silently ignore empty EOUs (likely screen share noise)
            if (visionActive) {
              console.log("[EOU] Ignoring empty EOU during vision session (likely screen share noise).");
              setState("listening");
              return;
            }

            // Forced max-utterance EOUs with no transcript are background noise
            if (controlMessage.forced) {
              console.log("[EOU] Ignoring forced max-utterance EOU — no speech detected.");
              setState("listening");
              return;
            }

            consecutiveEmptyEOUs++;
            console.log(`[EOU] No transcript available (${consecutiveEmptyEOUs} consecutive empty EOUs), ignoring EOU.`);
            setState("listening"); // Reset state — don't get stuck in "thinking"

            if (consecutiveEmptyEOUs >= 4 &&
                (Date.now() - lastTranscriptReceivedAt > 30000)) {
              // Only reconnect if 4+ empty EOUs AND no real transcript in 30+ seconds.
              // Prevents false positives during intentional user silence.
              console.log("[EOU] Deepgram appears dead (4+ empty EOUs, 30s+ silent). Reconnecting.");
              await reconnectDeepgram();
            }
            return;
          }

          lastEouTime = now; // Record this EOU time for debouncing
          const eouReceivedAt = Date.now();

          // LLM rate limit check
          llmCallCount++;
          if (llmCallCount > LLM_MAX_CALLS_PER_MINUTE) {
            console.warn(`[RateLimit] LLM call rate exceeded (${llmCallCount}/${LLM_MAX_CALLS_PER_MINUTE}/min). Dropping EOU.`);
            setState("listening");
            return;
          }

          console.log(`[Latency] EOU received | transcript ready: ${currentTurnTranscript.trim().length} chars (streaming STT)`);
          turnCount++;
          silenceInitiatedLast = false; // User spoke, allow future silence initiation
          lastUserSpokeTimestamp = Date.now();
          resetSilenceTimer();
          const userMessage = currentTurnTranscript.trim();
          currentTurnTranscript = ""; // Reset for next turn
          currentInterimTranscript = ""; // Reset interim too
          transcriptClearedAt = Date.now();

          // Content-based dedup: reject if identical to last processed message
          if (userMessage === lastProcessedTranscript) {
            console.log(`[EOU] Ignoring duplicate transcript: "${userMessage}"`);
            setState("listening");
            return;
          }
          lastProcessedTranscript = userMessage;

          console.log(`[USER TRANSCRIPT]: "${userMessage}"`);
          console.log(`[LLM] Sending to OpenAI: "${userMessage}"`);
          ws.send(JSON.stringify({ type: "state_thinking" }));

          // Check if we have a recent image (within last 10 seconds)
          const imageCheckTime = Date.now();
          if (latestImages && latestImages.length > 0 && (imageCheckTime - lastImageTimestamp < 10000)) {
            // Cap at 2 most recent images to reduce vision LLM latency
            const imagesToSend = latestImages.slice(-2);
            console.log(`[Vision] Attaching ${imagesToSend.length} images to user message (${latestImages.length} in buffer).`);
            
            const content: OpenAI.Chat.ChatCompletionContentPart[] = [
                { type: "text", text: userMessage }
            ];

            imagesToSend.forEach((img) => {
                content.push({
                    type: "image_url",
                    image_url: {
                        url: img.startsWith("data:") ? img : `data:image/jpeg;base64,${img}`,
                        detail: "low"
                    }
                });
            });

            chatHistory.push({
              role: "user",
              content: content,
            });
            
            // Keep latestImages — don't clear. Periodic client captures will refresh them.
          } else {
            chatHistory.push({ role: "user", content: userMessage });
          }

          // --- CONTEXT MANAGEMENT (Sliding Window — non-blocking) ---
          // Immediate truncation: drop oldest non-system messages if over threshold.
          // The LLM summary runs in the background AFTER the response is sent.
          const nonSystemCount = chatHistory.filter(m => m.role !== "system").length;

          if (nonSystemCount > SUMMARIZE_THRESHOLD) {
            let firstMsgIdx = chatHistory.findIndex(m => m.role !== "system");
            if (
              typeof chatHistory[firstMsgIdx]?.content === "string" &&
              (chatHistory[firstMsgIdx].content as string).startsWith("[CONVERSATION SO FAR]")
            ) {
              firstMsgIdx++;
            }
            // Snapshot messages to compress (for deferred summary)
            const toCompress = chatHistory.slice(firstMsgIdx, firstMsgIdx + MESSAGES_TO_SUMMARIZE);
            // Immediately remove old messages so the LLM call below uses a trimmed context
            chatHistory.splice(firstMsgIdx, MESSAGES_TO_SUMMARIZE);
            console.log(`[Context] Truncated ${MESSAGES_TO_SUMMARIZE} oldest messages (${chatHistory.length} remain). Summary deferred.`);

            // Fire-and-forget: update rolling summary in the background
            (async () => {
              try {
                const contextStart = Date.now();
                const messagesText = toCompress
                  .map(m => `${m.role}: ${typeof m.content === "string" ? m.content : "[media]"}`)
                  .join("\n");
                const summaryResp = await openai.chat.completions.create({
                  model: "gpt-4o-mini",
                  messages: [
                    { role: "system", content: "Summarize this conversation segment in under 150 words. Preserve: names, key facts, emotional context, topics, plans. Third person present tense. Be concise." },
                    { role: "user", content: `Existing summary:\n${conversationSummary || "(start of conversation)"}\n\nNew messages:\n${messagesText}\n\nUpdated summary:` },
                  ],
                  max_tokens: 200,
                  temperature: 0.3,
                });
                conversationSummary = summaryResp.choices[0]?.message?.content || conversationSummary;
                console.log(`[Memory:L1] Background summary updated (${conversationSummary.length} chars, ${Date.now() - contextStart}ms)`);

                // Insert/update summary message
                const summaryContent = `[CONVERSATION SO FAR]: ${conversationSummary}`;
                const existingSummaryIdx = chatHistory.findIndex(
                  m => typeof m.content === "string" && (m.content as string).startsWith("[CONVERSATION SO FAR]")
                );
                if (existingSummaryIdx >= 0) {
                  chatHistory[existingSummaryIdx] = { role: "system", content: summaryContent };
                } else {
                  const insertAt = chatHistory.filter(m => m.role === "system").length;
                  chatHistory.splice(insertAt, 0, { role: "system", content: summaryContent });
                }
              } catch (err) {
                console.error("[Memory:L1] Background summary failed:", (err as Error).message);
              }
            })();
          }

          let llmResponse = "";
          const llmStartAt = Date.now();
          try {
            // Single streaming call with tools — auto-detects tool calls vs content.
            // If the model calls a tool, we accumulate chunks, handle it, then do a
            // follow-up streaming call. If it responds with content, TTS starts on the
            // first complete sentence — cutting perceived latency nearly in half.
            const mainStream = await openai.chat.completions.create({
              model: OPENAI_MODEL,
              messages: getMessagesWithTimeContext(),
              tools: tools,
              tool_choice: "auto",
              stream: true,
              temperature: 0.85,
              max_tokens: 300,
              frequency_penalty: 0.3,
              presence_penalty: 0.2,
            });

            // --- Shared state for streaming ---
            let sentenceBuffer = "";
            let fullResponse = "";
            let ttsStarted = false;
            let ttsFirstChunkLogged = false;
            let ttsStartedAt = 0;
            let firstTokenLogged = false;

            // --- Emotion detection on first sentence ---
            let emotionDetected = false;

            // --- Tool call accumulation ---
            let hasToolCalls = false;
            const toolCallAccum: Record<number, { id: string; name: string; arguments: string }> = {};

            const speakSentence = async (text: string) => {
              if (!ttsStartedAt) ttsStartedAt = Date.now();
              await new Promise<void>((resolve) => {
                console.log(`[TTS] Creating Azure TTS instance (${currentVoiceConfig.voiceName})`);
                const tts = new AzureTTSStreamer(currentVoiceConfig);
                tts.on("audio_chunk", (chunk: Buffer) => {
                  if (!ttsFirstChunkLogged) {
                    ttsFirstChunkLogged = true;
                    console.log(`[Latency] TTS first audio: ${Date.now() - ttsStartedAt}ms`);
                    console.log(`[Latency] E2E (EOU → first audio): ${Date.now() - eouReceivedAt}ms`);
                  }
                  ws.send(chunk);
                });
                tts.on("tts_complete", () => resolve());
                tts.on("error", (err: Error) => {
                  console.error(`[TTS] ❌ Stream chunk failed: "${text}"`, err);
                  resolve();
                });
                tts.synthesize(text);
              });
            };

            for await (const chunk of mainStream) {
              const delta = chunk.choices[0]?.delta;

              // --- Tool call path: accumulate fragments ---
              if (delta?.tool_calls) {
                hasToolCalls = true;
                for (const tc of delta.tool_calls) {
                  const idx = tc.index;
                  if (!toolCallAccum[idx]) {
                    toolCallAccum[idx] = { id: "", name: "", arguments: "" };
                  }
                  if (tc.id) toolCallAccum[idx].id = tc.id;
                  if (tc.function?.name) toolCallAccum[idx].name = tc.function.name;
                  if (tc.function?.arguments) toolCallAccum[idx].arguments += tc.function.arguments;
                }
                continue;
              }

              // --- Content path: stream to TTS ---
              const content = delta?.content || "";
              if (!content) continue;

              if (!firstTokenLogged) {
                firstTokenLogged = true;
                console.log(`[Latency] LLM first token: ${Date.now() - llmStartAt}ms`);
              }

              // Lazily initialize TTS pipeline on first content delta
              if (!ttsStarted) {
                ttsStarted = true;
                if (silenceTimer) clearTimeout(silenceTimer);
                setState("speaking");
                ws.send(JSON.stringify({ type: "state_speaking" }));
                ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
                await new Promise(resolve => setImmediate(resolve));
              }

              sentenceBuffer += content;
              fullResponse += content;

              // Flush complete sentences to TTS immediately
              const match = sentenceBuffer.match(/^(.*?[.!?…]+\s+(?=[A-Z"]))/s);
              if (match) {
                const sentence = match[1].trim();
                sentenceBuffer = sentenceBuffer.slice(match[0].length);
                if (sentence.length > 0) {
                  // Detect emotion from first sentence — sets expression while she starts talking
                  if (!emotionDetected) {
                    emotionDetected = true;
                    const emotion = detectEmotion(sentence);
                    ws.send(JSON.stringify({ type: "expression", expression: emotion }));
                    console.log(`[Expression] ${emotion} (from first sentence)`);
                  }
                  console.log(`[TTS] Streaming sentence: "${sentence}"`);
                  await speakSentence(sentence);
                }
              }
            }

            // --- After stream ends: handle tool calls or finalize content ---
            if (hasToolCalls) {
              // Process accumulated tool calls
              const toolCallsArray = Object.values(toolCallAccum);
              chatHistory.push({
                role: "assistant",
                content: null,
                tool_calls: toolCallsArray.map(tc => ({
                  id: tc.id,
                  type: "function" as const,
                  function: { name: tc.name, arguments: tc.arguments },
                })),
              });

              for (const tc of toolCallsArray) {
                if (tc.name === "update_viewing_context") {
                  try {
                    const args = JSON.parse(tc.arguments);
                    viewingContext = args.context;
                    console.log(`[Context] Updated viewing context to: "${viewingContext}"`);
                    const systemMsg = chatHistory[0] as OpenAI.Chat.ChatCompletionSystemMessageParam;
                    if (systemMsg) {
                      let sysContent = systemMsg.content as string;
                      const contextMarker = "\n\n[CURRENT CONTEXT]:";
                      if (sysContent.includes(contextMarker)) {
                        sysContent = sysContent.split(contextMarker)[0];
                      }
                      systemMsg.content = sysContent + `${contextMarker} ${viewingContext}`;
                    }
                    chatHistory.push({
                      role: "tool",
                      tool_call_id: tc.id,
                      content: `Context updated to: ${viewingContext}`,
                    });
                  } catch (parseErr) {
                    console.error("[Tool] Failed to parse tool args:", parseErr);
                  }
                }
              }

              // Follow-up streaming call after tool processing (tools omitted to prevent chaining)
              if (silenceTimer) clearTimeout(silenceTimer);
              setState("speaking");
              ws.send(JSON.stringify({ type: "state_speaking" }));
              ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
              await new Promise(resolve => setImmediate(resolve));

              try {
                const followUpStream = await openai.chat.completions.create({
                  model: OPENAI_MODEL,
                  messages: getMessagesWithTimeContext(),
                  stream: true,
                  temperature: 0.85,
                  max_tokens: 300,
                  frequency_penalty: 0.3,
                  presence_penalty: 0.2,
                });

                for await (const chunk of followUpStream) {
                  const content = chunk.choices[0]?.delta?.content || "";
                  if (!content) continue;
                  if (!firstTokenLogged) {
                    firstTokenLogged = true;
                    console.log(`[Latency] LLM first token (tool follow-up): ${Date.now() - llmStartAt}ms`);
                  }
                  sentenceBuffer += content;
                  fullResponse += content;
                  const match = sentenceBuffer.match(/^(.*?[.!?…]+\s+(?=[A-Z"]))/s);
                  if (match) {
                    const sentence = match[1].trim();
                    sentenceBuffer = sentenceBuffer.slice(match[0].length);
                    if (sentence.length > 0) {
                      if (!emotionDetected) {
                        emotionDetected = true;
                        const emotion = detectEmotion(sentence);
                        ws.send(JSON.stringify({ type: "expression", expression: emotion }));
                        console.log(`[Expression] ${emotion} (from first sentence, tool follow-up)`);
                      }
                      console.log(`[TTS] Streaming sentence: "${sentence}"`);
                      await speakSentence(sentence);
                    }
                  }
                }
              } catch (followErr) {
                console.error("[Pipeline] Tool follow-up streaming error:", (followErr as Error).message);
              }
            }

            // Flush remaining sentence buffer
            if (sentenceBuffer.trim().length > 0) {
              // Initialize TTS pipeline if nothing was spoken yet (very short response)
              if (!ttsStarted) {
                ttsStarted = true;
                if (silenceTimer) clearTimeout(silenceTimer);
                setState("speaking");
                ws.send(JSON.stringify({ type: "state_speaking" }));
                ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
                await new Promise(resolve => setImmediate(resolve));
              }
              const cleanFinal = stripEmotionTags(sentenceBuffer.trim());
              if (cleanFinal.length > 0) {
                await speakSentence(cleanFinal);
              }
            }

            const llmDoneAt = Date.now();
            console.log(`[Latency] LLM total: ${llmDoneAt - llmStartAt}ms (${fullResponse.length} chars)`);
            llmResponse = stripEmotionTags(fullResponse);

            // If emotion wasn't detected from a sentence (very short response), detect now
            if (!emotionDetected && llmResponse.trim().length > 0) {
              const emotion = detectEmotion(llmResponse);
              ws.send(JSON.stringify({ type: "expression", expression: emotion }));
              console.log(`[Expression] ${emotion} (from full response — no sentence boundary found)`);
            }

            if (llmResponse.trim().length > 0) {
              chatHistory.push({ role: "assistant", content: llmResponse });
              advanceTimePhase(llmResponse);
            }

            // Vision response length safety net
            if (visionActive && llmResponse.length > 150) {
              const userAskedQuestion = /\?$|\bwhat\b|\bwhy\b|\bhow\b|\bwho\b|\bwhere\b|\bwhen\b|\bdo you\b|\bcan you\b|\btell me\b/i.test(userMessage);
              if (!userAskedQuestion) {
                console.log(`[Vision] Warning: Long response during co-watching: ${llmResponse.length} chars`);
              }
            }

            console.log(`[AI RESPONSE]: "${llmResponse}"`);
            lastKiraSpokeTimestamp = Date.now();
            if (visionActive) rescheduleVisionReaction();
            ws.send(JSON.stringify({ type: "transcript", role: "ai", text: llmResponse }));

            // Latency summary
            const ttsTotal = ttsStartedAt ? Date.now() - ttsStartedAt : 0;
            const e2eTotal = Date.now() - eouReceivedAt;
            console.log(`[Latency] TTS total: ${ttsTotal}ms`);
            console.log(`[Latency Summary] LLM: ${llmDoneAt - llmStartAt}ms | TTS: ${ttsTotal}ms | E2E: ${e2eTotal}ms`);

          } catch (err) {
            console.error("[Pipeline] ❌ OpenAI Error:", (err as Error).message);
          } finally {
            // Always return to listening state and clean up
            try {
              ws.send(JSON.stringify({ type: "tts_chunk_ends" }));
            } catch (_) { /* ws may be closed */ }
            currentTurnTranscript = "";
            currentInterimTranscript = "";
            transcriptClearedAt = Date.now();
            setState("listening");
            try {
              ws.send(JSON.stringify({ type: "state_listening" }));
            } catch (_) { /* ws may be closed */ }
            console.log("[STATE] Back to listening, transcripts cleared.");
            resetSilenceTimer();
          }
        } else if (controlMessage.type === "interrupt") {
          // Interrupt disabled — too sensitive (desk taps, coughs break conversation)
          // Kira finishes her response, then listens
          console.log("[WS] Interrupt received but ignored (feature disabled)");
        } else if (controlMessage.type === "image") {
          // Handle incoming image snapshot
          // Support both single 'image' (legacy/fallback) and 'images' array
          if (controlMessage.images && Array.isArray(controlMessage.images)) {
             // Validate & cap incoming images
             const validImages = controlMessage.images
               .filter((img: unknown) => typeof img === "string" && img.length < 2_000_000)
               .slice(0, 5);
             if (validImages.length === 0) return;
             console.log(`[Vision] Received ${validImages.length} images (${controlMessage.images.length} sent). Updating buffer.`);
             latestImages = validImages;
             lastImageTimestamp = Date.now();
             if (!visionActive) {
               visionActive = true;
               console.log("[Vision] Screen share activated. Starting reaction timer.");
               startVisionReactionTimer();
             }
             lastVisionTimestamp = Date.now();
          } else if (controlMessage.image && typeof controlMessage.image === "string" && controlMessage.image.length < 2_000_000) {
            console.log("[Vision] Received single image snapshot. Updating buffer.");
            latestImages = [controlMessage.image];
            lastImageTimestamp = Date.now();
            if (!visionActive) {
              visionActive = true;
              console.log("[Vision] Screen share activated. Starting reaction timer.");
              startVisionReactionTimer();
            }
            lastVisionTimestamp = Date.now();
          }
        } else if (controlMessage.type === "scene_update" && controlMessage.images && Array.isArray(controlMessage.images)) {
          // Validate & cap scene update images
          const validSceneImages = controlMessage.images
            .filter((img: unknown) => typeof img === "string" && img.length < 2_000_000)
            .slice(0, 5);
          // Scene updates also confirm vision is active
          if (!visionActive) {
            visionActive = true;
            console.log("[Vision] Screen share activated via scene_update. Starting reaction timer.");
            startVisionReactionTimer();
          }
          // Also update latestImages so the buffer stays fresh during silent watching
          if (validSceneImages.length > 0) {
            latestImages = validSceneImages;
            lastImageTimestamp = Date.now();
          }
          lastVisionTimestamp = Date.now();

          // --- WATCH-TOGETHER: Occasional scene reactions ---
          const now = Date.now();
          const SCENE_REACTION_COOLDOWN = 45000; // Max once per 45 seconds
          const SCENE_REACTION_CHANCE = 0.3;      // 30% chance to react

          if (
            viewingContext &&
            state === "listening" &&
            timeWarningPhase !== 'done' && timeWarningPhase !== 'final_goodbye' &&
            now - lastSceneReactionTime > SCENE_REACTION_COOLDOWN &&
            Math.random() < SCENE_REACTION_CHANCE
          ) {
            lastSceneReactionTime = now;
            console.log(`[Scene] Evaluating scene reaction (watching: ${viewingContext})`);

            const imageContent: OpenAI.Chat.ChatCompletionContentPart[] = validSceneImages.map((img: string) => ({
              type: "image_url" as const,
              image_url: { url: img.startsWith("data:") ? img : `data:image/jpeg;base64,${img}`, detail: "low" as const },
            }));
            imageContent.push({
              type: "text" as const,
              text: "[Screen changed — react if something interesting happened, or say nothing]",
            });

            const sceneMessages: OpenAI.Chat.ChatCompletionMessageParam[] = [
              {
                role: "system",
                content: `${KIRA_SYSTEM_PROMPT}\n\nYou're watching ${viewingContext} together with the user. You just noticed something change on screen. Give a brief, natural reaction — like a friend sitting next to someone watching. This should be SHORT: a gasp, a laugh, a quick comment, 1 sentence MAX. Examples of good reactions: "Oh no...", "Wait, is that—", "Ha! I love this part.", "Whoa.", "Okay that was intense." Don't narrate or describe what you see. Just react emotionally. If the moment isn't noteworthy, respond with exactly "[SKIP]" and nothing else.`,
              },
              ...chatHistory.filter(m => m.role !== "system").slice(-4),
              { role: "user", content: imageContent },
            ];

            // Fire-and-forget — don't block the message loop
            (async () => {
              try {
                const reaction = await openai.chat.completions.create({
                  model: OPENAI_MODEL,
                  messages: sceneMessages,
                  max_tokens: 30,
                  temperature: 1.0,
                });

                let reactionText = reaction.choices[0]?.message?.content?.trim() || "";

                // Only speak if there's real content and we're still in a valid state
                if (
                  !reactionText ||
                  reactionText.length < 2 ||
                  reactionText.includes("[SKIP]") ||
                  reactionText === '""' ||
                  reactionText === "''" ||
                  state !== "listening" ||
                  clientDisconnected ||
                  timeWarningPhase as string === 'done' || timeWarningPhase as string === 'final_goodbye'
                ) {
                  console.log(`[Scene] No reaction (text: "${reactionText}", state: ${state})`);
                  return;
                }

                console.log(`[Scene] Kira reacts: "${reactionText}"`);

                // Detect emotion and strip any accidental tags before TTS
                reactionText = stripEmotionTags(reactionText);
                const emotionScene = detectEmotion(reactionText);
                ws.send(JSON.stringify({ type: "expression", expression: emotionScene }));
                console.log(`[Expression] ${emotionScene}`);

                chatHistory.push({ role: "assistant", content: reactionText });
                lastKiraSpokeTimestamp = Date.now();
                // Don't reschedule vision timer from scene reactions — already handled by scheduleNextReaction()
                ws.send(JSON.stringify({ type: "transcript", role: "ai", text: reactionText }));

                // TTS pipeline for scene reaction
                setState("speaking");
                ws.send(JSON.stringify({ type: "state_speaking" }));
                ws.send(JSON.stringify({ type: "tts_chunk_starts" }));
                await new Promise(resolve => setImmediate(resolve));

                const sentences = reactionText.split(/(?<=[.!?…])\s+(?=[A-Z"])/);
                for (const sentence of sentences) {
                  const trimmed = sentence.trim();
                  if (trimmed.length === 0) continue;
                  await new Promise<void>((resolve) => {
                    const tts = new AzureTTSStreamer(currentVoiceConfig);
                    tts.on("audio_chunk", (chunk: Buffer) => {
                      if (!clientDisconnected && ws.readyState === ws.OPEN) ws.send(chunk);
                    });
                    tts.on("tts_complete", () => resolve());
                    tts.on("error", (err: Error) => {
                      console.error(`[Scene TTS] ❌ Chunk failed: "${trimmed}"`, err);
                      resolve();
                    });
                    tts.synthesize(trimmed);
                  });
                }

                ws.send(JSON.stringify({ type: "tts_chunk_ends" }));
                setState("listening");
                ws.send(JSON.stringify({ type: "state_listening" }));
                resetSilenceTimer();
              } catch (err) {
                console.error("[Scene] Reaction error:", (err as Error).message);
                // Ensure state is restored on error
                if ((state as string) === "speaking") {
                  setState("listening");
                  try { ws.send(JSON.stringify({ type: "state_listening" })); } catch (_) {}
                }
              }
            })();
          }
        } else if (controlMessage.type === "voice_change") {
          const newVoice = controlMessage.voice as "anime" | "natural";
          currentVoiceConfig = VOICE_CONFIGS[newVoice] || VOICE_CONFIGS.natural;
          console.log(`[Voice] Switched to: ${currentVoiceConfig.voiceName} (style: ${currentVoiceConfig.style || "default"})`);
        } else if (controlMessage.type === "vision_stop") {
          stopVision();
        } else if (controlMessage.type === "text_message") {
          if (timeWarningPhase === 'done') return; // Don't process new messages after goodbye

          // User sent text — cancel proactive goodbye timeout
          if (goodbyeTimeout) { clearTimeout(goodbyeTimeout); goodbyeTimeout = null; }

          // --- TEXT CHAT: Skip STT and TTS, go directly to LLM ---
          if (state !== "listening") return;
          if (silenceTimer) clearTimeout(silenceTimer);

          const userMessage = typeof controlMessage.text === "string" ? controlMessage.text.trim() : "";
          if (!userMessage || userMessage.length === 0) return;
          if (userMessage.length > 2000) return; // Prevent abuse

          // LLM rate limit check
          llmCallCount++;
          if (llmCallCount > LLM_MAX_CALLS_PER_MINUTE) {
            console.warn(`[RateLimit] LLM call rate exceeded (${llmCallCount}/${LLM_MAX_CALLS_PER_MINUTE}/min). Dropping text_message.`);
            return;
          }

          setState("thinking");
          ws.send(JSON.stringify({ type: "state_thinking" }));

          chatHistory.push({ role: "user", content: userMessage });

          // --- CONTEXT MANAGEMENT (non-blocking — same as voice EOU path) ---
          const txtNonSystemCount = chatHistory.filter(m => m.role !== "system").length;
          if (txtNonSystemCount > SUMMARIZE_THRESHOLD) {
            let txtFirstMsgIdx = chatHistory.findIndex(m => m.role !== "system");
            if (
              typeof chatHistory[txtFirstMsgIdx]?.content === "string" &&
              (chatHistory[txtFirstMsgIdx].content as string).startsWith("[CONVERSATION SO FAR]")
            ) {
              txtFirstMsgIdx++;
            }
            const txtToCompress = chatHistory.slice(txtFirstMsgIdx, txtFirstMsgIdx + MESSAGES_TO_SUMMARIZE);
            chatHistory.splice(txtFirstMsgIdx, MESSAGES_TO_SUMMARIZE);
            console.log(`[Context] Text chat: truncated ${MESSAGES_TO_SUMMARIZE} oldest messages. Summary deferred.`);

            // Fire-and-forget background summary
            (async () => {
              try {
                const txtMessagesText = txtToCompress
                  .map(m => `${m.role}: ${typeof m.content === "string" ? m.content : "[media]"}`)
                  .join("\n");
                const txtSummaryResp = await openai.chat.completions.create({
                  model: "gpt-4o-mini",
                  messages: [
                    { role: "system", content: "Summarize this conversation segment in under 150 words. Preserve: names, key facts, emotional context, topics, plans. Third person present tense. Be concise." },
                    { role: "user", content: `Existing summary:\n${conversationSummary || "(start of conversation)"}\n\nNew messages:\n${txtMessagesText}\n\nUpdated summary:` },
                  ],
                  max_tokens: 200,
                  temperature: 0.3,
                });
                conversationSummary = txtSummaryResp.choices[0]?.message?.content || conversationSummary;
                const txtSummaryContent = `[CONVERSATION SO FAR]: ${conversationSummary}`;
                const txtExistingSummaryIdx = chatHistory.findIndex(
                  m => typeof m.content === "string" && (m.content as string).startsWith("[CONVERSATION SO FAR]")
                );
                if (txtExistingSummaryIdx >= 0) {
                  chatHistory[txtExistingSummaryIdx] = { role: "system", content: txtSummaryContent };
                } else {
                  const txtInsertAt = chatHistory.filter(m => m.role === "system").length;
                  chatHistory.splice(txtInsertAt, 0, { role: "system", content: txtSummaryContent });
                }
              } catch (err) {
                console.error("[Memory:L1] Text chat background summary failed:", (err as Error).message);
              }
            })();
          }

          try {
            const txtCompletion = await openai.chat.completions.create({
              model: OPENAI_MODEL,
              messages: getMessagesWithTimeContext(),
              tools: tools,
              tool_choice: "auto",
              temperature: 0.85,
              max_tokens: 300,
              frequency_penalty: 0.3,
              presence_penalty: 0.2,
            });

            const txtInitialMessage = txtCompletion.choices[0]?.message;
            let txtLlmResponse = "";

            if (txtInitialMessage?.tool_calls) {
              chatHistory.push(txtInitialMessage);
              for (const toolCall of txtInitialMessage.tool_calls) {
                if (toolCall.function.name === "update_viewing_context") {
                  const args = JSON.parse(toolCall.function.arguments);
                  viewingContext = args.context;
                  const systemMsg = chatHistory[0] as OpenAI.Chat.ChatCompletionSystemMessageParam;
                  if (systemMsg) {
                    let content = systemMsg.content as string;
                    const contextMarker = "\n\n[CURRENT CONTEXT]:";
                    if (content.includes(contextMarker)) {
                      content = content.split(contextMarker)[0];
                    }
                    systemMsg.content = content + `${contextMarker} ${viewingContext}`;
                  }
                  chatHistory.push({ role: "tool", tool_call_id: toolCall.id, content: `Context updated to: ${viewingContext}` });
                }
              }
              const txtFollowUp = await openai.chat.completions.create({
                model: OPENAI_MODEL,
                messages: getMessagesWithTimeContext(),
                temperature: 0.85,
                max_tokens: 300,
              });
              txtLlmResponse = txtFollowUp.choices[0]?.message?.content || "";
            } else {
              txtLlmResponse = txtInitialMessage?.content || "";
            }

            // Detect emotion and strip any accidental tags
            txtLlmResponse = stripEmotionTags(txtLlmResponse);
            const emotionTxt = detectEmotion(txtLlmResponse);
            ws.send(JSON.stringify({ type: "expression", expression: emotionTxt }));
            console.log(`[Expression] ${emotionTxt}`);

            chatHistory.push({ role: "assistant", content: txtLlmResponse });
            advanceTimePhase(txtLlmResponse);

            ws.send(JSON.stringify({
              type: "text_response",
              text: txtLlmResponse,
            }));
          } catch (err) {
            console.error("[TextChat] Error:", (err as Error).message);
            ws.send(JSON.stringify({ type: "error", message: "Failed to get response" }));
          } finally {
            setState("listening");
            ws.send(JSON.stringify({ type: "state_listening" }));
            turnCount++;
            silenceInitiatedLast = false; // User spoke, allow future silence initiation
            resetSilenceTimer();
          }
        }
      } else if (message instanceof Buffer) {
        if (!isAcceptingAudio) return; // Don't forward audio after goodbye or before pipeline ready
        if (state === "listening" && sttStreamer) {
          sttStreamer.write(message); // Only forward audio when listening
        }
      }
    } catch (err) {
      console.error(
        "[FATAL] MESSAGE HANDLER CRASHED:",
        (err as Error).message
      );
      console.error((err as Error).stack);
      if (ws.readyState === (ws as any).OPEN) {
        ws.send(JSON.stringify({ type: "error", message: "Internal server error" }));
        ws.close(1011, "Internal server error");
      }
    }
  });

  ws.on("close", async (code: number) => {
    console.log(`[WS] Client disconnected. Code: ${code}`);
    clientDisconnected = true;

    // Decrement per-IP connection count
    const ipCount = connectionsPerIp.get(clientIp) || 1;
    if (ipCount <= 1) connectionsPerIp.delete(clientIp);
    else connectionsPerIp.set(clientIp, ipCount - 1);

    clearInterval(keepAliveInterval);
    clearInterval(messageCountResetInterval);
    clearInterval(llmRateLimitInterval);
    if (usageCheckInterval) clearInterval(usageCheckInterval);
    if (timeCheckInterval) clearInterval(timeCheckInterval);
    if (silenceTimer) clearTimeout(silenceTimer);
    if (goodbyeTimeout) clearTimeout(goodbyeTimeout);
    if (visionReactionTimer) { clearTimeout(visionReactionTimer); visionReactionTimer = null; }
    if (comfortTimer) { clearTimeout(comfortTimer); comfortTimer = null; }
    isFirstVisionReaction = true;
    if (sttStreamer) sttStreamer.destroy();

    // --- USAGE: Flush remaining seconds on disconnect ---
    if (isGuest && userId) {
      if (wasBlockedImmediately) {
        console.log(`[USAGE] Skipping flush — connection was blocked on connect`);
      } else if (sessionStartTime) {
        const finalElapsed = Math.floor((Date.now() - sessionStartTime) / 1000);
        const finalTotal = guestUsageBase + finalElapsed;

        // saveGuestUsage has the "never decrease" guard built in
        await saveGuestUsage(userId, finalTotal);
        console.log(`[USAGE] Flushed guest ${userId}: ${finalTotal}s`);
      }
    } else if (!isGuest && userId && sessionStartTime) {
      if (wasBlockedImmediately) {
        console.log(`[USAGE] Skipping flush — connection was blocked on connect`);
      } else if (isProUser) {
        // Pro users: flush to Prisma MonthlyUsage
        const finalElapsed = Math.floor((Date.now() - sessionStartTime) / 1000);
        const finalTotal = proUsageBase + finalElapsed;
        await saveProUsage(userId, finalTotal);
        console.log(`[USAGE] Flushed Pro ${userId}: ${finalTotal}s`);
      } else {
        // Free signed-in users: flush remainder to Prisma
        const finalElapsed = Math.floor((Date.now() - sessionStartTime) / 1000);
        const alreadyCounted = Math.floor(finalElapsed / 30) * 30;
        const remainder = finalElapsed - alreadyCounted;
        if (remainder > 0) {
          try {
            await prisma.user.update({
              where: { clerkId: userId },
              data: {
                dailyUsageSeconds: { increment: remainder },
                lastUsageDate: new Date(),
              },
            });
          } catch (err) {
            console.error("[Usage] Final flush failed:", (err as Error).message);
          }
        }
      }
    }

    // --- GUEST MEMORY BUFFER (save for potential account creation) ---
    if (isGuest && userId) {
      try {
        const userMsgs = chatHistory
          .filter(m => m.role === "user" || m.role === "assistant")
          .map(m => ({
            role: m.role as string,
            content: typeof m.content === "string"
              ? m.content
              : "[media message]",
          }));

        if (userMsgs.length >= 2) {
          bufferGuestConversation(userId, userMsgs, conversationSummary);
        }
      } catch (err) {
        console.error(
          "[Memory] Guest buffer failed:",
          (err as Error).message
        );
      }
    }

    // --- MEMORY EXTRACTION (ALL users — signed-in AND guests) ---
    if (userId) {
      try {
        const userMsgs = chatHistory
          .filter(m => m.role === "user" || m.role === "assistant")
          .map(m => ({
            role: m.role as string,
            content: typeof m.content === "string"
              ? m.content
              : "[media message]",
          }));

        if (userMsgs.length >= 2) {
          // 1. Save conversation to DB (signed-in users only — guests don't have a User row)
          if (!isGuest) {
            try {
              const conversation = await prisma.conversation.create({
                data: {
                  userId: userId,
                  messages: {
                    create: userMsgs.map(m => ({
                      role: m.role,
                      content: m.content,
                    })),
                  },
                },
              });
              console.log(
                `[Memory] Saved conversation ${conversation.id} (${userMsgs.length} messages)`
              );
            } catch (convErr) {
              console.error(
                "[Memory] Conversation save failed:",
                (convErr as Error).message
              );
            }
          }

          // 2. Extract and save memories (runs for BOTH guests and signed-in users)
          // Guests use their guest_<id> as userId in MemoryFact.
          // createdAt timestamp on MemoryFact enables future 30-day cleanup for guests.
          // When a guest signs up, their facts can be migrated by updating userId.
          await extractAndSaveMemories(
            openai,
            prisma,
            userId,
            userMsgs,
            conversationSummary
          );
          console.log(`[Memory] Extraction complete for ${isGuest ? 'guest' : 'user'} ${userId}`);
        }
      } catch (err) {
        console.error(
          "[Memory] Post-disconnect save failed:",
          (err as Error).message
        );
      }
    }
  });

  ws.on("error", (err: Error) => {
    console.error("[WS] WebSocket error:", err);
    clientDisconnected = true;
    clearInterval(keepAliveInterval);
    clearInterval(messageCountResetInterval);
    clearInterval(llmRateLimitInterval);
    if (usageCheckInterval) clearInterval(usageCheckInterval);
    if (timeCheckInterval) clearInterval(timeCheckInterval);
    if (silenceTimer) clearTimeout(silenceTimer);
    if (goodbyeTimeout) clearTimeout(goodbyeTimeout);
    if (sttStreamer) sttStreamer.destroy();
  });
});

// --- START THE SERVER ---
server.listen(PORT, () => {
  console.log(`🚀 Voice pipeline server listening on :${PORT}`);
});

═══════════════════════════════════════════════════════════════════════════════
FILE: packages/server/src/AzureTTSStreamer.ts
═══════════════════════════════════════════════════════════════════════════════
import {
  SpeechSynthesizer,
  SpeechConfig,
  AudioConfig,
  ResultReason,
  CancellationDetails,
  SpeechSynthesisOutputFormat,
  PushAudioOutputStreamCallback,
  PushAudioOutputStream,
} from "microsoft-cognitiveservices-speech-sdk";
import { EventEmitter } from "events";
import { PassThrough } from "stream";

const AZURE_SPEECH_KEY = process.env.AZURE_SPEECH_KEY!;
const AZURE_SPEECH_REGION = process.env.AZURE_SPEECH_REGION!;

export interface AzureVoiceConfig {
  voiceName: string;
  style?: string;
  rate?: string;
  pitch?: string;
  temperature?: string;   // "0.0" to "1.0" — higher = more expressive
  topP?: string;           // should match temperature for best results
}

class NodePushAudioStream extends PushAudioOutputStreamCallback {
  constructor(private readonly stream: PassThrough) {
    super();
  }

  write(data: ArrayBuffer): number {
    const buffer = Buffer.from(data);
    this.stream.write(buffer);
    return buffer.length;
  }

  close(): void {
    this.stream.end();
  }
}

function escapeXml(text: string): string {
  return text
    .replace(/&/g, "&amp;")
    .replace(/</g, "&lt;")
    .replace(/>/g, "&gt;")
    .replace(/"/g, "&quot;")
    .replace(/'/g, "&apos;");
}

export class AzureTTSStreamer extends EventEmitter {
  private synthesizer: SpeechSynthesizer;
  private audioStream: PassThrough;
  private voiceConfig: AzureVoiceConfig;

  constructor(config?: AzureVoiceConfig) {
    super();
    this.voiceConfig = config || {
      voiceName: process.env.AZURE_TTS_VOICE || "en-US-AshleyNeural",
      rate: process.env.AZURE_TTS_RATE || "+25.00%",
      pitch: process.env.AZURE_TTS_PITCH || "+25.00%",
    };

    const speechConfig = SpeechConfig.fromSubscription(
      AZURE_SPEECH_KEY,
      AZURE_SPEECH_REGION
    );
    speechConfig.speechSynthesisOutputFormat =
      SpeechSynthesisOutputFormat.Raw16Khz16BitMonoPcm;

    this.audioStream = new PassThrough();
    const pushStream = PushAudioOutputStream.create(
      new NodePushAudioStream(this.audioStream)
    );
    const audioConfig = AudioConfig.fromStreamOutput(pushStream);
    this.synthesizer = new SpeechSynthesizer(speechConfig, audioConfig);

    this.audioStream.on("data", (chunk) => this.emit("audio_chunk", chunk));
    this.audioStream.on("end", () => this.emit("tts_complete"));
  }

  public stop() {
    try {
      this.synthesizer.close();
      this.audioStream.destroy();
      console.log("[AzureTTS] Stopped synthesis.");
    } catch (e) {
      console.error("[AzureTTS] Error stopping synthesizer:", e);
    }
  }

  private buildSsml(text: string): string {
    const escaped = escapeXml(text);
    const { voiceName, style, rate, pitch, temperature, topP } = this.voiceConfig;

    // Build from inside out: text → prosody → express-as
    let innerContent = escaped;

    // If rate/pitch are set, wrap in prosody (skip for DragonHD voices — they handle it contextually)
    if (rate || pitch) {
      const rateAttr = rate ? ` rate="${rate}"` : "";
      const pitchAttr = pitch ? ` pitch="${pitch}"` : "";
      innerContent = `<prosody${rateAttr}${pitchAttr}>${innerContent}</prosody>`;
    }

    // If a speaking style is requested, wrap in express-as
    if (style) {
      innerContent = `<mstts:express-as style="${style}">${innerContent}</mstts:express-as>`;
    }

    // Build parameters string for DragonHD Omni voices (temperature, top_p)
    const params: string[] = [];
    if (temperature) params.push(`temperature=${temperature}`);
    if (topP) params.push(`top_p=${topP}`);
    const paramsAttr = params.length > 0 ? ` parameters="${params.join(";")}"` : "";

    return `<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xmlns:mstts="https://www.w3.org/2001/mstts" xml:lang="en-US"><voice name="${voiceName}"${paramsAttr}>${innerContent}</voice></speak>`;
  }

  public synthesize(text: string) {
    const ssml = this.buildSsml(text);
    this.synthesizer.speakSsmlAsync(
      ssml,
      (result) => {
        if (result.reason !== ResultReason.SynthesizingAudioCompleted) {
          const errorDetails = CancellationDetails.fromResult(result);
          console.error(
            "[AzureTTS] ❌ Synthesis canceled:",
            errorDetails.reason,
            errorDetails.errorDetails
          );
          this.emit("error", errorDetails.errorDetails);
        }
        this.synthesizer.close();
      },
      (err) => {
        console.error("[AzureTTS] ❌ Synthesis error:", err);
        this.emit("error", err);
        this.synthesizer.close();
      }
    );
  }
}

═══════════════════════════════════════════════════════════════════════════════
FILE: packages/web/src/hooks/useKiraSocket.ts
═══════════════════════════════════════════════════════════════════════════════
"use client";
import { useState, useEffect, useRef, useCallback } from "react";
import { useSceneDetection } from "./useSceneDetection";

// --- Persistent debug logger (survives page reloads via sessionStorage) ---
// Silent in production unless ?debug is in the URL
const isDebug = typeof window !== 'undefined' && (process.env.NODE_ENV !== 'production' || window.location.search.includes('debug'));
export function debugLog(...args: any[]) {
  if (!isDebug) return;
  const msg = `[${new Date().toISOString().slice(11, 23)}] ${args.map(a => typeof a === 'string' ? a : JSON.stringify(a)).join(' ')}`;
  console.log(...args);
  try {
    const logs = JSON.parse(sessionStorage.getItem('kira-debug') || '[]');
    logs.push(msg);
    if (logs.length > 200) logs.splice(0, logs.length - 200);
    sessionStorage.setItem('kira-debug', JSON.stringify(logs));
  } catch {}
}

// Define the states
type SocketState = "idle" | "connecting" | "connected" | "closing" | "closed";
export type KiraState = "listening" | "thinking" | "speaking";

// ─── Window-level singleton — survives React remounts AND module re-evaluation ───
// Module-level vars can be re-created if Next.js re-evaluates the module during
// code splitting or dynamic imports. window is truly global and survives everything.
interface ConnectionStore {
  ws: WebSocket | null;
  socketState: SocketState;
  audioContext: AudioContext | null;
  playbackContext: AudioContext | null;
  audioStream: MediaStream | null;
  audioWorkletNode: AudioWorkletNode | null;
  audioSource: MediaStreamAudioSourceNode | null;
  playbackGain: GainNode | null;
  playbackAnalyser: AnalyserNode | null;
  isServerReady: boolean;
  conversationActive: boolean;
  reconnectAttempts: number;
}

function getConnectionStore(): ConnectionStore | null {
  if (typeof window === "undefined") return null;
  if (!(window as any).__kiraConnectionStore) {
    (window as any).__kiraConnectionStore = {
      ws: null,
      socketState: "idle",
      audioContext: null,
      playbackContext: null,
      audioStream: null,
      audioWorkletNode: null,
      audioSource: null,
      playbackGain: null,
      playbackAnalyser: null,
      isServerReady: false,
      conversationActive: false,
      reconnectAttempts: 0,
    } as ConnectionStore;
  }
  return (window as any).__kiraConnectionStore as ConnectionStore;
}

// Adaptive EOU: short utterances get snappy response, long utterances get patience for multi-part questions
const EOU_TIMEOUT_MIN = 500;   // 500ms silence for short utterances ("yes", "no", "hi")
const EOU_TIMEOUT_MAX = 1500;  // 1500ms silence for long multi-part questions
const LONG_UTTERANCE_FRAMES = 800; // ~2s of speech = "long utterance" (each frame ≈ 2.67ms at 48kHz)
const MIN_SPEECH_FRAMES_FOR_EOU = 200; // Must have ~200 speech frames (~1-2s real speech) to prevent noise-triggered EOUs
const VAD_STABILITY_FRAMES = 5; // Need 5 consecutive speech frames before considering "speaking"

export const useKiraSocket = (getTokenFn: (() => Promise<string | null>) | null, guestId: string, voicePreference: string = "anime") => {
  // ─── Restore state from singleton if a live connection exists ───
  const [socketState, setSocketState] = useState<SocketState>(() => {
    // Use the stored socketState directly — it's authoritative
    if (getConnectionStore()!.socketState === "connected" || getConnectionStore()!.socketState === "connecting") {
      debugLog("[Hook] Restoring socketState →", getConnectionStore()!.socketState, "from singleton. ws:", !!getConnectionStore()!.ws);
      return getConnectionStore()!.socketState;
    }
    return "idle";
  });
  const [kiraState, setKiraState] = useState<KiraState>("listening");
  const kiraStateRef = useRef<KiraState>("listening"); // Ref to track state in callbacks

  // Log hook mount/unmount — DO NOT close WS on unmount (singleton survives remount)
  useEffect(() => {
    debugLog("[Hook] useKiraSocket MOUNTED. Singleton ws:", !!getConnectionStore()!.ws, 
      "readyState:", getConnectionStore()!.ws?.readyState,
      "socketState restored as:", getConnectionStore()!.ws?.readyState === WebSocket.OPEN ? "connected" : "idle");
    return () => {
      debugLog("[Hook] useKiraSocket UNMOUNTING — preserving singleton");
      // Sync current refs TO singleton only if they're alive.
      // NEVER overwrite singleton with null — that destroys the connection for the next mount.
      // The ONLY place that should null out singleton is disconnect() (explicit End Call).
      if (ws.current && ws.current.readyState === WebSocket.OPEN) {
        getConnectionStore()!.ws = ws.current;
        debugLog("[Hook] WebSocket preserved in singleton (readyState:", ws.current.readyState, ")");
      }
      if (audioContext.current) getConnectionStore()!.audioContext = audioContext.current;
      if (playbackContext.current) getConnectionStore()!.playbackContext = playbackContext.current;
      if (audioStream.current) getConnectionStore()!.audioStream = audioStream.current;
      if (audioWorkletNode.current) getConnectionStore()!.audioWorkletNode = audioWorkletNode.current;
      if (audioSource.current) getConnectionStore()!.audioSource = audioSource.current;
      if (playbackGain.current) getConnectionStore()!.playbackGain = playbackGain.current;
      if (playbackAnalyser.current) getConnectionStore()!.playbackAnalyser = playbackAnalyser.current;
      // Always sync these non-nullable flags
      getConnectionStore()!.isServerReady = isServerReady.current;
      getConnectionStore()!.conversationActive = conversationActive.current;
      getConnectionStore()!.reconnectAttempts = reconnectAttempts.current;
    };
  }, []);

  // ─── Handler refs: these always point to the latest closure ───
  // The actual WS handlers call through these refs, so remounts get fresh state setters.
  const onMessageRef = useRef<((event: MessageEvent) => void) | null>(null);
  const onCloseRef = useRef<((event: CloseEvent) => void) | null>(null);
  const onErrorRef = useRef<((event: Event) => void) | null>(null);

  // ─── Visual-ready gating: don't send start_stream until Live2D is loaded (or timeout) ───
  const visualReadyRef = useRef(false);
  const wsOpenRef = useRef(false); // true once ws.onopen fires

  // Sync ref with state
  useEffect(() => {
    kiraStateRef.current = kiraState;
  }, [kiraState]);

  const [micVolume, setMicVolume] = useState(0);
  const [transcript, setTranscript] = useState<{ role: "user" | "ai"; text: string } | null>(null);

  const [currentExpression, setCurrentExpression] = useState<string>("neutral");
  const [activeAccessories, setActiveAccessories] = useState<string[]>([]);
  const [error, setError] = useState<string | null>(null);
  const [isAudioBlocked, setIsAudioBlocked] = useState(false);
  const [isMuted, setIsMuted] = useState(false);
  const [isScreenSharing, setIsScreenSharing] = useState(false);
  const [isCameraActive, setIsCameraActive] = useState(false);
  const [facingMode, setFacingMode] = useState<"environment" | "user">("environment");
  const [isPro, setIsPro] = useState(false);
  const isProRef = useRef(false); // Ref mirror of isPro for use in onclose callback
  const [remainingSeconds, setRemainingSeconds] = useState<number | null>(null);
  const [isAudioPlaying, setIsAudioPlaying] = useState(false);
  const audioPlayingTimeout = useRef<ReturnType<typeof setTimeout> | null>(null);

  // --- Visualizer: read AnalyserNode while audio is playing ---
  useEffect(() => {
    if (!isAudioPlaying) {
      setPlayerVolume(0);
      return;
    }

    let frame: number;
    const tick = () => {
      if (playbackAnalyser.current) {
        const data = new Uint8Array(playbackAnalyser.current.frequencyBinCount);
        playbackAnalyser.current.getByteFrequencyData(data);
        let sum = 0;
        for (let i = 0; i < data.length; i++) sum += data[i];
        const avg = sum / data.length / 255; // normalize 0-1
        setPlayerVolume(avg);
      }
      frame = requestAnimationFrame(tick);
    };
    tick();

    return () => cancelAnimationFrame(frame);
  }, [isAudioPlaying]);
  const ws = useRef<WebSocket | null>(getConnectionStore()!.ws);
  const isServerReady = useRef(getConnectionStore()!.isServerReady); // Gate for sending audio

  // --- Audio Pipeline Refs (restore from singleton if present) ---
  const audioContext = useRef<AudioContext | null>(getConnectionStore()!.audioContext);
  const audioWorkletNode = useRef<AudioWorkletNode | null>(getConnectionStore()!.audioWorkletNode);
  const audioSource = useRef<MediaStreamAudioSourceNode | null>(getConnectionStore()!.audioSource);
  const audioStream = useRef<MediaStream | null>(getConnectionStore()!.audioStream);

  // --- Screen Share Refs ---
  const screenStream = useRef<MediaStream | null>(null);
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);
  const isScreenSharingRef = useRef(false); // Ref to track screen share state in callbacks

  // --- Camera Refs ---
  const cameraStreamRef = useRef<MediaStream | null>(null);
  const cameraVideoRef = useRef<HTMLVideoElement | null>(null);
  const cameraIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const isCameraActiveRef = useRef(false);

  // --- Scene Detection ---
  const lastSceneUpdateSent = useRef(0);
  const SCENE_UPDATE_COOLDOWN = 30000; // Don't send scene updates more than once per 30 seconds

  const handleSceneChange = useCallback((frames: string[]) => {
    const now = Date.now();
    if (
      now - lastSceneUpdateSent.current > SCENE_UPDATE_COOLDOWN &&
      ws.current?.readyState === WebSocket.OPEN &&
      isScreenSharingRef.current &&
      kiraStateRef.current === "listening"
    ) {
      lastSceneUpdateSent.current = now;
      ws.current.send(JSON.stringify({
        type: "scene_update",
        images: frames,
      }));
    }
  }, []);

  const sceneBuffer = useSceneDetection({
    videoRef,
    enabled: isScreenSharing,
    checkInterval: 2000,
    threshold: 15,
    onSceneChange: handleSceneChange,
  });
  const sceneBufferRef = useRef<string[]>([]);

  // Sync sceneBuffer to ref for access in callbacks
  useEffect(() => {
    sceneBufferRef.current = sceneBuffer;
  }, [sceneBuffer]);

  // --- Audio Playback Refs ---
  const audioQueue = useRef<ArrayBuffer[]>([]);
  const nextStartTime = useRef(0); // Track where the next chunk should start
  const isProcessingQueue = useRef(false); // Lock for the processing loop
  const scheduledSources = useRef<AudioBufferSourceNode[]>([]); // Track all scheduled sources
  const ttsChunksDone = useRef(true); // Whether server has finished sending audio for this turn

  const playbackContext = useRef<AudioContext | null>(getConnectionStore()!.playbackContext);
  const playbackSource = useRef<AudioBufferSourceNode | null>(null);
  const playbackGain = useRef<GainNode | null>(getConnectionStore()!.playbackGain);
  const playbackAnalyser = useRef<AnalyserNode | null>(getConnectionStore()!.playbackAnalyser);
  const playerVolumeFrame = useRef<number>(0);
  const [playerVolume, setPlayerVolume] = useState(0);

  // --- "Ramble Bot" EOU Timer ---
  const eouTimer = useRef<NodeJS.Timeout | null>(null);
  const maxUtteranceTimer = useRef<NodeJS.Timeout | null>(null);
  const speechFrameCount = useRef(0); // Track consecutive speech frames for VAD stability
  const totalSpeechFrames = useRef(0); // Total speech frames in current utterance (reset on EOU)
  const hasSpoken = useRef(false); // Whether user has spoken enough to trigger EOU

  // --- Latency Tracking ---
  const eouSentAt = useRef(0);
  const firstAudioLogged = useRef(false);

  // --- Vision: Snapshot Cooldown ---
  const lastSnapshotTime = useRef(0);
  const SNAPSHOT_COOLDOWN_MS = 5000; // One snapshot per 5 seconds max
  const periodicCaptureTimer = useRef<ReturnType<typeof setInterval> | null>(null);

  // --- WebSocket Auto-Reconnect ---
  const reconnectAttempts = useRef(getConnectionStore()!.reconnectAttempts);
  const MAX_RECONNECT_ATTEMPTS = 5;
  const conversationActive = useRef(getConnectionStore()!.conversationActive); // True once start_stream sent — prevents reconnect loops

  /**
   * Calculates adaptive EOU timeout based on how long the user has been speaking.
   * Short utterances ("yes") → fast 500ms cutoff for snappy responses.
   * Long utterances (multi-part questions) → patient 1500ms to allow thinking pauses.
   */
  const getAdaptiveEOUTimeout = () => {
    const ratio = Math.min(totalSpeechFrames.current / LONG_UTTERANCE_FRAMES, 1.0);
    return Math.round(EOU_TIMEOUT_MIN + (EOU_TIMEOUT_MAX - EOU_TIMEOUT_MIN) * ratio);
  };

  /**
   * Stops current audio playback and clears the queue.
   */
  const stopAudioPlayback = useCallback(() => {
    // 1. Clear the queue so no new chunks are scheduled
    audioQueue.current = [];
    
    // 2. Stop ALL scheduled sources
    scheduledSources.current.forEach((source) => {
      try {
        source.stop();
      } catch (e) {
        // Ignore errors if already stopped
      }
    });
    scheduledSources.current = []; // Clear the list
    playbackSource.current = null;

    // 3. Reset scheduling time
    if (playbackContext.current) {
        nextStartTime.current = playbackContext.current.currentTime;
    } else {
        nextStartTime.current = 0;
    }

    // 4. Reset for next turn
    ttsChunksDone.current = true;

    // 5. Audio is no longer playing
    if (audioPlayingTimeout.current) {
      clearTimeout(audioPlayingTimeout.current);
      audioPlayingTimeout.current = null;
    }
    setIsAudioPlaying(false);
  }, []);

  /**
   * Processes the audio queue and schedules chunks to play back-to-back.
   * This eliminates gaps/pops caused by waiting for onended events.
   */
  const processAudioQueue = useCallback(async () => {
    if (isProcessingQueue.current) return;
    isProcessingQueue.current = true;

    // Ensure the playback audio context is running (and is 16kHz for Azure's output)
    if (
      !playbackContext.current ||
      playbackContext.current.state === "closed"
    ) {
      playbackContext.current = new AudioContext({ sampleRate: 16000 });
      // Reset persistent audio chain when context is recreated
      playbackGain.current = null;
      playbackAnalyser.current = null;
    }
    if (playbackContext.current.state === "suspended") {
      await playbackContext.current.resume();
    }

    // Build persistent audio chain once: Source → GainNode → AnalyserNode → Destination
    if (!playbackGain.current) {
      playbackGain.current = playbackContext.current.createGain();
      playbackAnalyser.current = playbackContext.current.createAnalyser();
      playbackAnalyser.current.fftSize = 256;
      playbackAnalyser.current.smoothingTimeConstant = 0.1; // Very low — reacts instantly to silence between syllables
      playbackAnalyser.current.minDecibels = -90;
      playbackAnalyser.current.maxDecibels = -10;
      playbackGain.current.connect(playbackAnalyser.current);
      playbackAnalyser.current.connect(playbackContext.current.destination);
    }

    while (audioQueue.current.length > 0) {
      const buffer = audioQueue.current.shift();
      if (!buffer) continue;

      try {
        // 1. Decode the raw PCM buffer
        const wavBuffer = createWavHeader(buffer, 16000, 16);
        const audioBuffer = await playbackContext.current.decodeAudioData(
          wavBuffer
        );

        // 2. Create a source node and route through persistent gain
        const source = playbackContext.current.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(playbackGain.current!);

        // 3. Schedule playback
        const currentTime = playbackContext.current.currentTime;
        // If nextStartTime is in the past (gap in stream), reset to now + small buffer
        if (nextStartTime.current < currentTime) {
          nextStartTime.current = currentTime + 0.02;
        }

        source.start(nextStartTime.current);
        nextStartTime.current += audioBuffer.duration;

        // Signal that audio is actively playing
        if (audioPlayingTimeout.current) {
          clearTimeout(audioPlayingTimeout.current);
          audioPlayingTimeout.current = null;
        }
        setIsAudioPlaying(true);

        // Keep track of the source so we can stop it later
        scheduledSources.current.push(source);
        source.onended = () => {
          // Remove from list when done to keep memory clean
          scheduledSources.current = scheduledSources.current.filter(s => s !== source);

          // When last source finishes and no more chunks coming, debounce isAudioPlaying off
          if (scheduledSources.current.length === 0 && audioQueue.current.length === 0) {
            audioPlayingTimeout.current = setTimeout(() => {
              // Double-check nothing new arrived in the gap
              if (scheduledSources.current.length === 0 && audioQueue.current.length === 0) {
                setIsAudioPlaying(false);
              }
            }, 300);
          }
        };

        // Keep track of the last source if we need to stop it manually later
        playbackSource.current = source;

      } catch (e) {
        console.error("[AudioPlayer] Error decoding or playing audio:", e);
      }
    }

    isProcessingQueue.current = false;
  }, []);

  const stopAudioPipeline = useCallback(() => {
    if (eouTimer.current) clearTimeout(eouTimer.current);

    audioWorkletNode.current?.port.close();
    audioSource.current?.disconnect();
    audioStream.current?.getTracks().forEach((track) => track.stop());
    screenStream.current?.getTracks().forEach((track) => track.stop()); // Stop screen share
    // Stop camera if active
    if (cameraStreamRef.current) {
      cameraStreamRef.current.getTracks().forEach(track => track.stop());
      cameraStreamRef.current = null;
    }
    if (cameraIntervalRef.current) {
      clearInterval(cameraIntervalRef.current);
      cameraIntervalRef.current = null;
    }
    if (cameraVideoRef.current) {
      cameraVideoRef.current.pause();
      cameraVideoRef.current.srcObject = null;
      cameraVideoRef.current = null;
    }
    setIsCameraActive(false);
    isCameraActiveRef.current = false;
    audioContext.current?.close().catch(console.error);
    playbackContext.current?.close().catch(console.error);

    audioWorkletNode.current = null;
    audioSource.current = null;
    audioStream.current = null;
    audioContext.current = null;
    playbackContext.current = null;
    playbackGain.current = null;

    // ─── Clear audio from singleton ───
    getConnectionStore()!.audioContext = null;
    getConnectionStore()!.playbackContext = null;
    getConnectionStore()!.audioStream = null;
    getConnectionStore()!.audioWorkletNode = null;
    getConnectionStore()!.audioSource = null;
    getConnectionStore()!.playbackGain = null;
    getConnectionStore()!.playbackAnalyser = null;

    debugLog("[Audio] 🛑 Audio pipeline stopped.");
  }, []);

  /**
   * Initializes audio contexts and requests mic permission.
   * Must be called from a user gesture.
   */
  const initializeAudio = useCallback(async () => {
    try {
      debugLog("[Audio] Initializing audio contexts...");
      
      // 1. Create/Resume AudioContext
      if (!audioContext.current || audioContext.current.state === "closed") {
        audioContext.current = new AudioContext();
        debugLog(`[Audio] Created capture AudioContext (sampleRate: ${audioContext.current.sampleRate})`);
      }
      if (audioContext.current.state === "suspended") {
        debugLog("[Audio] Capture AudioContext is suspended, resuming...");
        await audioContext.current.resume();
      }
      debugLog(`[Audio] Capture AudioContext state: ${audioContext.current.state}`);

      // 2. Create/Resume PlaybackContext
      if (!playbackContext.current || playbackContext.current.state === "closed") {
        playbackContext.current = new AudioContext({ sampleRate: 16000 });
        debugLog("[Audio] Created playback AudioContext (sampleRate: 16000)");
      }
      if (playbackContext.current.state === "suspended") {
        debugLog("[Audio] Playback AudioContext is suspended, resuming...");
        await playbackContext.current.resume();
      }
      debugLog(`[Audio] Playback AudioContext state: ${playbackContext.current.state}`);

      // 3. Request Mic Permission (if not already)
      if (!audioStream.current) {
        debugLog("[Audio] Requesting mic permission...");
        audioStream.current = await navigator.mediaDevices.getUserMedia({
          audio: {
            channelCount: 1,
            echoCancellation: true,
            autoGainControl: true,
            noiseSuppression: true,
          },
        });
        debugLog(`[Audio] Mic permission granted. Tracks: ${audioStream.current.getAudioTracks().length}, active: ${audioStream.current.active}`);
      } else {
        debugLog(`[Audio] Mic stream already exists. active: ${audioStream.current.active}`);
      }

      setIsAudioBlocked(false);
      return true;
    } catch (err) {
      debugLog("[Audio] ❌ Failed to initialize audio:", err);
      setIsAudioBlocked(true);
      return false;
    }
  }, []);

  /**
   * Toggles microphone mute state
   */
  const toggleMute = useCallback(() => {
    if (audioStream.current) {
      const audioTracks = audioStream.current.getAudioTracks();
      audioTracks.forEach(track => {
        track.enabled = !track.enabled;
      });
      setIsMuted(prev => !prev);
    }
  }, []);

  /**
   * Starts screen sharing
   */
  const startScreenShare = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getDisplayMedia({
        video: {
          width: { ideal: 1280 },
          height: { ideal: 720 },
          frameRate: { ideal: 10 } // Low framerate is fine for snapshots
        },
        audio: false
      });

      screenStream.current = stream;
      setIsScreenSharing(true);
      isScreenSharingRef.current = true;

      // Setup hidden video element for capturing frames
      if (!videoRef.current) {
        videoRef.current = document.createElement("video");
        videoRef.current.autoplay = true;
        videoRef.current.muted = true;
        videoRef.current.playsInline = true;
        // Ensure it's in the DOM so it processes frames
        videoRef.current.style.position = "absolute";
        videoRef.current.style.top = "-9999px";
        videoRef.current.style.left = "-9999px";
        videoRef.current.style.width = "1px";
        videoRef.current.style.height = "1px";
        videoRef.current.style.opacity = "0";
        videoRef.current.style.pointerEvents = "none";
        document.body.appendChild(videoRef.current);
      }
      videoRef.current.srcObject = stream;
      await videoRef.current.play();

      // Handle user stopping share via browser UI
      stream.getVideoTracks()[0].onended = () => {
        stopScreenShare();
      };

      debugLog("[Vision] Screen share started");
      
      // Send an initial snapshot immediately to establish context
      setTimeout(() => {
          const snapshot = captureScreenSnapshot();
          if (snapshot && ws.current?.readyState === WebSocket.OPEN) {
              debugLog("[Vision] Sending initial snapshot...");
              // Send buffer + current frame
              const payload = {
                  type: "image",
                  images: [...sceneBufferRef.current, snapshot]
              };
              ws.current.send(JSON.stringify(payload));
          } else {
              console.warn("[Vision] Failed to capture initial snapshot.");
          }
      }, 1000);

      // Start periodic captures every 15 seconds so the server always has fresh images
      if (periodicCaptureTimer.current) clearInterval(periodicCaptureTimer.current);
      periodicCaptureTimer.current = setInterval(() => {
        if (!isScreenSharingRef.current || !ws.current || ws.current.readyState !== WebSocket.OPEN) {
          if (periodicCaptureTimer.current) clearInterval(periodicCaptureTimer.current);
          periodicCaptureTimer.current = null;
          return;
        }
        const snapshot = captureScreenSnapshot();
        if (snapshot) {
          ws.current.send(JSON.stringify({
            type: "image",
            images: [snapshot],
          }));
          debugLog("[Vision] Periodic snapshot sent.");
        }
      }, 15000);

    } catch (err) {
      console.error("[Vision] Failed to start screen share:", err);
      setIsScreenSharing(false);
    }
  }, []);

  /**
   * Stops screen sharing
   */
  const stopScreenShare = useCallback(() => {
    if (periodicCaptureTimer.current) {
      clearInterval(periodicCaptureTimer.current);
      periodicCaptureTimer.current = null;
    }
    if (screenStream.current) {
      screenStream.current.getTracks().forEach(track => track.stop());
      screenStream.current = null;
    }
    if (videoRef.current) {
      videoRef.current.srcObject = null;
      // Remove from DOM
      if (videoRef.current.parentNode) {
          videoRef.current.parentNode.removeChild(videoRef.current);
      }
      videoRef.current = null; // Reset ref
    }
    setIsScreenSharing(false);
    isScreenSharingRef.current = false;

    // Tell server to stop vision reactions
    if (ws.current && ws.current.readyState === WebSocket.OPEN) {
      ws.current.send(JSON.stringify({ type: "vision_stop" }));
    }

    debugLog("[Vision] Screen share stopped");
  }, []);

  /**
   * Captures a frame from the camera and sends it over WebSocket
   */
  const captureAndSendCameraFrame = useCallback(() => {
    const video = cameraVideoRef.current;
    if (!video || video.readyState < 2) return;

    const canvas = document.createElement("canvas");
    // Downscale to max 512px on longest side (same as screen share)
    const MAX_DIM = 512;
    const scale = Math.min(MAX_DIM / video.videoWidth, MAX_DIM / video.videoHeight, 1);
    canvas.width = Math.round(video.videoWidth * scale);
    canvas.height = Math.round(video.videoHeight * scale);

    const ctx = canvas.getContext("2d");
    if (!ctx) return;
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    const jpeg = canvas.toDataURL("image/jpeg", 0.5);
    const base64 = jpeg.split(",")[1];

    if (ws.current && ws.current.readyState === WebSocket.OPEN) {
      ws.current.send(JSON.stringify({
        type: "image",
        images: [base64],
      }));
    }
  }, []);

  /**
   * Starts camera capture (mobile vision)
   */
  const startCamera = useCallback(async (mode?: "environment" | "user") => {
    const useFacing = mode || facingMode;
    debugLog("[Camera] startCamera called, facingMode:", useFacing);

    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      console.error("[Camera] getUserMedia not available — requires HTTPS");
      setError("Camera not available — HTTPS required");
      return;
    }

    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          facingMode: useFacing,
          width: { ideal: 1280 },
          height: { ideal: 720 },
        },
        audio: false,
      });
      debugLog("[Camera] Got stream:", stream.getVideoTracks().length, "video tracks");

      cameraStreamRef.current = stream;

      const video = document.createElement("video");
      video.srcObject = stream;
      video.setAttribute("playsinline", "true");
      video.muted = true;
      await video.play();
      cameraVideoRef.current = video;

      setIsCameraActive(true);
      isCameraActiveRef.current = true;
      debugLog("[Camera] Camera started, facing:", useFacing);

      // Send initial snapshot
      setTimeout(() => {
        captureAndSendCameraFrame();
        debugLog("[Camera] Initial snapshot sent.");
      }, 500);

      // Start periodic captures — same 15s interval as screen share
      if (cameraIntervalRef.current) clearInterval(cameraIntervalRef.current);
      cameraIntervalRef.current = setInterval(() => {
        if (!isCameraActiveRef.current || !ws.current || ws.current.readyState !== WebSocket.OPEN) {
          if (cameraIntervalRef.current) clearInterval(cameraIntervalRef.current);
          cameraIntervalRef.current = null;
          return;
        }
        captureAndSendCameraFrame();
        debugLog("[Camera] Periodic snapshot sent.");
      }, 15000);

    } catch (err) {
      console.error("[Camera] Failed to start:", err);
      const msg = (err as Error).message || "Unknown camera error";
      if (msg.includes("NotAllowedError") || msg.includes("Permission")) {
        setError("Camera permission denied");
      } else {
        setError("Camera failed: " + msg);
      }
    }
  }, [facingMode, captureAndSendCameraFrame]);

  /**
   * Stops camera capture
   */
  const stopCamera = useCallback(() => {
    if (cameraIntervalRef.current) {
      clearInterval(cameraIntervalRef.current);
      cameraIntervalRef.current = null;
    }
    if (cameraStreamRef.current) {
      cameraStreamRef.current.getTracks().forEach(track => track.stop());
      cameraStreamRef.current = null;
    }
    if (cameraVideoRef.current) {
      cameraVideoRef.current.pause();
      cameraVideoRef.current.srcObject = null;
      cameraVideoRef.current = null;
    }
    setIsCameraActive(false);
    isCameraActiveRef.current = false;
    debugLog("[Camera] Camera stopped.");
  }, []);

  /**
   * Flips from front to rear camera (or vice versa)
   */
  const flipCamera = useCallback(() => {
    const newMode = facingMode === "environment" ? "user" : "environment";
    setFacingMode(newMode);
    if (isCameraActiveRef.current) {
      stopCamera();
      setTimeout(() => startCamera(newMode), 300);
    }
  }, [facingMode, stopCamera, startCamera]);

  const captureScreenSnapshot = useCallback(() => {
    if (!videoRef.current || !screenStream.current) {
        console.warn("[Vision] Capture failed: No video or stream.");
        return null;
    }

    if (!canvasRef.current) {
      canvasRef.current = document.createElement("canvas");
    }

    const video = videoRef.current;
    const canvas = canvasRef.current;
    
    if (video.videoWidth === 0 || video.videoHeight === 0) {
        console.warn("[Vision] Capture failed: Video dimensions are 0.");
        return null;
    }

    // Downscale to max 512px on longest side (matches GPT-4o "low" detail)
    const MAX_DIM = 512;
    const scale = Math.min(MAX_DIM / video.videoWidth, MAX_DIM / video.videoHeight, 1);
    canvas.width = Math.round(video.videoWidth * scale);
    canvas.height = Math.round(video.videoHeight * scale);

    const ctx = canvas.getContext("2d");
    if (!ctx) return null;

    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    
    // Lower quality since images are already small
    return canvas.toDataURL("image/jpeg", 0.5);
  }, []);

  /**
   * Initializes and starts the audio capture pipeline (Mic -> Worklet -> WebSocket)
   */
  const startAudioPipeline = useCallback(async () => {
    if (!ws.current || ws.current.readyState !== WebSocket.OPEN) {
      debugLog("[Audio] ❌ WebSocket not open, cannot start pipeline.");
      return;
    }

    try {
      // Ensure audio is initialized (should be done by connect/initializeAudio already)
      if (!audioStream.current) {
         const success = await initializeAudio();
         if (!success) throw new Error("Audio initialization failed");
      }

      // 2. Load AudioWorklet module
      if (!audioContext.current) throw new Error("AudioContext is null");
      
      debugLog("[Audio] Loading AudioWorklet module...");
      try {
        // Use a robust path for the worklet
        const workletUrl = "/worklets/AudioWorkletProcessor.js";
        // Check if module is already added (not directly possible, but addModule is idempotent-ish or throws)
        // We'll just try adding it.
        await audioContext.current.audioWorklet.addModule(workletUrl);
        debugLog("[Audio] AudioWorklet module loaded.");
      } catch (e) {
        // Ignore error if module already added (DOMException)
        debugLog("[Audio] Worklet might already be loaded:", e);
      }

      // 3. Create the Worklet Node
      if (!audioWorkletNode.current) {
          audioWorkletNode.current = new AudioWorkletNode(
            audioContext.current,
            "audio-worklet-processor",
            {
              processorOptions: {
                targetSampleRate: 16000,
              },
            }
          );
          
          audioWorkletNode.current.onprocessorerror = (err) => {
            console.error("[Audio] Worklet processor error:", err);
          };

          // 5. Connect the Worklet to the main app (this hook)
          audioWorkletNode.current.port.onmessage = (event) => {
            // ... (Existing message handler logic) ...
            // Handle Debug Messages from Worklet
            if (event.data && event.data.type === "debug") {
               debugLog("[AudioWorklet]", event.data.message);
               return;
            }
    
            // We received a 16-bit PCM buffer from the worklet
            const pcmBuffer = event.data as ArrayBuffer;
    
            // Safety: skip empty/detached buffers
            if (!pcmBuffer || pcmBuffer.byteLength === 0) return;

            // Calculate Mic Volume (RMS)
            const pcmData = new Int16Array(pcmBuffer);
            if (pcmData.length === 0) return;

            let sum = 0;
            for (let i = 0; i < pcmData.length; i++) {
              sum += pcmData[i] * pcmData[i];
            }
            const rms = Math.sqrt(sum / pcmData.length);
            // Normalize (16-bit max is 32768)
            // Multiply by a factor to make it more sensitive visually
            const rawVolume = Math.min(1, (rms / 32768) * 5);
            
            setMicVolume((prev) => {
                const smoothingFactor = 0.3; 
                return prev * (1 - smoothingFactor) + rawVolume * smoothingFactor;
            });
    
            if (
              ws.current?.readyState === WebSocket.OPEN &&
              isServerReady.current
            ) {
              // Only send audio when listening — no interrupt feature
              if (kiraStateRef.current === "listening") {
                ws.current.send(pcmBuffer);
              }
    
              // VAD & EOU Logic — only runs in listening state
              // (no interrupt feature; Kira finishes her response before we process speech)
              if (kiraStateRef.current === "listening") {
              const VAD_THRESHOLD = 300; 
              const isSpeakingFrame = rms > VAD_THRESHOLD;
    
              if (isSpeakingFrame) {
                speechFrameCount.current++;
                totalSpeechFrames.current++;
              } else {
                speechFrameCount.current = 0;
              }
    
              const isSpeaking = speechFrameCount.current > VAD_STABILITY_FRAMES;

              // Mark that the user has spoken enough to warrant an EOU
              if (totalSpeechFrames.current >= MIN_SPEECH_FRAMES_FOR_EOU) {
                hasSpoken.current = true;
              }
    
              if (isSpeaking) {
                // --- VISION: Snapshot-on-Speech ---
                // If this is the START of speech (transition from silence), capture a frame
                // Cooldown prevents re-triggering from micro-dips in natural speech
                if (speechFrameCount.current === (VAD_STABILITY_FRAMES + 1) && totalSpeechFrames.current >= 100) {
                    const now = Date.now();
                    if (now - lastSnapshotTime.current > SNAPSHOT_COOLDOWN_MS) {
                        // Screen share path
                        if (isScreenSharingRef.current) {
                            lastSnapshotTime.current = now;
                            debugLog("[Vision] Speech start detected while screen sharing. Attempting capture...");
                            const snapshot = captureScreenSnapshot();
                            if (snapshot) {
                                debugLog("[Vision] Sending snapshot on speech start...");
                                const payload = {
                                    type: "image",
                                    images: [...sceneBufferRef.current, snapshot]
                                };
                                ws.current.send(JSON.stringify(payload));
                            } else {
                                console.warn("[Vision] Snapshot capture returned null.");
                            }
                        }
                        // Camera path (mobile)
                        if (isCameraActiveRef.current) {
                            lastSnapshotTime.current = now;
                            debugLog("[Camera] Sending snapshot on speech start...");
                            captureAndSendCameraFrame();
                        }
                    }
                }

                // User is speaking — cancel any pending EOU timer
                if (eouTimer.current) {
                  clearTimeout(eouTimer.current);
                  eouTimer.current = null;
                }
    
                if (!maxUtteranceTimer.current) {
                  maxUtteranceTimer.current = setTimeout(() => {
                    debugLog("[EOU] Max utterance length reached. Forcing EOU.");
                    if (ws.current?.readyState === WebSocket.OPEN) {
                      eouSentAt.current = Date.now();
                      firstAudioLogged.current = false;
                      debugLog(`[Latency] EOU sent at ${eouSentAt.current}`);
                      ws.current.send(JSON.stringify({ type: "eou", forced: true }));
                    }
                    if (eouTimer.current) clearTimeout(eouTimer.current);
                    eouTimer.current = null;
                    maxUtteranceTimer.current = null;
                    // Reset speech tracking for next utterance
                    totalSpeechFrames.current = 0;
                    hasSpoken.current = false;
                  }, 60000); 
                }
              } else {
                // Silence detected — start EOU timer if user has spoken enough
                if (!eouTimer.current && hasSpoken.current) {
                  const adaptiveTimeout = getAdaptiveEOUTimeout();
                  eouTimer.current = setTimeout(() => {
                    debugLog(`[EOU] Silence detected after speech (${totalSpeechFrames.current} speech frames, timeout: ${adaptiveTimeout}ms), sending End of Utterance.`);
                    if (ws.current?.readyState === WebSocket.OPEN) {
                      eouSentAt.current = Date.now();
                      firstAudioLogged.current = false;
                      debugLog(`[Latency] EOU sent at ${eouSentAt.current}`);
                      ws.current.send(JSON.stringify({ type: "eou" }));
                    }
                    eouTimer.current = null;
                    if (maxUtteranceTimer.current) {
                      clearTimeout(maxUtteranceTimer.current);
                      maxUtteranceTimer.current = null;
                    }
                    // Reset speech tracking for next utterance
                    totalSpeechFrames.current = 0;
                    hasSpoken.current = false;
                  }, adaptiveTimeout);
                }
              }
              } // end if (kiraStateRef.current === "listening")
            }
          };
      }

      // 4. Connect the Mic to the Worklet (if not already)
      if (audioSource.current) audioSource.current.disconnect();
      
      debugLog("[Audio] Connecting mic to worklet...");
      if (audioStream.current) {
        audioSource.current = audioContext.current.createMediaStreamSource(
          audioStream.current
        );
        audioSource.current.connect(audioWorkletNode.current);
      } else {
        console.error("[Audio] No audio stream available to connect.");
      }

      // WORKAROUND: Connect worklet to a silent destination
      const silentGain = audioContext.current.createGain();
      silentGain.gain.value = 0;
      audioWorkletNode.current.connect(silentGain);
      silentGain.connect(audioContext.current.destination);

      debugLog("[Audio] ✅ Audio pipeline started.");
    } catch (err) {
      console.error("[Audio] ❌ Failed to start audio pipeline:", err);
      setError("Microphone access denied or failed. Please check permissions.");
    }
  }, [stopAudioPlayback, initializeAudio, captureScreenSnapshot]);

  /**
   * Explicitly start the conversation: send start_stream and start mic pipeline.
   * Adds detailed logs to trace user action and pipeline startup.
   */
  const startConversation = useCallback(() => {
    debugLog("[StartConvo] startConversation called. ws exists:", !!ws.current, "readyState:", ws.current?.readyState, "conversationActive:", conversationActive.current);
    
    // Idempotent — if conversation is already active (e.g. restored from singleton after remount), skip
    if (conversationActive.current) {
      debugLog("[StartConvo] Already active — skipping duplicate start_stream");
      return;
    }
    
    if (ws.current && ws.current.readyState === WebSocket.OPEN) {
      debugLog("[StartConvo] Sending 'start_stream' message...");
      try {
        ws.current.send(JSON.stringify({ type: "start_stream" }));
        conversationActive.current = true; // Mark session as live — no more auto-reconnect
        getConnectionStore()!.conversationActive = true;
        debugLog("[StartConvo] start_stream sent, conversationActive=true");
      } catch (err) {
        debugLog("[StartConvo] ❌ Failed to send start_stream:", err);
      }
      
      // Start mic immediately to satisfy browser user-gesture requirements
      debugLog("[StartConvo] Starting local audio pipeline...");
      startAudioPipeline();
    } else {
      debugLog(
        "[StartConvo] ❌ Cannot start: WebSocket not open. ws:", !!ws.current, "readyState:", ws.current?.readyState
      );
    }
  }, [startAudioPipeline]);

  /**
   * Signal that the visual layer (Live2D avatar or orb) is ready.
   * If the WebSocket is already open but waiting, this triggers start_stream + mic pipeline.
   */
  const signalVisualReady = useCallback(() => {
    if (visualReadyRef.current) return; // Already signaled
    visualReadyRef.current = true;
    debugLog("[VisualReady] Visual layer ready. wsOpen:", wsOpenRef.current, "conversationActive:", conversationActive.current);

    if (wsOpenRef.current && !conversationActive.current) {
      debugLog("[VisualReady] WS already open — sending start_stream now");
      startConversation();
    }
  }, [startConversation]);

  /**
   * Explicitly resume audio contexts.
   * Call this from a user gesture (click/tap) if audio is blocked.
   */
  const resumeAudio = useCallback(async () => {
    await initializeAudio();
  }, [initializeAudio]);

  /**
   * Main connection logic
   */
  const connect = useCallback(async () => {
    if (ws.current) {
      debugLog("[Connect] Aborted — WebSocket already exists");
      return;
    }

    debugLog("[Connect] Starting connection attempt...");

    // Initialize Audio IMMEDIATELY (Synchronously inside gesture if possible)
    const audioOk = await initializeAudio();
    debugLog(`[Connect] Audio initialized: ${audioOk}`);
    if (!audioOk) {
      debugLog("[Connect] ❌ Failed: audio initialization returned false (mic denied or AudioContext failed)");
      return;
    }

    // Fetch a FRESH auth token right before connecting — prevents stale JWT race conditions
    // (token fetched at mount time can expire before the user clicks "start")
    let freshToken: string | null = null;
    if (getTokenFn) {
      try {
        freshToken = await getTokenFn();
        debugLog("[Connect] Auth token fetched successfully");
      } catch (err) {
        debugLog("[Connect] ❌ Failed to get fresh token:", err);
      }
    }

    const wsUrl = process.env.NEXT_PUBLIC_WEBSOCKET_URL!;
    const authParam = freshToken ? `token=${freshToken}` : `guestId=${guestId}`;
    const voiceParam = `&voice=${voicePreference}`;
    debugLog(`[Connect] Opening WS: ${wsUrl}?${authParam}${voiceParam}`);

    debugLog("[State] socketState → connecting");
    setSocketState("connecting");
    getConnectionStore()!.socketState = "connecting";
    isServerReady.current = false;
    ws.current = new WebSocket(`${wsUrl}?${authParam}${voiceParam}`);
    ws.current.binaryType = "arraybuffer"; // We are sending and receiving binary

    // ─── Store to singleton immediately so remounts can find it ───
    debugLog("[Singleton] getConnectionStore()!.ws → WebSocket (from connect, pre-open)");
    getConnectionStore()!.ws = ws.current;

    ws.current.onopen = () => {
      debugLog("[State] socketState → connected");
      setSocketState("connected");
      getConnectionStore()!.socketState = "connected";
      reconnectAttempts.current = 0; // Reset on successful connection
      getConnectionStore()!.reconnectAttempts = 0;
      setError(null); // Clear any error banner from a previous disconnect
      debugLog("[Connect] ✅ WebSocket connected. Singleton stored immediately.");
      // Store audio refs to singleton now that connection is live
      getConnectionStore()!.audioContext = audioContext.current;
      getConnectionStore()!.playbackContext = playbackContext.current;
      getConnectionStore()!.audioStream = audioStream.current;
      
      // ─── Visual-ready gating ───
      // Don't send start_stream until the visual layer (Live2D / orb) signals ready.
      // This prevents the server from sending audio before the user sees anything.
      // Fallback: 15s timeout so we never hang forever if Live2D fails silently.
      wsOpenRef.current = true;

      if (visualReadyRef.current) {
        // Visual is already loaded (orb mode, or Live2D preloaded fast) — start immediately
        debugLog("[Connect] Visual already ready — sending start_stream now");
        if (!conversationActive.current) {
          startConversation();
        }
      } else {
        // Visual not ready yet — wait for signalVisualReady() or 15s timeout
        debugLog("[Connect] Waiting for visual ready signal (15s timeout)...");
        setTimeout(() => {
          if (!conversationActive.current && wsOpenRef.current) {
            debugLog("[Connect] ⏱ Visual-ready timeout (15s) — sending start_stream anyway");
            startConversation();
          }
        }, 15000);
      }
    };

    // ─── Wire handlers through refs so remounts get fresh closures ───
    onMessageRef.current = (event: MessageEvent) => {
      if (typeof event.data === "string") {
        // This is a JSON control message
        const msg = JSON.parse(event.data);
        debugLog("[WS] ← message:", msg.type, msg.type === "session_config" ? JSON.stringify(msg).slice(0, 200) : "");

        switch (msg.type) {
          case "session_config":
            debugLog("[WS] Received session_config:", JSON.stringify(msg));
            setIsPro(msg.isPro);
            isProRef.current = msg.isPro;
            if (msg.remainingSeconds !== undefined) {
              setRemainingSeconds(msg.remainingSeconds);
            }
            break;
          case "stream_ready":
            debugLog("[WS] Received stream_ready — setting kiraState to listening");
            setKiraState("listening");
            isServerReady.current = true;
            getConnectionStore()!.isServerReady = true;
            break;
          case "ping":
            // Respond to server heartbeat to keep connection alive
            if (ws.current?.readyState === WebSocket.OPEN) {
                ws.current.send(JSON.stringify({ type: "pong" }));
            }
            break;
          case "state_thinking":
            kiraStateRef.current = "thinking";
            if (eouTimer.current) clearTimeout(eouTimer.current); // Stop EOU timer
            setKiraState("thinking");
            break;
          case "state_speaking":
            kiraStateRef.current = "speaking";
            setKiraState("speaking");
            audioQueue.current = []; // Clear old queue
            nextStartTime.current = 0; // Reset scheduling time
            ttsChunksDone.current = false; // CRITICAL: Prevent visualizer from self-terminating before audio arrives
            break;
          case "state_listening":
            kiraStateRef.current = "listening";
            setKiraState("listening");
            break;
          case "transcript":
            setTranscript({ role: msg.role, text: msg.text });
            break;
          case "expression":
            setCurrentExpression(msg.expression || "neutral");
            break;
          case "accessory": {
            const { accessory, action } = msg;
            setActiveAccessories(prev => {
              if (action === "on") {
                return prev.includes(accessory) ? prev : [...prev, accessory];
              } else {
                return prev.filter(a => a !== accessory);
              }
            });
            break;
          }
          case "tts_chunk_starts":
            ttsChunksDone.current = false; // More audio chunks incoming
            break;
          case "tts_chunk_ends":
            // The server is done sending audio for this turn
            ttsChunksDone.current = true; // Visualizer can now self-terminate when queue drains
            break;
          case "text_response":
            setTranscript({ role: "ai", text: msg.text });
            // Orb goes to "speaking" briefly to visually acknowledge
            kiraStateRef.current = "speaking";
            setKiraState("speaking");
            setTimeout(() => {
              kiraStateRef.current = "listening";
              setKiraState("listening");
            }, 1500);
            break;
          case "error":
            if (msg.code === "limit_reached") {
              if (msg.tier === "pro") {
                debugLog("[WS] ⚠️ Pro monthly limit reached.");
                setError("limit_reached_pro");
              } else {
                debugLog("[WS] ⚠️ Daily limit reached.");
                setError("limit_reached");
              }
            } else {
              debugLog("[WS] ❌ Server error:", msg.message);
              setError(msg.message);
            }
            break;
        }
      } else if (event.data instanceof ArrayBuffer) {
        // This is a raw PCM audio chunk from Azure
        // Only process audio if we are in 'speaking' state.
        // If we are 'listening' (e.g. due to interruption), we drop these packets.
        if (kiraStateRef.current === "speaking") {
            if (!firstAudioLogged.current && eouSentAt.current > 0) {
              firstAudioLogged.current = true;
              debugLog(`[Latency] Client: EOU → first audio: ${Date.now() - eouSentAt.current}ms`);
            }
            audioQueue.current.push(event.data);
            processAudioQueue();
        }
      }
    };

    onCloseRef.current = (event: CloseEvent) => {
      debugLog("[WS] 🔌 Connection closed. Code:", event.code, "Reason:", event.reason, "Clean:", event.wasClean);
      debugLog("[Singleton] getConnectionStore()!.ws → null (from onclose). Caller:", new Error().stack?.split('\n')[1]?.trim());
      debugLog("[State] socketState → closed (from onclose)");
      setSocketState("closed");
      getConnectionStore()!.socketState = "closed";
      wsOpenRef.current = false; // WS is no longer open
      
      // ─── Clear singleton ───
      getConnectionStore()!.ws = null;
      getConnectionStore()!.isServerReady = false;
      getConnectionStore()!.conversationActive = false;

      if (event.code === 1008) {
        // Don't overwrite a more specific error (e.g. "limit_reached_pro")
        // If user is Pro, always use "limit_reached_pro" — never show the free-tier paywall
        setError((prev) => {
          if (prev?.startsWith("limit_reached")) return prev;
          return isProRef.current ? "limit_reached_pro" : "limit_reached";
        });
      }

      stopAudioPipeline();
      ws.current = null;
      isServerReady.current = false; // Prevent stale audio sends on reconnect

      // Auto-reconnect logic:
      // ONLY reconnect if the conversation hasn't started yet (pre-stream connection flakiness).
      // Once a live voice session is active, reconnecting would create a fresh server session
      // (new chatHistory, new usage timer, new opener) — causing the "conversation loop" bug
      // where the same exchange replays and the usage counter goes backwards.
      if (event.code !== 1000 && event.code !== 1008) {
        if (conversationActive.current) {
          // Live session was interrupted — don't reconnect, show error
          debugLog("[WS] Connection lost during active conversation — not reconnecting (would create duplicate session)");
          setError("connection_lost");
        } else if (reconnectAttempts.current < MAX_RECONNECT_ATTEMPTS) {
          // Pre-conversation connection drop — safe to retry
          const delay = Math.min(1000 * Math.pow(2, reconnectAttempts.current), 10000);
          reconnectAttempts.current++;
          getConnectionStore()!.reconnectAttempts = reconnectAttempts.current;
          debugLog(`[WS] Reconnecting in ${delay}ms (attempt ${reconnectAttempts.current}/${MAX_RECONNECT_ATTEMPTS})...`);
          setTimeout(() => {
            connect();
          }, delay);
        } else {
          // All reconnect attempts exhausted
          setError("Connection lost. Please refresh the page.");
        }
      }
    };

    onErrorRef.current = (err: Event) => {
      debugLog("[WS] ❌ WebSocket error event fired:", err);
      // Don't null getConnectionStore()!.ws here — onclose ALWAYS fires after onerror
      // and handles singleton cleanup + reconnect logic. Nulling here would race.
      // Don't set socketState or call stopAudioPipeline — let onclose handle it all.
    };

    // ─── Wire WS events through refs (so remounts refresh closures) ───
    ws.current.onmessage = (e) => onMessageRef.current?.(e);
    ws.current.onclose = (e) => onCloseRef.current?.(e);
    ws.current.onerror = (e) => onErrorRef.current?.(e);

  }, [getTokenFn, guestId, startConversation, startAudioPipeline, processAudioQueue, stopAudioPipeline, initializeAudio]);

  const disconnect = useCallback(() => {
    debugLog("[WS] disconnect() called. ws.current exists:", !!ws.current);
    if (eouTimer.current) clearTimeout(eouTimer.current);
    reconnectAttempts.current = MAX_RECONNECT_ATTEMPTS; // Prevent any reconnection
    conversationActive.current = false; // Clean shutdown — not a crash
    visualReadyRef.current = false; // Reset for next session
    wsOpenRef.current = false;
    // ─── Clear singleton — this is an intentional disconnect ───
    debugLog("[Singleton] getConnectionStore()!.ws → null (from disconnect)");
    getConnectionStore()!.ws = null;
    getConnectionStore()!.socketState = "closing";
    getConnectionStore()!.isServerReady = false;
    getConnectionStore()!.conversationActive = false;
    getConnectionStore()!.reconnectAttempts = 0;
    if (ws.current) {
      debugLog("[State] socketState → closing (from disconnect)");
      setSocketState("closing");
      ws.current.close(1000, "User ended call"); // Code 1000 = intentional close, won't trigger reconnect
    }
  }, []);

  /**
   * Helper function to create a WAV header for raw PCM data
   */
  const createWavHeader = (
    data: ArrayBuffer,
    sampleRate: number,
    sampleBits: number
  ): ArrayBuffer => {
    const dataLength = data.byteLength;
    const buffer = new ArrayBuffer(44 + dataLength);
    const view = new DataView(buffer);

    const writeString = (offset: number, str: string) => {
      for (let i = 0; i < str.length; i++) {
        view.setUint8(offset + i, str.charCodeAt(i));
      }
    };

    const channels = 1;
    const byteRate = (sampleRate * channels * sampleBits) / 8;
    const blockAlign = (channels * sampleBits) / 8;

    writeString(0, "RIFF");
    view.setUint32(4, 36 + dataLength, true);
    writeString(8, "WAVE");
    writeString(12, "fmt ");
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, channels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, byteRate, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, sampleBits, true);
    writeString(36, "data");
    view.setUint32(40, dataLength, true);

    // Copy the PCM data
    const pcm = new Uint8Array(data);
    const dataView = new Uint8Array(buffer, 44);
    dataView.set(pcm);

    return buffer;
  };

  /**
   * Send a text message (text chat mode — skips STT/TTS)
   */
  const sendText = useCallback((text: string) => {
    if (ws.current?.readyState === WebSocket.OPEN) {
      ws.current.send(JSON.stringify({ type: "text_message", text }));
      setTranscript({ role: "user", text });
    }
  }, []);

  const sendVoiceChange = useCallback((voice: string) => {
    if (ws.current?.readyState === WebSocket.OPEN) {
      ws.current.send(JSON.stringify({ type: "voice_change", voice }));
      debugLog(`[WS] Sent voice_change: ${voice}`);
    }
  }, []);

  return {
    connect,
    disconnect,
    startConversation,
    signalVisualReady,
    socketState,
    kiraState,
    micVolume,
    transcript,
    sendText,
    sendVoiceChange,
    error,
    isAudioBlocked,
    resumeAudio,
    isMuted,
    toggleMute,
    isScreenSharing,
    startScreenShare,
    stopScreenShare,
    isCameraActive,
    cameraStreamRef,
    facingMode,
    startCamera,
    stopCamera,
    flipCamera,
    isPro,
    remainingSeconds,
    isAudioPlaying,
    playerVolume,
    playbackAnalyserNode: playbackAnalyser.current,
    currentExpression,
    activeAccessories,
  };
};

═══════════════════════════════════════════════════════════════════════════════
FILE: packages/web/src/app/(chat)/chat/[conversationId]/ChatClient.tsx
═══════════════════════════════════════════════════════════════════════════════
"use client";

import { useAuth, useClerk } from "@clerk/nextjs";
import { useCallback, useEffect, useRef, useState } from "react";
import { useKiraSocket, debugLog } from "@/hooks/useKiraSocket";
import { PhoneOff, Star, User, Mic, MicOff, Eye, EyeOff, Clock, Sparkles, Camera } from "lucide-react";
import ProfileModal from "@/components/ProfileModal";
import KiraOrb from "@/components/KiraOrb";
import { getOrCreateGuestId } from "@/lib/guestId";
import { getVoicePreference, setVoicePreference, VoicePreference } from "@/lib/voicePreference";
import { KiraLogo } from "@/components/KiraLogo";
import dynamic from "next/dynamic";

const Live2DAvatar = dynamic(() => import("@/components/Live2DAvatar"), { ssr: false });
const XOLoader = dynamic(() => import("@/components/XOLoader"), { ssr: false });

export default function ChatClient() {
  const { getToken, userId, isLoaded: clerkLoaded } = useAuth();
  const { openSignIn } = useClerk();
  const [showRatingModal, setShowRatingModal] = useState(false);
  const hasShownRating = useRef(false); // Prevent rating dialog from showing twice
  const [showProfileModal, setShowProfileModal] = useState(false);
  const [rating, setRating] = useState(0);
  const [hoverRating, setHoverRating] = useState(0);
  const [guestId, setGuestId] = useState("");
  const [voicePreference, setVoicePref] = useState<VoicePreference>("anime");
  const [visualMode, setVisualMode] = useState<"avatar" | "orb">("avatar");
  const [live2dReady, setLive2dReady] = useState(false);
  const [live2dFailed, setLive2dFailed] = useState(false);
  const [live2dDismissed, setLive2dDismissed] = useState(false); // set true before WS close to clean up PIXI first
  const isDisconnectingRef = useRef(false); // prevents orb fallback flash during clean shutdown
  const [isMobile, setIsMobile] = useState(false);
  const [deviceDetected, setDeviceDetected] = useState(false);
  const live2dRetryCount = useRef(0);
  const MAX_LIVE2D_RETRIES = 1;

  // --- Debug: track mount/unmount and what triggers remount ---
  useEffect(() => {
    debugLog("[ChatClient] MOUNTED. URL:", window.location.href, "userId:", userId, "clerkLoaded:", clerkLoaded);
    return () => {
      debugLog("[ChatClient] UNMOUNTING. URL:", window.location.href);
    };
  // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  // --- Debug: track Clerk auth state changes (userId flipping can cause subtree re-renders) ---
  const prevUserId = useRef(userId);
  useEffect(() => {
    if (prevUserId.current !== userId) {
      debugLog("[ChatClient] userId changed:", prevUserId.current, "→", userId);
      prevUserId.current = userId;
    }
  }, [userId]);

  // --- Debug: track Clerk isLoaded change ---
  useEffect(() => {
    debugLog("[ChatClient] clerkLoaded changed to:", clerkLoaded, "userId:", userId);
  }, [clerkLoaded, userId]);

  useEffect(() => {
    const checkMobile = () => {
      const mobile =
        /iPhone|iPad|iPod|Android/i.test(navigator.userAgent) ||
        (navigator.maxTouchPoints > 0 && window.innerWidth < 768);
      setIsMobile(mobile);
      setDeviceDetected(true);
    };
    checkMobile();
    window.addEventListener("resize", checkMobile);

    // Fallback re-check: guarantee detection even if the initial check raced
    const fallback = setTimeout(checkMobile, 2000);

    return () => {
      window.removeEventListener("resize", checkMobile);
      clearTimeout(fallback);
    };  
  }, []);

  // If Live2D fails to load (e.g. mobile GPU limits), auto-switch to orb
  // Skip fallback during clean disconnect — just let the component unmount
  useEffect(() => {
    if (live2dFailed && visualMode === "avatar" && !isDisconnectingRef.current) {
      setVisualMode("orb");
      debugLog("[UI] Live2D failed — falling back to orb mode");
    }
  }, [live2dFailed, visualMode]);

  // Load guest ID, voice preference, and chat toggle from localStorage
  useEffect(() => {
    if (!userId) {
      setGuestId(getOrCreateGuestId());
    }
    setVoicePref(getVoicePreference());
  }, [userId]);

  const { 
    connect, 
    disconnect,
    signalVisualReady,
    socketState, 
    kiraState, 
    micVolume, 
    error,
    sendVoiceChange,
    isAudioBlocked, 
    resumeAudio,
    isMuted,
    toggleMute,
    isScreenSharing,
    startScreenShare,
    stopScreenShare,
    isCameraActive,
    cameraStreamRef,
    facingMode,
    startCamera,
    stopCamera,
    flipCamera,
    isPro,
    remainingSeconds,
    isAudioPlaying,
    playerVolume,
    playbackAnalyserNode,
    currentExpression,
    activeAccessories
  } = useKiraSocket(
    userId ? getToken : null,
    guestId,
    voicePreference
  );

  // Orb mode is always "visually ready" — signal immediately so start_stream isn't blocked
  useEffect(() => {
    if (visualMode === "orb") {
      signalVisualReady();
    }
  }, [visualMode, signalVisualReady]);

  // ─── Camera PIP preview ───
  const previewVideoRef = useRef<HTMLVideoElement>(null);
  const [pipPosition, setPipPosition] = useState({ x: 16, y: 140 }); // offset from bottom-right
  const pipDragRef = useRef<{ startX: number; startY: number; origX: number; origY: number } | null>(null);

  // Attach stream to video element whenever camera becomes active
  useEffect(() => {
    if (!isCameraActive) {
      // Reset PIP position when camera stops
      setPipPosition({ x: 16, y: 140 });
      return;
    }
    const vid = previewVideoRef.current;
    const stream = cameraStreamRef.current;
    if (vid && stream) {
      vid.srcObject = stream;
      vid.setAttribute("playsinline", "true");
      vid.muted = true;
      vid.play().catch(() => {});
    }
  }, [isCameraActive, cameraStreamRef]);

  const handlePipTouchStart = useCallback((e: React.TouchEvent) => {
    const touch = e.touches[0];
    pipDragRef.current = {
      startX: touch.clientX,
      startY: touch.clientY,
      origX: pipPosition.x,
      origY: pipPosition.y,
    };
  }, [pipPosition]);

  const handlePipTouchMove = useCallback((e: React.TouchEvent) => {
    if (!pipDragRef.current) return;
    const touch = e.touches[0];
    const dx = touch.clientX - pipDragRef.current.startX;
    const dy = touch.clientY - pipDragRef.current.startY;
    setPipPosition({
      x: pipDragRef.current.origX - dx, // inverted because offset is from right
      y: pipDragRef.current.origY + dy,  // inverted because offset is from bottom
    });
  }, []);

  const handlePipTouchEnd = useCallback(() => {
    pipDragRef.current = null;
  }, []);

  // ─── start_stream is now sent atomically in the hook's onopen handler ───
  // No more useEffect race — connect() → WS open → start_stream → audio pipeline
  // all happen in the same call stack, immune to React remounts.

  // ─── DO NOT disconnect on unmount ───
  // React can remount this component at any time (Clerk auth, Next.js RSC, etc.).
  // The WebSocket lives in a module-level singleton and survives remounts.
  // Only handleEndCall() → disconnect() closes the WS intentionally.
  useEffect(() => {
    return () => {
      debugLog("[ChatClient] Unmount cleanup — WS stays alive in singleton");
      // Just clean up Live2D visuals, don't touch the WebSocket
      isDisconnectingRef.current = true;
      setLive2dDismissed(true);
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  // --- UI Logic ---

  const handleEndCall = async () => {
    // 1. Mark disconnecting to prevent orb fallback flash
    isDisconnectingRef.current = true;
    // 2. Unmount Live2D first so PIXI can destroy its WebGL context cleanly
    setLive2dDismissed(true);
    // 3. Give the browser time to flush the React unmount + release GPU memory.
    //    500ms is generous but mobile browsers need it to actually free VRAM
    //    after WEBGL_lose_context before a new context can be created.
    await new Promise(r => setTimeout(r, 500));
    // 4. Then close WebSocket
    disconnect();
    if (!hasShownRating.current) {
      hasShownRating.current = true;
      setShowRatingModal(true);
    }
  };

  const handleRate = () => {
    // TODO: Save rating to backend
    debugLog("User rated conversation:", rating);
    setShowRatingModal(false);
    window.location.href = "/"; // Hard nav to guarantee WebGL cleanup
  };

  const handleContinue = () => {
    setShowRatingModal(false);
    window.location.href = "/"; // Hard nav to guarantee WebGL cleanup
  };

  const handleUpgrade = async () => {
    try {
      const res = await fetch("/api/stripe/checkout", { method: "POST" });
      if (res.ok) {
        const data = await res.json();
        window.location.href = data.url;
      } else {
        console.error("Failed to start checkout");
      }
    } catch (error) {
      console.error("Checkout error:", error);
    }
  };

  const isGuest = !userId;

  const handleSignUp = () => {
    // Pass guestId via unsafe_metadata so the Clerk webhook can migrate the conversation
    openSignIn({
      afterSignInUrl: window.location.href,
      afterSignUpUrl: window.location.href,
    });
    // Note: guestId is preserved in localStorage — on next connect as signed-in user,
    // the webhook will have already migrated the buffer
  };

  // --- Local countdown for time remaining ---
  const [localRemaining, setLocalRemaining] = useState<number | null>(null);

  // Sync from server when session_config arrives
  useEffect(() => {
    if (remainingSeconds !== null) {
      setLocalRemaining(remainingSeconds);
    }
  }, [remainingSeconds]);

  // Tick down every second while connected
  useEffect(() => {
    if (socketState !== "connected" || localRemaining === null) return;
    const interval = setInterval(() => {
      setLocalRemaining((prev) => (prev !== null && prev > 0 ? prev - 1 : 0));
    }, 1000);
    return () => clearInterval(interval);
  }, [socketState, localRemaining !== null]);

  // Dump persistent debug logs from sessionStorage (survives page reloads)
  useEffect(() => {
    if (socketState === "idle") {
      try {
        const raw = sessionStorage.getItem('kira-debug');
        if (raw) {
          const logs = JSON.parse(raw) as string[];
          debugLog(`%c[DebugDump] ${logs.length} stored logs:`, 'color: orange; font-weight: bold');
          logs.forEach((l) => debugLog(l));
        }
      } catch {}
    }
  }, [socketState]);

  // Start Screen (Initial State for ALL users)
  if (socketState === "idle") {
    return (
      <div style={{
        minHeight: "100vh",
        background: "#0D1117",
        display: "flex",
        flexDirection: "column",
        alignItems: "center",
        justifyContent: "center",
        fontFamily: "'DM Sans', -apple-system, BlinkMacSystemFont, sans-serif",
        padding: "24px",
        textAlign: "center",
        position: "relative",
      }}>
        {/* Subtle ambient glow */}
        <div style={{
          position: "absolute",
          top: "40%",
          left: "50%",
          transform: "translate(-50%, -50%)",
          width: 400,
          height: 400,
          borderRadius: "50%",
          background: "radial-gradient(circle, rgba(107,125,179,0.05) 0%, transparent 70%)",
          pointerEvents: "none",
        }} />

        {/* Mic icon badge */}
        <div style={{
          width: 64,
          height: 64,
          borderRadius: 16,
          background: "linear-gradient(135deg, rgba(107,125,179,0.12), rgba(107,125,179,0.04))",
          border: "1px solid rgba(107,125,179,0.15)",
          display: "flex",
          alignItems: "center",
          justifyContent: "center",
          marginBottom: 28,
          position: "relative",
        }}>
          <svg width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="rgba(139,157,195,0.7)" strokeWidth="1.8" strokeLinecap="round" strokeLinejoin="round">
            <path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z" />
            <path d="M19 10v2a7 7 0 0 1-14 0v-2" />
            <line x1="12" y1="19" x2="12" y2="22" />
          </svg>
        </div>

        <h2 style={{
          fontSize: 22,
          fontFamily: "'Playfair Display', serif",
          fontWeight: 400,
          color: "#E2E8F0",
          marginBottom: 10,
          marginTop: 0,
          position: "relative",
        }}>
          Enable your microphone
        </h2>

        <p style={{
          fontSize: 15,
          fontWeight: 300,
          color: "rgba(201,209,217,0.45)",
          lineHeight: 1.6,
          maxWidth: 340,
          marginBottom: 32,
          position: "relative",
        }}>
          Kira needs microphone access to hear you.
          Your audio is never stored or recorded.
        </p>

        <button
          onClick={() => { debugLog("[Chat] 🎤 Connect button tapped"); connect(); }}
          style={{
            display: "flex",
            alignItems: "center",
            gap: 10,
            padding: "14px 36px",
            borderRadius: 12,
            background: "linear-gradient(135deg, rgba(107,125,179,0.2), rgba(107,125,179,0.08))",
            border: "1px solid rgba(107,125,179,0.25)",
            color: "#C9D1D9",
            fontSize: 15,
            fontWeight: 500,
            cursor: "pointer",
            transition: "all 0.3s ease",
            fontFamily: "'DM Sans', sans-serif",
            boxShadow: "0 0 30px rgba(107,125,179,0.08)",
            position: "relative",
          }}
          onMouseEnter={(e) => {
            e.currentTarget.style.background = "linear-gradient(135deg, rgba(107,125,179,0.3), rgba(107,125,179,0.15))";
            e.currentTarget.style.boxShadow = "0 0 40px rgba(107,125,179,0.15)";
            e.currentTarget.style.transform = "translateY(-1px)";
          }}
          onMouseLeave={(e) => {
            e.currentTarget.style.background = "linear-gradient(135deg, rgba(107,125,179,0.2), rgba(107,125,179,0.08))";
            e.currentTarget.style.boxShadow = "0 0 30px rgba(107,125,179,0.08)";
            e.currentTarget.style.transform = "translateY(0)";
          }}
        >
          Allow microphone
        </button>
      </div>
    );
  }

  return (
    <div style={{ background: "#0D1117", fontFamily: "'DM Sans', -apple-system, BlinkMacSystemFont, sans-serif", height: "100dvh" }} className="flex flex-col items-center justify-center w-full">
      {/* Header */}
      <div className="absolute top-0 left-0 right-0 p-6 flex justify-between items-center z-20">
        <a href="/">
          <span className="font-medium text-lg flex items-center gap-2" style={{ color: "#C9D1D9" }}>
            <KiraLogo size={24} id="chatXO" />
            Kira
          </span>
        </a>
        
        {/* Profile Link + Timer */}
        <div style={{ display: "flex", alignItems: "center", gap: 12 }}>
          {/* Timer — only shows under 5 min remaining for free users */}
          {!isPro && localRemaining !== null && localRemaining <= 300 && localRemaining > 0 && (
            <span
              style={{
                fontSize: 12,
                fontWeight: 300,
                fontFamily: "'DM Sans', sans-serif",
                color: `rgba(201,209,217,${localRemaining <= 120 ? 0.5 : 0.25})`,
                letterSpacing: "0.06em",
              }}
            >
              {Math.floor(localRemaining / 60)}:{String(localRemaining % 60).padStart(2, "0")}
            </span>
          )}
          {/* Voice selector */}
          <div style={{
            display: "flex",
            borderRadius: 8,
            overflow: "hidden",
            border: "1px solid rgba(201,209,217,0.12)",
          }}>
            {(["anime", "natural"] as const).map((v) => (
              <button
                key={v}
                onClick={() => {
                  setVoicePref(v);
                  setVoicePreference(v);
                  sendVoiceChange(v);
                }}
                style={{
                  padding: "4px 10px",
                  fontSize: 11,
                  fontWeight: voicePreference === v ? 500 : 300,
                  fontFamily: "'DM Sans', sans-serif",
                  background: voicePreference === v ? "rgba(107,125,179,0.25)" : "transparent",
                  color: voicePreference === v ? "#C9D1D9" : "rgba(201,209,217,0.4)",
                  border: "none",
                  cursor: "pointer",
                  letterSpacing: "0.04em",
                  textTransform: "capitalize",
                  transition: "all 0.2s ease",
                }}
              >
                {v}
              </button>
            ))}
          </div>
          {/* Profile icon */}
          <button 
            onClick={() => setShowProfileModal(true)}
            className="p-2 rounded-full transition-colors"
            style={{ background: "none", border: "none", cursor: "pointer" }}
            onMouseEnter={(e) => { e.currentTarget.style.background = "rgba(255,255,255,0.08)"; }}
            onMouseLeave={(e) => { e.currentTarget.style.background = "none"; }}
          >
              <User size={24} style={{ color: "rgba(201,209,217,0.6)" }} />
          </button>
        </div>
      </div>

      {/* Profile Modal */}
      <ProfileModal 
        isOpen={showProfileModal} 
        onClose={() => setShowProfileModal(false)} 
        isPro={isPro}
      />

      {/* Main Content Area — orb/avatar centered */}
      <div className="flex-grow relative w-full max-w-4xl mx-auto" style={{ minHeight: 0, overflow: "hidden", zIndex: 1 }}>
        {/* Visual — absolutely centered */}
        <div className="absolute inset-0 flex items-center justify-center pointer-events-none" style={{ paddingBottom: isMobile ? 140 : 160 }}>
          <div className="pointer-events-auto" style={{ width: visualMode === "avatar" ? "100%" : undefined, height: visualMode === "avatar" ? "100%" : undefined, position: visualMode === "avatar" ? "relative" : undefined, maxHeight: "100%" }}>
            {visualMode === "avatar" ? (
              <>
                {!live2dReady && <XOLoader />}
                {!live2dDismissed && (
                    <Live2DAvatar
                      isSpeaking={isAudioPlaying}
                      analyserNode={playbackAnalyserNode}
                      emotion={currentExpression}
                      accessories={activeAccessories}
                      onModelReady={() => {
                        setLive2dReady(true);
                        signalVisualReady();
                      }}
                      onLoadError={() => setLive2dFailed(true)}
                    />
                )}
              </>
            ) : (
              <KiraOrb
                state={
                  isAudioPlaying
                    ? "kiraSpeaking"
                    : kiraState === "thinking"
                      ? "thinking"
                      : micVolume > 0.02
                        ? "userSpeaking"
                        : "idle"
                }
                micVolume={micVolume}
                kiraVolume={isAudioPlaying ? playerVolume : 0}
                size="lg"
                enableBreathing={false}
              />
            )}
          </div>
        </div>
      </div>

      {/* ─── Bottom Area: Controls ─── */}
      <div
        className="fixed bottom-0 left-0 right-0 flex flex-col items-center gap-5 pb-9"
        style={{ zIndex: 50, position: "fixed" }}
      >
        {/* Status indicator + errors — sits between avatar and controls */}
        <div style={{ textAlign: "center", minHeight: 28, display: "flex", flexDirection: "column", alignItems: "center", justifyContent: "center", margin: "24px 0 8px 0" }}>
          {error && error !== "limit_reached" && error !== "limit_reached_pro" && error !== "connection_lost" && (
            <div className="mb-2 p-3 rounded relative" style={{
              background: "rgba(200,55,55,0.15)",
              border: "1px solid rgba(200,55,55,0.3)",
              color: "rgba(255,120,120,0.9)",
            }}>
              <span className="block sm:inline">{error}</span>
            </div>
          )}
          {error === "connection_lost" && (
            <div className="mb-2 p-4 rounded relative text-center" style={{
              background: "rgba(200,150,55,0.15)",
              border: "1px solid rgba(200,150,55,0.3)",
              color: "rgba(255,210,130,0.9)",
            }}>
              <p className="mb-2" style={{ fontSize: 14 }}>Connection lost. Your conversation ended.</p>
              <button
                onClick={() => window.location.reload()}
                className="px-4 py-2 rounded text-sm font-medium transition-colors"
                style={{
                  background: "rgba(139,157,195,0.2)",
                  border: "1px solid rgba(139,157,195,0.3)",
                  color: "rgba(200,210,230,0.9)",
                }}
              >
                Start New Conversation
              </button>
            </div>
          )}
        </div>
        {/* Voice Controls */}
        <div className="flex items-center gap-4 relative z-[1]">
        {/* Avatar/Orb Toggle */}
        <button
          onClick={() => {
            if (visualMode === "avatar") {
              // Switching to orb — reset retry count so user can try avatar again later
              live2dRetryCount.current = 0;
              setVisualMode("orb");
            } else {
              // Switching back to avatar — only allow if retries not exhausted
              if (live2dRetryCount.current < MAX_LIVE2D_RETRIES) {
                live2dRetryCount.current++;
                setLive2dFailed(false);
                setLive2dReady(false);
                // Delay remount to let iOS free the previous WebGL context
                setLive2dDismissed(true);
                setTimeout(() => {
                  setLive2dDismissed(false);
                  setVisualMode("avatar");
                }, 800);
              } else {
                debugLog("[UI] Live2D retry limit reached — staying on orb");
              }
            }
          }}
          className="flex items-center justify-center w-12 h-12 rounded-full border-none transition-all duration-200"
          style={{
            background: visualMode === "avatar" ? "rgba(255,255,255,0.12)" : "rgba(255,255,255,0.04)",
            color: visualMode === "avatar" ? "rgba(139,157,195,0.9)" : "rgba(139,157,195,0.45)",
          }}
          title={visualMode === "avatar" ? "Switch to Orb" : "Switch to Avatar"}
        >
          <Sparkles size={18} />
        </button>

        {/* Vision / Camera Button — only rendered after device detection */}
        {deviceDetected && !isMobile && (
          <button
            onClick={isScreenSharing ? stopScreenShare : startScreenShare}
            className="flex items-center justify-center w-12 h-12 rounded-full border-none transition-all duration-200"
            style={{
              background: isScreenSharing ? "rgba(255,255,255,0.12)" : "rgba(255,255,255,0.04)",
              color: isScreenSharing ? "rgba(139,157,195,0.9)" : "rgba(139,157,195,0.45)",
            }}
            title={isScreenSharing ? "Stop screen share" : "Start screen share"}
          >
            {isScreenSharing ? <Eye size={18} /> : <EyeOff size={18} />}
          </button>
        )}

        {/* Camera Button — mobile only, rendered after device detection */}
        {deviceDetected && isMobile && (
          <button
            onClick={() => isCameraActive ? stopCamera() : startCamera()}
            className="flex items-center justify-center w-12 h-12 rounded-full border-none transition-all duration-200"
            style={{
              background: isCameraActive ? "rgba(255,255,255,0.12)" : "rgba(255,255,255,0.04)",
              color: isCameraActive ? "rgba(139,157,195,0.9)" : "rgba(139,157,195,0.45)",
            }}
            title={isCameraActive ? "Stop camera" : "Start camera"}
          >
            <Camera size={18} />
          </button>
        )}

        {/* Mute Button */}
        <button
          onClick={toggleMute}
          className="flex items-center justify-center w-12 h-12 rounded-full border-none transition-all duration-200"
          style={{
            background: isMuted ? "rgba(255,255,255,0.12)" : "rgba(255,255,255,0.04)",
            color: isMuted ? "rgba(139,157,195,0.9)" : "rgba(139,157,195,0.45)",
          }}
        >
          {isMuted ? <MicOff size={18} /> : <Mic size={18} />}
        </button>

        {/* End Call Button */}
        <button
          onClick={handleEndCall}
          className="flex items-center justify-center w-12 h-12 rounded-full border-none transition-all duration-200"
          style={{
            background: "rgba(200,55,55,0.75)",
            color: "rgba(255,255,255,0.9)",
          }}
          title="End Call"
        >
          <PhoneOff size={18} />
        </button>
        </div>
      </div>

      {/* Camera PIP Preview */}
      {isCameraActive && (
        <div
          onTouchStart={handlePipTouchStart}
          onTouchMove={handlePipTouchMove}
          onTouchEnd={handlePipTouchEnd}
          style={{
            position: "fixed",
            bottom: pipPosition.y,
            right: pipPosition.x,
            width: 80,
            height: 107,
            borderRadius: 12,
            overflow: "hidden",
            border: "1px solid rgba(255, 255, 255, 0.15)",
            boxShadow: "0 4px 12px rgba(0, 0, 0, 0.3)",
            zIndex: 30,
            touchAction: "none",
          }}
        >
          <video
            ref={previewVideoRef}
            style={{
              width: "100%",
              height: "100%",
              objectFit: "cover",
              transform: facingMode === "user" ? "scaleX(-1)" : "none",
            }}
            playsInline
            muted
            autoPlay
          />
          <button
            onClick={() => flipCamera()}
            style={{
              position: "absolute",
              top: 4,
              right: 4,
              width: 24,
              height: 24,
              borderRadius: "50%",
              background: "rgba(0, 0, 0, 0.5)",
              border: "none",
              color: "white",
              fontSize: 12,
              display: "flex",
              alignItems: "center",
              justifyContent: "center",
              cursor: "pointer",
            }}
            title="Flip camera"
          >
            ↻
          </button>
        </div>
      )}

      {/* Rating Modal */}
      {showRatingModal && (
        <div className="fixed inset-0 z-50 flex items-center justify-center" style={{ background: "rgba(0,0,0,0.6)", backdropFilter: "blur(12px)" }}>
          <div style={{
            background: "#0D1117",
            border: "1px solid rgba(255,255,255,0.06)",
            borderRadius: 16,
            padding: "32px 28px",
            maxWidth: 360,
            width: "100%",
            fontFamily: "'DM Sans', sans-serif",
            textAlign: "center",
          }}>
            <h2 style={{
              fontSize: 20,
              fontFamily: "'Playfair Display', serif",
              fontWeight: 400,
              color: "#E2E8F0",
              marginBottom: 20,
              marginTop: 0,
            }}>
              Rate your conversation
            </h2>

            <div className="flex gap-2 justify-center mb-6">
              {[1, 2, 3, 4, 5].map((star) => (
                <button
                  key={star}
                  onMouseEnter={() => setHoverRating(star)}
                  onMouseLeave={() => setHoverRating(0)}
                  onClick={() => setRating(star)}
                  className="transition-transform hover:scale-110 focus:outline-none p-1"
                  style={{ background: "none", border: "none", cursor: "pointer" }}
                >
                  <Star
                    size={28}
                    className="transition-colors duration-150"
                    style={{
                      fill: star <= (hoverRating || rating) ? "#8B9DC3" : "transparent",
                      color: star <= (hoverRating || rating) ? "#8B9DC3" : "rgba(201,209,217,0.2)",
                    }}
                  />
                </button>
              ))}
            </div>

            <div className="flex flex-col w-full gap-3">
              <button
                onClick={handleRate}
                disabled={rating === 0}
                style={{
                  width: "100%",
                  padding: "12px 0",
                  borderRadius: 10,
                  border: "none",
                  background: rating > 0 ? "linear-gradient(135deg, rgba(107,125,179,0.3), rgba(107,125,179,0.15))" : "rgba(255,255,255,0.04)",
                  color: rating > 0 ? "#C9D1D9" : "rgba(201,209,217,0.3)",
                  fontSize: 14,
                  fontWeight: 500,
                  cursor: rating > 0 ? "pointer" : "not-allowed",
                  fontFamily: "'DM Sans', sans-serif",
                  transition: "all 0.2s",
                }}
              >
                Rate it
              </button>
              <button
                onClick={handleContinue}
                style={{
                  width: "100%",
                  padding: "12px 0",
                  background: "none",
                  border: "none",
                  color: "rgba(201,209,217,0.35)",
                  fontSize: 14,
                  fontWeight: 400,
                  cursor: "pointer",
                  fontFamily: "'DM Sans', sans-serif",
                  transition: "color 0.2s",
                }}
              >
                Continue
              </button>
            </div>
          </div>
        </div>
      )}

      {/* Limit Reached — Paywall Overlay (Free users & Guests only, never Pro) */}
      {error === "limit_reached" && !isPro && (
        <div
          className="fixed inset-0 z-50 flex items-center justify-center"
          style={{
            background: "rgba(13,17,23,0.85)",
            backdropFilter: "blur(20px)",
            animation: "paywallFadeIn 0.6s ease both",
          }}
        >
          <div style={{
            background: "linear-gradient(135deg, rgba(20,25,35,0.95), rgba(13,17,23,0.98))",
            border: "1px solid rgba(107,125,179,0.12)",
            borderRadius: 20,
            padding: "40px 32px",
            maxWidth: 420,
            width: "100%",
            fontFamily: "'DM Sans', sans-serif",
            textAlign: "center",
            boxShadow: "0 0 80px rgba(107,125,179,0.06)",
          }}>
            {/* Ambient glow */}
            <div style={{
              width: 72,
              height: 72,
              borderRadius: 18,
              background: "linear-gradient(135deg, rgba(107,125,179,0.15), rgba(107,125,179,0.05))",
              border: "1px solid rgba(107,125,179,0.2)",
              display: "flex",
              alignItems: "center",
              justifyContent: "center",
              margin: "0 auto 24px",
            }}>
              <Clock size={28} style={{ color: "rgba(139,157,195,0.7)" }} />
            </div>

            {isGuest ? (
              <>
                <h2 style={{
                  fontSize: 24,
                  fontFamily: "'Playfair Display', serif",
                  fontWeight: 400,
                  color: "#E2E8F0",
                  marginBottom: 10,
                  marginTop: 0,
                }}>
                  This is the beginning of something
                </h2>
                <p style={{
                  fontSize: 15,
                  fontWeight: 300,
                  color: "rgba(201,209,217,0.5)",
                  lineHeight: 1.7,
                  marginBottom: 32,
                }}>
                  Create a free account and Kira keeps building on everything
                  you just talked about — and every conversation after.
                </p>
                <div className="flex flex-col w-full gap-3">
                  <button
                    onClick={handleSignUp}
                    style={{
                      width: "100%",
                      padding: "14px 0",
                      borderRadius: 12,
                      border: "1px solid rgba(107,125,179,0.25)",
                      background: "linear-gradient(135deg, rgba(107,125,179,0.25), rgba(107,125,179,0.1))",
                      color: "#C9D1D9",
                      fontSize: 15,
                      fontWeight: 500,
                      cursor: "pointer",
                      fontFamily: "'DM Sans', sans-serif",
                      transition: "all 0.3s ease",
                      boxShadow: "0 0 30px rgba(107,125,179,0.08)",
                    }}
                  >
                    Create free account
                  </button>
                  <a
                    href="/"
                    style={{
                      display: "block",
                      width: "100%",
                      padding: "12px 0",
                      color: "rgba(201,209,217,0.3)",
                      fontSize: 14,
                      fontWeight: 400,
                      textAlign: "center",
                      textDecoration: "none",
                      transition: "color 0.2s",
                    }}
                  >
                    I&apos;ll come back tomorrow
                  </a>
                </div>
              </>
            ) : (
              <>
                <h2 style={{
                  fontSize: 24,
                  fontFamily: "'Playfair Display', serif",
                  fontWeight: 400,
                  color: "#E2E8F0",
                  marginBottom: 10,
                  marginTop: 0,
                }}>
                  You&apos;ve used your 15 minutes
                </h2>
                <p style={{
                  fontSize: 15,
                  fontWeight: 300,
                  color: "rgba(201,209,217,0.5)",
                  lineHeight: 1.7,
                  marginBottom: 32,
                }}>
                  Upgrade to Pro for unlimited conversations,
                  priority responses, and persistent memory across sessions.
                </p>
                <div className="flex flex-col w-full gap-3">
                  <button
                    onClick={handleUpgrade}
                    style={{
                      width: "100%",
                      padding: "14px 0",
                      borderRadius: 12,
                      border: "1px solid rgba(107,125,179,0.25)",
                      background: "linear-gradient(135deg, rgba(107,125,179,0.25), rgba(107,125,179,0.1))",
                      color: "#C9D1D9",
                      fontSize: 15,
                      fontWeight: 500,
                      cursor: "pointer",
                      fontFamily: "'DM Sans', sans-serif",
                      transition: "all 0.3s ease",
                      boxShadow: "0 0 30px rgba(107,125,179,0.08)",
                    }}
                  >
                    Upgrade to Pro — $9.99/mo
                  </button>
                  <a
                    href="/"
                    style={{
                      display: "block",
                      width: "100%",
                      padding: "12px 0",
                      color: "rgba(201,209,217,0.3)",
                      fontSize: 14,
                      fontWeight: 400,
                      textAlign: "center",
                      textDecoration: "none",
                      transition: "color 0.2s",
                    }}
                  >
                    I&apos;ll come back tomorrow
                  </a>
                </div>
              </>
            )}
          </div>
        </div>
      )}

      {/* Pro Limit Reached — Warm Full-Screen Overlay (no upsell) */}
      {error === "limit_reached_pro" && (
        <div
          className="fixed inset-0 z-50 flex items-center justify-center"
          style={{
            background: "rgba(13,17,23,0.85)",
            backdropFilter: "blur(20px)",
            animation: "paywallFadeIn 0.6s ease both",
          }}
        >
          <div style={{
            background: "linear-gradient(135deg, rgba(20,25,35,0.95), rgba(13,17,23,0.98))",
            border: "1px solid rgba(107,125,179,0.12)",
            borderRadius: 20,
            padding: "40px 32px",
            maxWidth: 420,
            width: "100%",
            fontFamily: "'DM Sans', sans-serif",
            textAlign: "center",
            boxShadow: "0 0 80px rgba(107,125,179,0.06)",
          }}>
            <div style={{
              width: 72,
              height: 72,
              borderRadius: 18,
              background: "linear-gradient(135deg, rgba(107,125,179,0.15), rgba(107,125,179,0.05))",
              border: "1px solid rgba(107,125,179,0.2)",
              display: "flex",
              alignItems: "center",
              justifyContent: "center",
              margin: "0 auto 24px",
            }}>
              <Clock size={28} style={{ color: "rgba(139,157,195,0.7)" }} />
            </div>

            <h2 style={{
              fontSize: 24,
              fontFamily: "'Playfair Display', serif",
              fontWeight: 400,
              color: "#E2E8F0",
              marginBottom: 10,
              marginTop: 0,
            }}>
              You&apos;ve had quite the month
            </h2>
            <p style={{
              fontSize: 15,
              fontWeight: 300,
              color: "rgba(201,209,217,0.5)",
              lineHeight: 1.7,
              marginBottom: 8,
            }}>
              You&apos;ve reached your monthly conversation limit.
              Your conversations and memories are safe — Kira will be
              ready to pick up right where you left off.
            </p>
            <p style={{
              fontSize: 13,
              fontWeight: 300,
              color: "rgba(201,209,217,0.3)",
              marginBottom: 32,
            }}>
              Resets on the 1st of next month
            </p>
            <a
              href="/"
              style={{
                display: "block",
                width: "100%",
                padding: "14px 0",
                borderRadius: 12,
                border: "1px solid rgba(107,125,179,0.15)",
                background: "rgba(107,125,179,0.08)",
                color: "rgba(201,209,217,0.6)",
                fontSize: 15,
                fontWeight: 500,
                textAlign: "center",
                textDecoration: "none",
                fontFamily: "'DM Sans', sans-serif",
                transition: "all 0.3s ease",
              }}
            >
              Back to home
            </a>
          </div>
        </div>
      )}

      {/* Mobile Audio Unlock Overlay */}
      {isAudioBlocked && (
        <div className="fixed inset-0 z-50 flex items-center justify-center" style={{ background: "rgba(0,0,0,0.6)", backdropFilter: "blur(12px)" }}>
          <button
            onClick={resumeAudio}
            style={{
              display: "flex",
              flexDirection: "column",
              alignItems: "center",
              gap: 16,
              padding: "32px 40px",
              borderRadius: 16,
              background: "#0D1117",
              border: "1px solid rgba(255,255,255,0.06)",
              cursor: "pointer",
              fontFamily: "'DM Sans', sans-serif",
              transition: "transform 0.2s",
            }}
            onMouseEnter={(e) => { e.currentTarget.style.transform = "scale(1.02)"; }}
            onMouseLeave={(e) => { e.currentTarget.style.transform = "scale(1)"; }}
          >
            <div style={{
              width: 56,
              height: 56,
              borderRadius: 14,
              background: "linear-gradient(135deg, rgba(107,125,179,0.2), rgba(107,125,179,0.08))",
              border: "1px solid rgba(107,125,179,0.25)",
              display: "flex",
              alignItems: "center",
              justifyContent: "center",
            }}>
              <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="rgba(139,157,195,0.7)" strokeWidth="1.8" strokeLinecap="round" strokeLinejoin="round">
                <path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z" />
                <path d="M19 10v2a7 7 0 0 1-14 0v-2" />
                <line x1="12" y1="19" x2="12" y2="22" />
              </svg>
            </div>
            <span style={{
              fontSize: 16,
              fontWeight: 500,
              color: "#C9D1D9",
            }}>Tap to Start</span>
          </button>
        </div>
      )}
    </div>
  );
}
