â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FILE: packages/web/src/components/Live2DAvatar.tsx
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"use client";

import { useEffect, useRef, useState } from "react";

// Silent in production unless ?debug is in the URL
const isDebug = typeof window !== 'undefined' && (process.env.NODE_ENV !== 'production' || window.location.search.includes('debug'));
function debugLog(...args: any[]) { if (isDebug) debugLog(...args); }

// pixi-live2d-display requires PIXI on window before import.
// Dynamic import is handled below to avoid SSR issues.

/** Load the Cubism 4 Core SDK if not already present */
function loadCubismCore(): Promise<void> {
  return new Promise((resolve, reject) => {
    if ((window as any).Live2DCubismCore) {
      resolve();
      return;
    }
    const script = document.createElement("script");
    script.src = "https://cubism.live2d.com/sdk-web/cubismcore/live2dcubismcore.min.js";
    script.onload = () => {
      debugLog("[Live2D] Cubism Core loaded");
      resolve();
    };
    script.onerror = () => reject(new Error("Failed to load Cubism Core SDK"));
    document.head.appendChild(script);
  });
}

/** Static emotionâ†’expression map (used in init flush + expression effect) */
const EMOTION_MAP_STATIC: Record<string, string | null> = {
  neutral: null,
  happy: null,
  excited: "star_eyes",
  love: "heart_eyes",
  blush: "blush",
  sad: "tears",
  angry: "angry",
  playful: "tongue_out",
  thinking: "dazed",
  speechless: "speechless",
  eyeroll: "eye_roll",
  sleepy: "sleeping",
};

interface Live2DAvatarProps {
  isSpeaking: boolean;
  analyserNode: AnalyserNode | null;
  emotion?: string | null;
  accessories?: string[];
  onModelReady?: () => void;
  onLoadError?: () => void;
}

/** Log JS heap usage (Chrome only â€” no-op on Safari/Firefox) */
function logMemory(label: string) {
  try {
    const mem = (performance as any).memory;
    if (mem) {
      debugLog(`[Memory] ${label} â€” Used: ${(mem.usedJSHeapSize / 1024 / 1024).toFixed(1)}MB, Total: ${(mem.totalJSHeapSize / 1024 / 1024).toFixed(1)}MB`);
    }
  } catch {}
}

export default function Live2DAvatar({ isSpeaking, analyserNode, emotion, accessories, onModelReady, onLoadError }: Live2DAvatarProps) {
  const containerRef = useRef<HTMLDivElement>(null);
  const appRef = useRef<any>(null);
  const modelRef = useRef<any>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null); // explicit canvas ref for cleanup
  const glRef = useRef<WebGLRenderingContext | WebGL2RenderingContext | null>(null); // stored GL context â€” NEVER call getContext during cleanup
  const contextLostHandlerRef = useRef<((e: Event) => void) | null>(null); // stored for removal
  const animFrameRef = useRef<number>(0);
  const expressionTimeoutRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const activeAccessoriesRef = useRef<Set<string>>(new Set());
  const initializedRef = useRef(false);
  const modelStableRef = useRef(false);
  const modelStableTimer = useRef<ReturnType<typeof setTimeout> | null>(null);
  const pendingEmotion = useRef<string | null>(null);
  const pendingAccessories = useRef<string[]>([]);
  const webglCrashCount = useRef(0);
  const pixiCreatedAt = useRef(0); // timestamp for crash diagnostics
  const pixiResolutionRef = useRef(1); // store actual PIXI resolution for positioning math
  const baseScaleRef = useRef(0);
  const baseYRef = useRef(0);
  const lastPinchDistance = useRef<number | null>(null);
  const [zoomLevel, setZoomLevel] = useState(1.0);
  const zoomLevelRef = useRef(1.0);
  const onModelReadyRef = useRef(onModelReady);
  onModelReadyRef.current = onModelReady;
  const onLoadErrorRef = useRef(onLoadError);
  onLoadErrorRef.current = onLoadError;
  const [modelReady, setModelReady] = useState(false);

  /**
   * Full GPU + memory cleanup â€” must release ALL resources to prevent the
   * "second conversation crash" on mobile (iOS limits WebGL to ~2 contexts).
   *
   * Order matters:
   *   1. Stop render loop (no more GPU draw calls)
   *   2. Destroy Live2D model (releases model buffers)
   *   3. Remove webglcontextlost listener (prevent closure leak)
   *   4. Destroy PIXI app *while context is still valid* (so it can delete
   *      textures / framebuffers via real WebGL calls)
   *   5. Lose WebGL context (forces the browser to free GPU memory)
   *   6. Remove canvas from DOM
   *   7. Clear PIXI global texture caches (module-level singletons that survive unmount)
   *   8. Reset refs
   */
  const cleanupLive2D = useRef(() => {
    debugLog("[Live2D] Starting full cleanupâ€¦");
    logMemory("before cleanup");

    // 1. Stop render loop
    if (animFrameRef.current) {
      cancelAnimationFrame(animFrameRef.current);
      animFrameRef.current = 0;
    }

    // Clear timers
    if (modelStableTimer.current) {
      clearTimeout(modelStableTimer.current);
      modelStableTimer.current = null;
    }
    if (expressionTimeoutRef.current) {
      clearTimeout(expressionTimeoutRef.current);
      expressionTimeoutRef.current = null;
    }

    // 2. Destroy the Live2D model (frees model buffers + child display objects)
    if (modelRef.current) {
      try {
        modelRef.current.destroy({ children: true });
        debugLog("[Live2D] Model destroyed");
      } catch (e) {
        console.warn("[Live2D] Model destroy error (may already be destroyed):", e);
      }
      modelRef.current = null;
    }

    // 3. Remove the webglcontextlost listener (its closure captures the PIXI app,
    //    preventing GC if left attached)
    if (canvasRef.current && contextLostHandlerRef.current) {
      canvasRef.current.removeEventListener("webglcontextlost", contextLostHandlerRef.current);
      contextLostHandlerRef.current = null;
    }

    // 4. Destroy the PIXI Application *before* losing the context â€”
    //    PIXI needs a live context to call deleteTexture / deleteBuffer etc.
    if (appRef.current) {
      try {
        appRef.current.ticker.stop();
      } catch {}
      try {
        appRef.current.destroy(true, {
          children: true,
          texture: true,
          baseTexture: true,
        });
        debugLog("[Live2D] PIXI app destroyed");
      } catch (e) {
        console.warn("[Live2D] PIXI app destroy error:", e);
      }
      appRef.current = null;
    }

    // 5. Explicitly lose the WebGL context using the STORED reference.
    //    NEVER call getContext() here â€” on iOS it can CREATE a new context
    //    (counting against the ~2-3 context limit) instead of returning the old one.
    if (glRef.current) {
      try {
        if (!glRef.current.isContextLost()) {
          const ext = glRef.current.getExtension("WEBGL_lose_context");
          if (ext) {
            ext.loseContext();
            debugLog("[Live2D] WebGL context explicitly released");
          }
        }
      } catch {}
      glRef.current = null;
    }

    // 6. Remove canvas from DOM (PIXI's destroy(true) should do this,
    //    but belt-and-suspenders for the context-loss path)
    if (canvasRef.current && canvasRef.current.parentNode) {
      canvasRef.current.parentNode.removeChild(canvasRef.current);
    }
    canvasRef.current = null;

    // 7. Flush PIXI's global texture caches â€” these are module-level Maps
    //    that survive component unmount and hold GPU texture references.
    try {
      const PIXI = (window as any).PIXI;
      if (PIXI) {
        if (PIXI.utils?.TextureCache) {
          for (const key in PIXI.utils.TextureCache) {
            try { PIXI.utils.TextureCache[key].destroy(true); } catch {}
          }
        }
        if (PIXI.utils?.BaseTextureCache) {
          for (const key in PIXI.utils.BaseTextureCache) {
            try { PIXI.utils.BaseTextureCache[key].destroy(); } catch {}
          }
        }
        debugLog("[Live2D] PIXI texture caches cleared");
      }
    } catch (e) {
      console.warn("[Live2D] Cache cleanup error:", e);
    }

    // 8. Reset all state refs
    initializedRef.current = false;
    modelStableRef.current = false;
    pendingEmotion.current = null;
    pendingAccessories.current = [];
    activeAccessoriesRef.current = new Set();
    setModelReady(false);

    // 9. Clear PIXI from window â€” the Live2D SDK reads from window.PIXI,
    //    and stale references from a previous instance can cause init failures.
    try {
      delete (window as any).PIXI;
    } catch {}

    logMemory("after cleanup");
    debugLog("[Live2D] Cleanup complete");
  });

  // Initialize PixiJS app + load model (runs once on mount)
  useEffect(() => {
    if (!containerRef.current || initializedRef.current) return;

    // Guard: destroy any orphaned PIXI app from a previous mount (React strict mode)
    if (appRef.current) {
      console.warn("[Live2D] PIXI app already exists â€” running full cleanup first");
      cleanupLive2D.current();
    }

    // Remove orphaned canvases from previous mounts (React strict mode).
    // Do NOT call getContext on them â€” on iOS that can CREATE a new context
    // and waste one of the ~2-3 available context slots.
    const container = containerRef.current;
    if (container) {
      const oldCanvases = container.querySelectorAll("canvas");
      oldCanvases.forEach(c => {
        c.remove();
        debugLog("[Live2D] Removed orphaned canvas");
      });
    }

    initializedRef.current = true;
    logMemory("before init");

    let destroyed = false;
    let loadTimeoutId: ReturnType<typeof setTimeout> | null = null;

    (async () => {
      const loadStart = performance.now();
      try {
        const MODEL_LOAD_TIMEOUT = 30000;

        // Race the entire init against a timeout
        const timeoutPromise = new Promise<never>((_, reject) => {
          loadTimeoutId = setTimeout(
            () => reject(new Error(`Model load timeout (${MODEL_LOAD_TIMEOUT}ms)`)),
            MODEL_LOAD_TIMEOUT
          );
        });

        await Promise.race([_initLive2D(), timeoutPromise]);
      } catch (err) {
        if (!destroyed) {
          console.error(`[Live2D] Initialization failed after ${(performance.now() - loadStart).toFixed(0)}ms:`, err);
          onLoadErrorRef.current?.();
        }
      } finally {
        if (loadTimeoutId) clearTimeout(loadTimeoutId);
      }

      async function _initLive2D() {
        // 1. Load Cubism Core SDK (required by pixi-live2d-display)
        await loadCubismCore();

        // 2. Dynamic imports to avoid SSR â€” pixi-live2d-display touches window/document
        const PIXI = await import("pixi.js");
        // Set PIXI on window BEFORE importing pixi-live2d-display
        (window as any).PIXI = PIXI;

        const { Live2DModel } = await import("pixi-live2d-display/cubism4");

        if (destroyed || !containerRef.current) return;

        // Detect mobile for GPU budget decisions
        const isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);
        // Full native resolution â€” no cap. iPhone 12 gets 3x, desktop whatever the display supports.
        const resolution = window.devicePixelRatio || 1;

        let app: InstanceType<typeof PIXI.Application>;
        try {
          app = new PIXI.Application({
            backgroundAlpha: 0,
            resizeTo: containerRef.current,
            resolution,
            autoDensity: true,
            antialias: !isMobile,
            powerPreference: isMobile ? "low-power" : "default",
          });
          containerRef.current.appendChild(app.view as unknown as HTMLCanvasElement);
        } catch (pixiErr) {
          console.error("[Live2D] Failed to create PIXI app:", pixiErr);
          onLoadErrorRef.current?.();
          return;
        }
        appRef.current = app;
        // Store canvas ref for cleanup (PIXI.view is the <canvas>)
        canvasRef.current = app.view as unknown as HTMLCanvasElement;
        pixiCreatedAt.current = Date.now();
        pixiResolutionRef.current = resolution;
        debugLog(`[Live2D] PIXI app created (resolution: ${resolution}, antialias: ${!isMobile})`);

        // Listen for WebGL context loss (iOS kills GPU context under memory pressure)
        const canvas = canvasRef.current!;
        // Store the GL context NOW â€” never call getContext again (iOS context limit)
        const gl = canvas.getContext("webgl2") || canvas.getContext("webgl");
        glRef.current = gl;
        if (gl) {
          // Log GPU memory budget if available (WEBGL_debug_renderer_info)
          try {
            const ext = gl.getExtension("WEBGL_debug_renderer_info");
            if (ext) {
              debugLog(`[Live2D] GPU: ${gl.getParameter(ext.UNMASKED_RENDERER_WEBGL)}`);
            }
          } catch {}
        }
        const handleContextLost = (e: Event) => {
          e.preventDefault();
          webglCrashCount.current++;
          const aliveSeconds = ((Date.now() - pixiCreatedAt.current) / 1000).toFixed(1);
          console.error(`[Live2D] WebGL context lost (crash #${webglCrashCount.current}) after ${aliveSeconds}s`);
          logMemory("at context loss");
          if (webglCrashCount.current >= 2) {
            console.error("[Live2D] Multiple WebGL crashes â€” staying on orb permanently");
          }
          // Stop the PIXI ticker to prevent further render attempts on a dead context
          try { appRef.current?.ticker.stop(); } catch {}
          cancelAnimationFrame(animFrameRef.current);
          onLoadErrorRef.current?.();
        };
        // Store handler ref so cleanup can remove it (prevents closure leak)
        contextLostHandlerRef.current = handleContextLost;
        canvas.addEventListener("webglcontextlost", handleContextLost);

        let model;
        try {
          const modelPath = isMobile
            ? "/worklets/models/Kira/kira.mobile.model3.json"
            : "/worklets/models/Kira/kira.model3.json";
          model = await Live2DModel.from(
            modelPath,
            { autoInteract: false }
          );
        } catch (modelErr) {
          console.error("[Live2D] Failed to load model:", modelErr);
          onLoadErrorRef.current?.();
          return;
        }

        if (destroyed) return;

        app.stage.addChild(model as any);

        // Framing: show head to mid-thigh, centered with breathing room
        // Use the actual PIXI resolution (not device DPR) to convert renderer pixels â†’ CSS pixels
        const containerWidth = app.renderer.width / resolution;
        const containerHeight = app.renderer.height / resolution;

        const scaleFactor = 0.9;
        const scale = Math.min(
          containerWidth / model.width,
          containerHeight / model.height
        ) * scaleFactor;
        model.scale.set(scale);
        model.x = containerWidth / 2;
        const yPosition = containerHeight * (isMobile ? 0.59 : 0.56);
        model.y = yPosition;
        model.anchor.set(0.5, 0.5);

        // Store base positioning for zoom math
        baseScaleRef.current = scale;
        baseYRef.current = yPosition;

        // Eye tracking â€” eyes follow the cursor
        app.stage.interactive = true;
        app.stage.hitArea = app.renderer.screen;
        app.stage.on("pointermove", (e: any) => {
          model.focus(e.global.x, e.global.y);
        });

        modelRef.current = model;
        const loadMs = (performance.now() - loadStart).toFixed(0);
        debugLog(`[Live2D] Model loaded successfully in ${loadMs}ms`);

        // Hide the built-in watermark overlay.
        // Live2D resets parameters every frame, so we must override
        // Param155 on every tick. The original update method is patched
        // to force the watermark parameter after each internal update.
        try {
          const internalModel = model.internalModel;
          const origUpdate = internalModel.update.bind(internalModel);
          internalModel.update = function (dt: number, now: number) {
            origUpdate(dt, now);
            try {
              (internalModel.coreModel as any).setParameterValueById("Param155", 1);
            } catch {}
          };
          debugLog("[Live2D] Watermark hide patch applied (Param155=1 per frame)");
        } catch (err2) {
          console.warn("[Live2D] Could not patch watermark hide:", err2);
        }

        // Wait 2 frames for the watermark parameter to take effect before showing
        requestAnimationFrame(() => {
          requestAnimationFrame(() => {
            if (!destroyed) {
              setModelReady(true);
              onModelReadyRef.current?.();
              debugLog(`[Live2D] Model ready â€” revealing (total ${(performance.now() - loadStart).toFixed(0)}ms)`);

              // Delay expressions/accessories for 2s to let GPU settle
              modelStableTimer.current = setTimeout(() => {
                modelStableRef.current = true;
                debugLog("[Live2D] Model stable â€” expressions/accessories enabled");

                // Flush any queued emotion
                if (pendingEmotion.current && modelRef.current) {
                  const expr = pendingEmotion.current;
                  pendingEmotion.current = null;
                  try {
                    const mapped = EMOTION_MAP_STATIC[expr];
                    if (mapped) {
                      modelRef.current.expression(mapped);
                      debugLog(`[Live2D] Flushed queued expression: ${mapped}`);
                    }
                  } catch {}
                }

                // Flush any queued accessories
                if (pendingAccessories.current.length > 0 && modelRef.current) {
                  pendingAccessories.current.forEach(acc => {
                    try {
                      modelRef.current.expression(acc);
                      activeAccessoriesRef.current.add(acc);
                      debugLog(`[Live2D] Flushed queued accessory: ${acc}`);
                    } catch {}
                  });
                  pendingAccessories.current = [];
                }
              }, 2000);
            }
          });
        });
      } // end _initLive2D
    })();

    // Handle resize
    const handleResize = () => {
      if (appRef.current?.renderer && containerRef.current) {
        appRef.current.renderer.resize(
          containerRef.current.clientWidth,
          containerRef.current.clientHeight
        );
        // Re-center model on resize
        if (modelRef.current) {
          const res = pixiResolutionRef.current;
          const w = appRef.current.renderer.width / res;
          const h = appRef.current.renderer.height / res;

          // Recalculate base scale from the model's intrinsic size
          // (model.width/height already factor in scale, so divide it out first)
          const currentScale = modelRef.current.scale.x || 1;
          const rawWidth = modelRef.current.width / currentScale;
          const rawHeight = modelRef.current.height / currentScale;
          const mobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);
          const sf = 0.9;
          const newBaseScale = Math.min(w / rawWidth, h / rawHeight) * sf;
          baseScaleRef.current = newBaseScale;
          baseYRef.current = h * (mobile ? 0.59 : 0.56);

          const z = zoomLevelRef.current;
          modelRef.current.scale.set(newBaseScale * z);
          modelRef.current.x = w / 2;
          modelRef.current.y = baseYRef.current + (z - 1.0) * modelRef.current.height * 0.35;
        }
      }
    };
    window.addEventListener("resize", handleResize);

    return () => {
      destroyed = true;
      window.removeEventListener("resize", handleResize);
      cleanupLive2D.current();
    };
  }, []);

  // Safety net: release GPU resources if the page is being unloaded (tab close,
  // hard navigation, etc.) â€” React's unmount may not fire in time on mobile Safari.
  useEffect(() => {
    const handleBeforeUnload = () => {
      cleanupLive2D.current();
    };
    window.addEventListener("beforeunload", handleBeforeUnload);
    return () => {
      window.removeEventListener("beforeunload", handleBeforeUnload);
    };
  }, []);

  // Lip sync with instant open + rapid multiplicative decay
  useEffect(() => {
    const model = modelRef.current;

    if (!isSpeaking || !analyserNode || !model) {
      // Close mouth when not speaking
      try {
        model?.internalModel?.coreModel?.setParameterValueById("ParamMouthOpenY", 0);
        model?.internalModel?.coreModel?.setParameterValueById("ParamMouthForm", 0.15);
      } catch {}
      cancelAnimationFrame(animFrameRef.current);
      return;
    }

    const dataArray = new Uint8Array(analyserNode.frequencyBinCount);
    let smoothedVolume = 0;

    function animateMouth() {
      if (!modelRef.current || !analyserNode) return;

      analyserNode.getByteFrequencyData(dataArray);

      // Sample speech frequency bins (roughly 100-1000Hz range)
      // Skip bin 0 (DC offset), bins 1-5 carry most speech energy
      let sum = 0;
      const startBin = 1;
      const endBin = Math.min(6, dataArray.length);
      for (let i = startBin; i < endBin; i++) sum += dataArray[i];
      const rawVolume = sum / (endBin - startBin);

      // Normalize to 0-1 range
      const normalizedVolume = Math.min(rawVolume / 80, 1.0);

      // Two-speed: instant open, rapid multiplicative close
      if (normalizedVolume > smoothedVolume) {
        smoothedVolume = normalizedVolume; // Instant open â€” no smoothing up
      } else {
        smoothedVolume *= 0.6; // Rapid decay â€” drops to near-zero in ~3-4 frames
      }

      // Hard cutoff for near-silence
      if (smoothedVolume < 0.03) smoothedVolume = 0;

      // Square root curve â€” makes quiet speech more visible
      const mouthOpen = Math.sqrt(smoothedVolume);

      try {
        const core = modelRef.current.internalModel?.coreModel;
        if (core) {
          core.setParameterValueById("ParamMouthOpenY", mouthOpen);
          core.setParameterValueById("ParamMouthForm", 0.15 + mouthOpen * 0.2);
        }
      } catch {}

      animFrameRef.current = requestAnimationFrame(animateMouth);
    }

    animateMouth();

    return () => cancelAnimationFrame(animFrameRef.current);
  }, [isSpeaking, analyserNode]);

  // Use module-level emotion map
  const EMOTION_MAP = EMOTION_MAP_STATIC;

  /**
   * Properly reset Live2D expressions by clearing the expression manager state.
   * model.expression() with no args cycles to the NEXT expression in the list
   * (triggering random accessory expressions), so we must clear explicitly.
   */
  function resetExpression(model: any) {
    try {
      const mgr = model.internalModel?.motionManager?.expressionManager;
      if (mgr) {
        // Clear the currently playing expression
        if (mgr._expressions) {
          mgr._expressions.forEach((expr: any) => {
            if (expr && typeof expr.weight !== "undefined") {
              expr.weight = 0;
            }
          });
        }
        // Null out the tracked current expression
        if ("_currentExpression" in mgr) mgr._currentExpression = null;
        if ("currentExpression" in mgr) mgr.currentExpression = null;
        if ("_expressionIndex" in mgr) mgr._expressionIndex = -1;
        if ("expressionIndex" in mgr) mgr.expressionIndex = -1;
        debugLog("[Live2D] Expression cleared via manager");
      }
    } catch (err) {
      console.warn("[Live2D] Failed to reset expression:", err);
    }

    // Re-apply active accessories (they must persist through emotion resets)
    Array.from(activeAccessoriesRef.current).forEach(acc => {
      try {
        model.expression(acc);
      } catch (err) {
        // ignore â€” accessory may not exist
      }
    });
  }

  // Watch for expression changes
  useEffect(() => {
    const model = modelRef.current;
    if (!model || !emotion) return;

    // Queue if model not yet stable (prevents WebGL crash from early expression)
    if (!modelStableRef.current) {
      debugLog(`[Live2D] Queuing emotion â€” model not yet stable: ${emotion}`);
      pendingEmotion.current = emotion;
      return;
    }

    // Clear any pending reset
    if (expressionTimeoutRef.current) {
      clearTimeout(expressionTimeoutRef.current);
      expressionTimeoutRef.current = null;
    }

    const expressionName = EMOTION_MAP[emotion];

    if (expressionName) {
      // Trigger the expression
      try {
        model.expression(expressionName);
        debugLog(`[Live2D] Expression: ${expressionName} (emotion: ${emotion})`);
      } catch (err) {
        console.warn(`[Live2D] Failed to set expression: ${expressionName}`, err);
      }

      // Auto-reset to neutral after 4 seconds
      expressionTimeoutRef.current = setTimeout(() => {
        resetExpression(model);
      }, 4000);
    } else {
      // neutral/happy â€” clear any active expression
      resetExpression(model);
    }
  }, [emotion]);

  // Watch for accessory changes â€” accessories persist (unlike emotions which flash)
  useEffect(() => {
    const model = modelRef.current;
    if (!model) return;

    // Queue if model not yet stable
    if (!modelStableRef.current && accessories) {
      const newItems = accessories.filter(a => !activeAccessoriesRef.current.has(a));
      if (newItems.length > 0) {
        debugLog(`[Live2D] Queuing accessories â€” model not yet stable: ${newItems.join(", ")}`);
        pendingAccessories.current.push(...newItems);
      }
      return;
    }

    if (accessories) {
      const newSet = new Set(accessories);

      // Turn ON new accessories
      Array.from(newSet).forEach(acc => {
        if (!activeAccessoriesRef.current.has(acc)) {
          try {
            model.expression(acc);
            debugLog(`[Live2D] Accessory ON: ${acc}`);
          } catch (err) {
            console.warn(`[Live2D] Failed to apply accessory: ${acc}`, err);
          }
        }
      });

      activeAccessoriesRef.current = newSet;
    }
  }, [accessories]);

  // Clean up expression timeout on unmount
  useEffect(() => {
    return () => {
      if (expressionTimeoutRef.current) {
        clearTimeout(expressionTimeoutRef.current);
      }
    };
  }, []);

  // Zoom: scroll wheel (desktop) + pinch (mobile)
  useEffect(() => {
    const container = containerRef.current;
    if (!container) return;

    const MIN_ZOOM = 1.0;
    const MAX_ZOOM = 2.0;
    const ZOOM_STEP = 0.1;

    const handleWheel = (e: WheelEvent) => {
      e.preventDefault();
      setZoomLevel(prev => {
        const next = prev + (e.deltaY < 0 ? ZOOM_STEP : -ZOOM_STEP);
        return Math.min(MAX_ZOOM, Math.max(MIN_ZOOM, Math.round(next * 100) / 100));
      });
    };

    const handleTouchMove = (e: TouchEvent) => {
      if (e.touches.length !== 2) return;
      e.preventDefault();

      const dx = e.touches[0].clientX - e.touches[1].clientX;
      const dy = e.touches[0].clientY - e.touches[1].clientY;
      const distance = Math.sqrt(dx * dx + dy * dy);

      if (lastPinchDistance.current !== null) {
        const delta = distance - lastPinchDistance.current;
        setZoomLevel(prev => {
          const next = prev + delta * 0.005;
          return Math.min(MAX_ZOOM, Math.max(MIN_ZOOM, Math.round(next * 100) / 100));
        });
      }
      lastPinchDistance.current = distance;
    };

    const handleTouchEnd = () => {
      lastPinchDistance.current = null;
    };

    container.addEventListener("wheel", handleWheel, { passive: false });
    container.addEventListener("touchmove", handleTouchMove, { passive: false });
    container.addEventListener("touchend", handleTouchEnd);
    container.addEventListener("touchcancel", handleTouchEnd);

    return () => {
      container.removeEventListener("wheel", handleWheel);
      container.removeEventListener("touchmove", handleTouchMove);
      container.removeEventListener("touchend", handleTouchEnd);
      container.removeEventListener("touchcancel", handleTouchEnd);
    };
  }, []);

  // Apply zoom to model â€” scale up + shift down to keep face centered
  useEffect(() => {
    zoomLevelRef.current = zoomLevel;
    const model = modelRef.current;
    if (!model || !baseScaleRef.current) return;

    model.scale.set(baseScaleRef.current * zoomLevel);
    const yOffset = (zoomLevel - 1.0) * model.height * 0.35;
    model.y = baseYRef.current + yOffset;
  }, [zoomLevel]);

  return (
    <div style={{ width: "100%", height: "100%", maxWidth: "600px", maxHeight: "90vh", margin: "0 auto", position: "relative", overflow: "hidden" }}>
      <div
        ref={containerRef}
        style={{
          width: "100%",
          height: "100%",
          pointerEvents: "auto",
          overflow: "hidden",
          opacity: modelReady ? 1 : 0,
          transition: "opacity 0.3s ease-in",
        }}
      />
    </div>
  );
}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
AVAILABLE MODEL PARAMETERS (from .model3.json):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
--- packages/web/public/worklets/models/Kira/kira.mobile.model3.json ---
{
	"Version": 3,
	"FileReferences": {
		"Moc": "kira.moc3",
		"Textures": [
			"kira.2048/texture_00.png",
			"kira.2048/texture_01.png"
		],
		"Physics": "kira.physics3.json",
		"DisplayInfo": "kira.cdi3.json",
		"Expressions": [
			{ "Name": "blush", "File": "blush.exp3.json" },
			{ "Name": "angry", "File": "angry.exp3.json" },
			{ "Name": "tears", "File": "tears.exp3.json" },
			{ "Name": "star_eyes", "File": "star_eyes.exp3.json" },
			{ "Name": "heart_eyes", "File": "heart_eyes.exp3.json" },
			{ "Name": "dazed", "File": "dazed.exp3.json" },
			{ "Name": "speechless", "File": "speechless.exp3.json" },
			{ "Name": "sleeping", "File": "sleeping.exp3.json" },
			{ "Name": "spiral_eyes", "File": "spiral_eyes.exp3.json" },
			{ "Name": "eye_roll", "File": "eye_roll.exp3.json" },
			{ "Name": "dark_face", "File": "dark_face.exp3.json" },
			{ "Name": "tongue_out", "File": "tongue_out.exp3.json" },
			{ "Name": "donut_mouth", "File": "donut_mouth.exp3.json" },
			{ "Name": "bandaid", "File": "bandaid.exp3.json" },
			{ "Name": "eye_mask", "File": "eye_mask.exp3.json" },
			{ "Name": "glasses", "File": "glasses.exp3.json" },
			{ "Name": "short_hair", "File": "short_hair.exp3.json" },
			{ "Name": "headphones_on", "File": "headphones_on.exp3.json" },
			{ "Name": "neck_headphones", "File": "neck_headphones.exp3.json" },
			{ "Name": "earbuds", "File": "earbuds.exp3.json" },
			{ "Name": "clip_bangs", "File": "clip_bangs.exp3.json" },
			{ "Name": "low_twintails", "File": "low_twintails.exp3.json" },
			{ "Name": "hide_ears_tail", "File": "hide_ears_tail.exp3.json" },
			{ "Name": "cat_mic", "File": "cat_mic.exp3.json" },
			{ "Name": "watermark", "File": "watermark.exp3.json" },
			{ "Name": "hold_knife", "File": "hold_knife.exp3.json" },
			{ "Name": "hold_phone", "File": "hold_phone.exp3.json" },
			{ "Name": "hold_lollipop", "File": "hold_lollipop.exp3.json" },
			{ "Name": "hold_drawing_board", "File": "hold_drawing_board.exp3.json" },
			{ "Name": "hold_pen", "File": "hold_pen.exp3.json" },
			{ "Name": "photo_off", "File": "photo_off.exp3.json" },
			{ "Name": "gaming", "File": "gaming.exp3.json" },
			{ "Name": "remove_jacket", "File": "remove_jacket.exp3.json" },
			{ "Name": "blood_stain", "File": "blood_stain.exp3.json" }
		],
		"Motions": {
			"Idle": [
				{ "File": "Scene1.motion3.json", "FadeInTime": 0.5, "FadeOutTime": 0.5 }
			]
		}
	},
	"Groups": [
		{
			"Target": "Parameter",
			"Name": "EyeBlink",
			"Ids": [
				"ParamEyeROpen",
				"ParamEyeRSmile",
				"ParamEyeLOpen",
				"ParamEyeLSmile"
			]
		},
		{
			"Target": "Parameter",
			"Name": "LipSync",
			"Ids": [
				"ParamMouthOpenY"
			]
		}
	]
}
--- packages/web/public/worklets/models/Kira/kira.model3.json ---
{
	"Version": 3,
	"FileReferences": {
		"Moc": "kira.moc3",
		"Textures": [
			"kira.8192/texture_00.png",
			"kira.8192/texture_01.png"
		],
		"Physics": "kira.physics3.json",
		"DisplayInfo": "kira.cdi3.json",
		"Expressions": [
			{ "Name": "blush", "File": "blush.exp3.json" },
			{ "Name": "angry", "File": "angry.exp3.json" },
			{ "Name": "tears", "File": "tears.exp3.json" },
			{ "Name": "star_eyes", "File": "star_eyes.exp3.json" },
			{ "Name": "heart_eyes", "File": "heart_eyes.exp3.json" },
			{ "Name": "dazed", "File": "dazed.exp3.json" },
			{ "Name": "speechless", "File": "speechless.exp3.json" },
			{ "Name": "sleeping", "File": "sleeping.exp3.json" },
			{ "Name": "spiral_eyes", "File": "spiral_eyes.exp3.json" },
			{ "Name": "eye_roll", "File": "eye_roll.exp3.json" },
			{ "Name": "dark_face", "File": "dark_face.exp3.json" },
			{ "Name": "tongue_out", "File": "tongue_out.exp3.json" },
			{ "Name": "donut_mouth", "File": "donut_mouth.exp3.json" },
			{ "Name": "bandaid", "File": "bandaid.exp3.json" },
			{ "Name": "eye_mask", "File": "eye_mask.exp3.json" },
			{ "Name": "glasses", "File": "glasses.exp3.json" },
			{ "Name": "short_hair", "File": "short_hair.exp3.json" },
			{ "Name": "headphones_on", "File": "headphones_on.exp3.json" },
			{ "Name": "neck_headphones", "File": "neck_headphones.exp3.json" },
			{ "Name": "earbuds", "File": "earbuds.exp3.json" },
			{ "Name": "clip_bangs", "File": "clip_bangs.exp3.json" },
			{ "Name": "low_twintails", "File": "low_twintails.exp3.json" },
			{ "Name": "hide_ears_tail", "File": "hide_ears_tail.exp3.json" },
			{ "Name": "cat_mic", "File": "cat_mic.exp3.json" },
			{ "Name": "watermark", "File": "watermark.exp3.json" },
			{ "Name": "hold_knife", "File": "hold_knife.exp3.json" },
			{ "Name": "hold_phone", "File": "hold_phone.exp3.json" },
			{ "Name": "hold_lollipop", "File": "hold_lollipop.exp3.json" },
			{ "Name": "hold_drawing_board", "File": "hold_drawing_board.exp3.json" },
			{ "Name": "hold_pen", "File": "hold_pen.exp3.json" },
			{ "Name": "photo_off", "File": "photo_off.exp3.json" },
			{ "Name": "gaming", "File": "gaming.exp3.json" },
			{ "Name": "remove_jacket", "File": "remove_jacket.exp3.json" },
			{ "Name": "blood_stain", "File": "blood_stain.exp3.json" }
		],
		"Motions": {
			"Idle": [
				{ "File": "Scene1.motion3.json", "FadeInTime": 0.5, "FadeOutTime": 0.5 }
			]
		}
	},
	"Groups": [
		{
			"Target": "Parameter",
			"Name": "EyeBlink",
			"Ids": [
				"ParamEyeROpen",
				"ParamEyeRSmile",
				"ParamEyeLOpen",
				"ParamEyeLSmile"
			]
		},
		{
			"Target": "Parameter",
			"Name": "LipSync",
			"Ids": [
				"ParamMouthOpenY"
			]
		}
	]
}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
AVAILABLE MOTIONS (list all .motion3.json files):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
--- packages/web/public/worklets/models/Kira/Scene1.motion3.json ---

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
AVAILABLE EXPRESSIONS (list all .exp3.json files):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
--- packages/web/public/worklets/models/Kira/dark_face.exp3.json ---
--- packages/web/public/worklets/models/Kira/photo_off.exp3.json ---
--- packages/web/public/worklets/models/Kira/blush.exp3.json ---
--- packages/web/public/worklets/models/Kira/hold_lollipop.exp3.json ---
--- packages/web/public/worklets/models/Kira/eye_mask.exp3.json ---
--- packages/web/public/worklets/models/Kira/sleeping.exp3.json ---
--- packages/web/public/worklets/models/Kira/eye_roll.exp3.json ---
--- packages/web/public/worklets/models/Kira/tears.exp3.json ---
--- packages/web/public/worklets/models/Kira/neck_headphones.exp3.json ---
--- packages/web/public/worklets/models/Kira/headphones_on.exp3.json ---
--- packages/web/public/worklets/models/Kira/tongue_out.exp3.json ---
--- packages/web/public/worklets/models/Kira/cat_mic.exp3.json ---
--- packages/web/public/worklets/models/Kira/flying_head.exp3.json ---
--- packages/web/public/worklets/models/Kira/blood_stain.exp3.json ---
--- packages/web/public/worklets/models/Kira/hold_pen.exp3.json ---
--- packages/web/public/worklets/models/Kira/short_hair.exp3.json ---
--- packages/web/public/worklets/models/Kira/gaming.exp3.json ---
--- packages/web/public/worklets/models/Kira/chibi.exp3.json ---
--- packages/web/public/worklets/models/Kira/star_eyes.exp3.json ---
--- packages/web/public/worklets/models/Kira/angry.exp3.json ---
--- packages/web/public/worklets/models/Kira/heart_eyes.exp3.json ---
--- packages/web/public/worklets/models/Kira/clip_bangs.exp3.json ---
--- packages/web/public/worklets/models/Kira/hold_drawing_board.exp3.json ---
--- packages/web/public/worklets/models/Kira/hide_ears_tail.exp3.json ---
--- packages/web/public/worklets/models/Kira/low_twintails.exp3.json ---
--- packages/web/public/worklets/models/Kira/dazed.exp3.json ---
--- packages/web/public/worklets/models/Kira/remove_jacket.exp3.json ---
--- packages/web/public/worklets/models/Kira/speechless.exp3.json ---
--- packages/web/public/worklets/models/Kira/spiral_eyes.exp3.json ---
--- packages/web/public/worklets/models/Kira/donut_mouth.exp3.json ---
--- packages/web/public/worklets/models/Kira/hold_knife.exp3.json ---
--- packages/web/public/worklets/models/Kira/watermark.exp3.json ---
--- packages/web/public/worklets/models/Kira/glasses.exp3.json ---
--- packages/web/public/worklets/models/Kira/hold_phone.exp3.json ---
--- packages/web/public/worklets/models/Kira/earbuds.exp3.json ---
--- packages/web/public/worklets/models/Kira/bandaid.exp3.json ---
--- packages/web/public/worklets/models/Kira/mouse_hairpin.exp3.json ---

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ALL PARAMETER IDs IN MODEL (what we can actually control):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SERVER EMOTION DETECTION (what emotions server detects):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
75:    return "blush";
82:    return "excited";
88:    return "love";
94:    return "sad";
100:    return "playful";
106:    return "thinking";
111:    return "speechless";
116:    return "eyeroll";
121:    return "sleepy";
126:    return "angry";
132:    return "happy";
135:  return "neutral";

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CLIENT EXPRESSION HANDLING (how expressions map to model):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
6:const isDebug = typeof window !== 'undefined' && (process.env.NODE_ENV !== 'production' || window.location.search.includes('debug'));
30:/** Static emotionâ†’expression map (used in init flush + expression effect) */
35:  love: "heart_eyes",
37:  sad: "tears",
49:  emotion?: string | null;
65:export default function Live2DAvatar({ isSpeaking, analyserNode, emotion, accessories, onModelReady, onLoadError }: Live2DAvatarProps) {
73:  const expressionTimeoutRef = useRef<ReturnType<typeof setTimeout> | null>(null);
78:  const pendingEmotion = useRef<string | null>(null);
106:   *   7. Clear PIXI global texture caches (module-level singletons that survive unmount)
119:    // Clear timers
121:      clearTimeout(modelStableTimer.current);
124:    if (expressionTimeoutRef.current) {
125:      clearTimeout(expressionTimeoutRef.current);
126:      expressionTimeoutRef.current = null;
204:        debugLog("[Live2D] PIXI texture caches cleared");
213:    pendingEmotion.current = null;
218:    // 9. Clear PIXI from window â€” the Live2D SDK reads from window.PIXI,
276:        if (loadTimeoutId) clearTimeout(loadTimeoutId);
403:        // Live2D resets parameters every frame, so we must override
405:        // to force the watermark parameter after each internal update.
420:        // Wait 2 frames for the watermark parameter to take effect before showing
428:              // Delay expressions/accessories for 2s to let GPU settle
431:                debugLog("[Live2D] Model stable â€” expressions/accessories enabled");
433:                // Flush any queued emotion
434:                if (pendingEmotion.current && modelRef.current) {
435:                  const expr = pendingEmotion.current;
436:                  pendingEmotion.current = null;
440:                      modelRef.current.expression(mapped);
441:                      debugLog(`[Live2D] Flushed queued expression: ${mapped}`);
450:                      modelRef.current.expression(acc);
452:                      debugLog(`[Live2D] Flushed queued accessory: ${acc}`);
553:        smoothedVolume *= 0.6; // Rapid decay â€” drops to near-zero in ~3-4 frames
556:      // Hard cutoff for near-silence
578:  // Use module-level emotion map
582:   * Properly reset Live2D expressions by clearing the expression manager state.
583:   * model.expression() with no args cycles to the NEXT expression in the list
584:   * (triggering random accessory expressions), so we must clear explicitly.
588:      const mgr = model.internalModel?.motionManager?.expressionManager;
590:        // Clear the currently playing expression
591:        if (mgr._expressions) {
592:          mgr._expressions.forEach((expr: any) => {
598:        // Null out the tracked current expression
601:        if ("_expressionIndex" in mgr) mgr._expressionIndex = -1;
602:        if ("expressionIndex" in mgr) mgr.expressionIndex = -1;
603:        debugLog("[Live2D] Expression cleared via manager");
606:      console.warn("[Live2D] Failed to reset expression:", err);
609:    // Re-apply active accessories (they must persist through emotion resets)
612:        model.expression(acc);
614:        // ignore â€” accessory may not exist
619:  // Watch for expression changes
622:    if (!model || !emotion) return;
624:    // Queue if model not yet stable (prevents WebGL crash from early expression)
626:      debugLog(`[Live2D] Queuing emotion â€” model not yet stable: ${emotion}`);
627:      pendingEmotion.current = emotion;
631:    // Clear any pending reset
632:    if (expressionTimeoutRef.current) {
633:      clearTimeout(expressionTimeoutRef.current);
634:      expressionTimeoutRef.current = null;
637:    const expressionName = EMOTION_MAP[emotion];
639:    if (expressionName) {

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
FULL PARAMETER LIST (from kira.cdi3.json â€” human-readable names)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

--- TOGGLE/SWITCH PARAMS (â˜†â˜†åˆ‡æ¢æŒ‰é”®â˜†â˜†) ---
Param4          â˜†â˜†åˆ‡æ¢æŒ‰é”®â˜†â˜†
Param155        æ°´å°å…³é—­ (watermark off)
Param156        é£å¤´ (flying head)
Param78         è½¬Q (chibi)
Param52         å¤¹åˆ˜æµ· (clip bangs)
Param45         ä½èºæ—‹ (low twintails)
Param46         çŸ­å‘ (short hair)
Param61         å¤´æˆ´è€³æœº (headphones on)
Param159        è€³éº¦ (earbuds)
Param158        éº¦å…‹é£éšè— (mic hide)
Param62         è„–é¢ˆè€³æœº (neck headphones)
Param64         åˆ›å¯è´´ (bandaid)
Param66         çœ¼ç½© (eye mask)
Param67         é¼ æ ‡å‘é¥° (mouse hairpin)
Param80         å¤–å¥—ç©¿è„± (remove jacket)
Param117        çœ¼é•œ (glasses)
Param123        ç™½çœ¼ (eye roll)
Param154        å°çŒ«éº¦å…‹é£ (cat mic)
Param157        å…½è€³å…½å°¾å…³é—­ (HIDE EARS + TAIL)

--- EXPRESSION PARAMS (â˜†â˜†è¡¨æƒ…æŒ‰é”®â˜†â˜†) ---
Param5          â˜†â˜†è¡¨æƒ…æŒ‰é”®â˜†â˜†
Param113        ç—´å‘† (dazed)
Param60         è„¸çº¢ (blush)
Param90         è¡€è¿¹ (blood stain)
Param63         é»‘è„¸ (dark face)
Param97         æ˜Ÿæ˜Ÿçœ¼ (star eyes)
Param99         ç¡è§‰ (sleeping)
Param110        çœ¼æ³ª (tears)
Param112        çˆ±å¿ƒçœ¼ (heart eyes)
Param115        åœˆåœˆçœ¼ (spiral eyes)
Param118        å˜´å¼ç”œç”œåœˆ (donut mouth)
Param119        æ— è¯­ (speechless)
Param120        ç”Ÿæ°” (angry)

--- ACTION PARAMS (â˜†â˜†åŠ¨ä½œæŒ‰é”®â˜†â˜†) ---
Param6          â˜†â˜†åŠ¨ä½œæŒ‰é”®â˜†â˜†
Param79         æ‹¿ç”»æ¿ (hold drawing board)
Param48         æ‹¿ç¬”å†™å­— (hold pen)
Param81         æ‹¿æ‰‹æœº (hold phone)
Param42         æ‹ç…§å…³é—­ (photo off)
Param82         æ‹¿æ£’æ£’ç³– (hold lollipop)
Param83         æ‹¿åˆ€ (hold knife)
Param88         æ‰“æ¸¸æˆ (gaming)

--- FACE/HEAD INPUT PARAMS ---
ParamAngle5     è¾“å…¥ X (face input X)
ParamAngle4     è¾“å…¥ Y (face input Y)
ParamAngle3     è¾“å…¥ Z (face input Z)
ParamAngle12    è¾“å…¥ èº«ä½“X (body input X)
ParamAngle13    è¾“å…¥ èº«ä½“Y (body input Y)
ParamAngle14    è¾“å…¥ èº«ä½“Z (body input Z)
ParamAngle7     è„¸ X (face X)
ParamAngle8     è„¸ Y (face Y)
ParamAngle9     è„¸ Z (face Z)
ParamAngle15    èº«X (body X)
ParamAngle16    èº«Y (body Y)
ParamAngle17    èº«Z (body Z)

--- EYE PARAMS ---
ParamEyeROpen   å·¦çœ¼ é–‹é–‰ (right eye open/close)
ParamEyeRSmile  å·¦çœ¼ å¾®ç¬‘ (right eye smile)
ParamEyeLOpen   å³çœ¼ (left eye open/close)
ParamEyeLSmile  å³çœ¼ å¾®ç¬‘ (left eye smile)
ParamEyeBallX   çœ¼ç  X (eyeball X)
ParamEyeBallY   çœ¼ç  Y (eyeball Y)
ParamBrowLY     å³çœ‰ä¸Šä¸‹ (left brow Y)
ParamBrowRY     å·¦çœ‰ ä¸Šä¸‹ (right brow Y)
ParamBrowLForm  å³çœ‰ å¤‰å½¢ (left brow form)
ParamBrowRForm  å·¦çœ‰ å¤‰å½¢ (right brow form)

--- EAR PARAMS (10 params total!) ---
Param71         å·¦è€³ (left ear 1)
Param72         å·¦è€³2 (left ear 2)
Param73         å·¦è€³3 (left ear 3)
Param76         å·¦è€³4 (left ear 4)
Param77         å·¦è€³5 (left ear 5)
Param68         å³è€³ (right ear 1)
Param69         å³è€³2 (right ear 2)
Param70         å³è€³3 (right ear 3)
Param74         å³è€³4 (right ear 4)
Param75         å³è€³5 (right ear 5)

--- MOUTH PARAMS ---
ParamMouthForm     å˜´ å˜å½¢ (mouth form)
ParamMouthOpenY    å˜´ å¼ å¼€å’Œé—­åˆ (mouth open/close)
ParamTongueOut     åèˆŒ (tongue out)
ParamMouthFunnel   æ’…å˜´ (funnel/pucker)
ParamMouthShrug    æŠ¿å˜´ (shrug/press)
ParamJawOpen       åš¼åš¼ é¢æ• (jaw open - face capture)
ParamJawOpen2      åš¼åš¼ (jaw open)
ParamCheekPuff     é¼“è„¸é¢æ• (cheek puff - face capture)
ParamCheekPuff3    é¼“è„¸ (cheek puff)
ParamMouthX        æ­ªå˜´é¢æ• (mouth X - face capture)
ParamMouthX2       æ­ªå˜´ (mouth X)

--- BREATHING ---
ParamBreath        å‘¼å¸ (breathing)

--- PHYSICS-DRIVEN EARS ---
Physics[35]: ParamEyeROpen -> Param71, Param72, Param73, Param147, Param148 (LEFT ear)
Physics[36]: ParamEyeLOpen -> Param68, Param69, Param70, Param149, Param150 (RIGHT ear)

--- IDLE MOTION (Scene1.motion3.json) ---
Duration: 3s, Loop: true
Curves (6 total):
  Param109  (ZåŠ¨ç”» - Z animation)
  Param111  (çœ¼æ³ªåŠ¨ç”» - tear animation)
  Param114  (ç—´å‘†åŠ¨ç”» - daze animation)
  Param116  (åœˆåœˆåŠ¨ç”» - spiral animation)
  Param47   (é—ªå…‰ç¯åŠ¨ç”» - flash animation)
  Param145  (å°¾å·´åŠ¨ç”» - TAIL animation)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
AUDIT ANALYSIS â€” WHAT'S BROKEN / UNUSED / QUICK WINS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”´ EAR BUG â€” ROOT CAUSE:
   Ears are driven by PHYSICS linked to eye open/close params (ParamEyeROpen/LOpen).
   The EyeBlink group in model3.json uses these same params for auto-blink.
   If the auto-blink system is properly running, ears SHOULD animate via physics.
   Param157 = "å…½è€³å…½å°¾å…³é—­" = HIDE EARS + TAIL toggle.
   If hide_ears_tail expression was ever activated (or stuck), ears vanish.
   CHECK: Is Param157 being set to 1 anywhere? Or is EyeBlink group not running?

ğŸŸ¡ PARAMS THE CODE USES (only 4!):
   - ParamMouthOpenY (lip sync)
   - ParamMouthForm (lip sync smile shape)
   - Param155 (watermark hide)
   - Eye tracking via model.focus() (sets ParamEyeBallX/Y)

ğŸŸ¢ PARAMS THE CODE DOES NOT USE (rich model features wasted!):
   - ParamBreath (breathing animation â€” should be a gentle idle sine wave!)
   - ParamBrowLY/RY + ParamBrowLForm/RForm (eyebrow movement â€” huge expressiveness)
   - ParamAngle7/8/9 (face angle X/Y/Z â€” head tilt for emotions!)
   - ParamAngle15/16/17 (body sway â€” could add idle motion)
   - ParamTongueOut (tongue â€” for playful expression)
   - ParamMouthFunnel (pucker lips)
   - ParamMouthShrug (press lips â€” thinking)
   - ParamCheekPuff3 (cheek puff â€” cute idle reaction)
   - ParamJawOpen2 (jaw movement â€” could supplement lip sync)
   - ParamMouthX2 (mouth sideways â€” could add personality to speech)

ğŸŸ¢ EXPRESSIONS ON DISK BUT NOT IN model3.json:
   - flying_head.exp3.json (exists on disk, not in Expressions array)
   - chibi.exp3.json (exists on disk, not in Expressions array)
   - mouse_hairpin.exp3.json (exists on disk, not in Expressions array)

ğŸŸ¢ EMOTION MAP GAPS:
   Server detects 12 emotions â†’ EMOTION_MAP maps to 10 expressions.
   "neutral" and "happy" both map to null (no expression change).
   Could use brow/head params for subtle happy vs neutral difference.

ğŸ”µ IDLE MOTION IS MINIMAL:
   Only 6 curves in Scene1.motion3.json â€” animates Z position, tear/daze/spiral/flash effects, and tail.
   Does NOT animate: breathing, body sway, head micro-movements, ear wiggles, brow raises.
   Most Live2D characters have much richer idle motions.
